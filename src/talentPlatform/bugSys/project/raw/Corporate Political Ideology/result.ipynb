{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#실질적으로 입력 해야 하는 부분은 year = 2010 이라고 되어 있는데, 여기서 연도를 바꿔 주시면 됩니다.\n",
    "#셀은 크게 2개로 나누어져 있고 위에는 정치인 연설 데이터를 이용해서 모델 학습 하는 부분이고 \n",
    "#아래 데이터는 기업 데이터를 이용하여 검증하는 부분입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import tweepy\n",
    "import csv\n",
    "import re\n",
    "import operator \n",
    "from collections import Counter\n",
    "from tweepy import OAuthHandler\n",
    "import sys\n",
    "import time\n",
    "#import urllib2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    "from tensorflow.keras.datasets import reuters\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model"
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\indisystem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\indisystem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\indisystem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\indisystem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = 2010\n",
    "\n",
    "pol_2010_data = pd.read_csv(\"./Congressmen+Speech/POL_RES/polres_\"+str(years)+\".csv\",encoding=\"euc-kr\")\n",
    "# pol_2010_data = pd.read_csv(\"./Congressmen+Speech/POL_RES/polres_\"+str(years)+\".csv\",encoding=\"euc-kr\")\n",
    "\n",
    "# 정제와 단어 토큰화\n",
    "vocab = {} # 파이썬의 dictionary 자료형\n",
    "sentences = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "pol_2010_data[\"textlens\"] = 0\n",
    "\n",
    "for te in range(0,len(pol_2010_data)):\n",
    "    text = sent_tokenize(pol_2010_data[\"V2\"][te])\n",
    "    del text[0]\n",
    "    pol_2010_data[\"textlens\"][te] = len(text)\n",
    "    for i in text:\n",
    "        \n",
    "        sentence = word_tokenize(i) # 단어 토큰화를 수행합니다.\n",
    "        result = []\n",
    "\n",
    "        for word in sentence: \n",
    "            word = word.lower() # 모든 단어를 소문자화하여 단어의 개수를 줄입니다.\n",
    "            if word not in stop_words: # 단어 토큰화 된 결과에 대해서 불용어를 제거합니다.\n",
    "                if len(word) > 2: # 단어 길이가 2이하인 경우에 대하여 추가로 단어를 제거합니다.\n",
    "                    result.append(word)\n",
    "                    if word not in vocab:\n",
    "                        vocab[word] = 0 \n",
    "                    vocab[word] += 1\n",
    "        sentences.append(result) \n",
    "        \n",
    "vocab_sorted = sorted(vocab.items(), key = lambda x:x[1], reverse = True)\n",
    "\n",
    "word_to_index = {}\n",
    "i=0\n",
    "for (word, frequency) in vocab_sorted :\n",
    "    if frequency > 1 : # \n",
    "        i=i+1\n",
    "        word_to_index[word] = i\n",
    "        \n",
    "\n",
    "vocab_size = 99999\n",
    "words_frequency = [w for w,c in word_to_index.items() if c >= vocab_size + 1] \n",
    "for w in words_frequency:\n",
    "    del word_to_index[w] # 해당 단어에 대한 인덱스 정보를 삭제\n",
    "#print(word_to_index)\n",
    "\n",
    "\n",
    "encoded = []\n",
    "for s in sentences:\n",
    "    temp = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            temp.append(word_to_index[w])\n",
    "        except KeyError:\n",
    "            continue\n",
    "                        \n",
    "\n",
    "    encoded.append(temp)\n",
    "    \n",
    "list_result = []\n",
    "for i in range(0,len(pol_2010_data[\"textlens\"])):\n",
    "    for j in range(0,pol_2010_data[\"textlens\"][i]):\n",
    "        list_result.append(pol_2010_data[\"result\"][i])\n",
    "          \n",
    "        \n",
    "print('최대 길이 :{}'.format(max(len(l) for l in encoded)))\n",
    "print('평균 길이 :{}'.format(sum(map(len, encoded))/len(encoded)))\n",
    "\n",
    "# 나누기\n",
    "df_x_train, df_x_test, df_y_train, df_y_test = train_test_split(encoded, list_result, test_size=0.3, stratify=list_result)\n",
    "        \n",
    "max_len = 30\n",
    "X_TRAIN = pad_sequences(df_x_train, maxlen=max_len) # 훈련용 패딩\n",
    "X_TEST = pad_sequences(df_x_test, maxlen=max_len) # 훈련용 패딩\n",
    "Y_TRAIN = to_categorical(df_y_train)\n",
    "Y_TEST = to_categorical(df_y_test)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(100000, 120))\n",
    "model.add(LSTM(120))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('best_model_'+str(years)+'.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "history = model.fit(X_TRAIN, Y_TRAIN, batch_size=128, epochs=30, callbacks=[es, mc], validation_data=(X_TEST, Y_TEST))\n",
    "\n",
    "loaded_model = load_model('best_model_'+str(years)+'.h5')\n",
    "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_TEST, Y_TEST)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, len(history.history['acc']) + 1)\n",
    "plt.plot(epochs, history.history['loss'])\n",
    "plt.plot(epochs, history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "화사이름 : 2020_ and welcome.csv\n",
      "화사이름 : 2020_ Joe. Good afternoon.csv\n",
      "화사이름 : 2020_ John. Good afternoon and welcome to our first quarter earnings call. Before we begin.csv\n",
      "화사이름 : 2020_ Rob. Overall.csv\n",
      "화사이름 : 2020_.csv\n",
      "화사이름 : 2020_1-800-FLOWERS.COM, Inc..csv\n",
      "공화당적 발언 정도 : 0.6477891047024864\n",
      "민주당적 발언 정도 : 0.3522108921716953\n",
      "화사이름 : 2020_10x Genomics, Inc..csv\n",
      "공화당적 발언 정도 : 0.4956341708000277\n",
      "민주당적 발언 정도 : 0.5043658202277942\n",
      "화사이름 : 2020_111, Inc..csv\n",
      "공화당적 발언 정도 : 0.5878028806411859\n",
      "민주당적 발언 정도 : 0.4121971219175994\n",
      "화사이름 : 2020_180 Degree Capital Corp.csv\n",
      "공화당적 발언 정도 : 0.47394437130505795\n",
      "민주당적 발언 정도 : 0.5260556239251778\n",
      "화사이름 : 2020_1Life Healthcare, Inc..csv\n",
      "공화당적 발언 정도 : 0.4644277523902635\n",
      "민주당적 발언 정도 : 0.5355722412707344\n",
      "화사이름 : 2020_21Vianet Group, Inc..csv\n",
      "공화당적 발언 정도 : 0.5150176038344702\n",
      "민주당적 발언 정도 : 0.48498239951829114\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-377-d695bafb3b59>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     49\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mencoded\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     50\u001B[0m         \u001B[1;32mif\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mencoded\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m>=\u001B[0m \u001B[1;36m10\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 51\u001B[1;33m             \u001B[0maa\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mloaded_model\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mencoded\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     52\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     53\u001B[0m             \u001B[0mrows\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0maa\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001B[0m in \u001B[0;36mpredict\u001B[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1721\u001B[0m       \u001B[0mcallbacks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mon_predict_begin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1722\u001B[0m       \u001B[0mbatch_outputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1723\u001B[1;33m       \u001B[1;32mfor\u001B[0m \u001B[0m_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0miterator\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0menumerate_epochs\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m  \u001B[1;31m# Single epoch.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1724\u001B[0m         \u001B[1;32mwith\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcatch_stop_iteration\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1725\u001B[0m           \u001B[1;32mfor\u001B[0m \u001B[0mstep\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msteps\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001B[0m in \u001B[0;36menumerate_epochs\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1197\u001B[0m     \u001B[1;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1198\u001B[0m     \u001B[1;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_truncate_execution_to_epoch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1199\u001B[1;33m       \u001B[0mdata_iterator\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0miter\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_dataset\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1200\u001B[0m       \u001B[1;32mfor\u001B[0m \u001B[0mepoch\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_initial_epoch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_epochs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1201\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_insufficient_data\u001B[0m\u001B[1;33m:\u001B[0m  \u001B[1;31m# Set by `catch_stop_iteration`.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001B[0m in \u001B[0;36m__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    484\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mcontext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexecuting_eagerly\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minside_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    485\u001B[0m       \u001B[1;32mwith\u001B[0m \u001B[0mops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcolocate_with\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_variant_tensor\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 486\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0miterator_ops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mOwnedIterator\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    487\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    488\u001B[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, dataset, components, element_spec)\u001B[0m\n\u001B[0;32m    694\u001B[0m       \u001B[1;32mif\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mcomponents\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0melement_spec\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    695\u001B[0m         \u001B[1;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0merror_message\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 696\u001B[1;33m       \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_create_iterator\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    697\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    698\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0m_create_iterator\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdataset\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001B[0m in \u001B[0;36m_create_iterator\u001B[1;34m(self, dataset)\u001B[0m\n\u001B[0;32m    717\u001B[0m               \u001B[0moutput_types\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_flat_output_types\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    718\u001B[0m               output_shapes=self._flat_output_shapes))\n\u001B[1;32m--> 719\u001B[1;33m       \u001B[0mgen_dataset_ops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmake_iterator\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mds_variant\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_iterator_resource\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    720\u001B[0m       \u001B[1;31m# Delete the resource when this object is deleted\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    721\u001B[0m       self._resource_deleter = IteratorResourceDeleter(\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\u001B[0m in \u001B[0;36mmake_iterator\u001B[1;34m(dataset, iterator, name)\u001B[0m\n\u001B[0;32m   3117\u001B[0m   \u001B[1;32mif\u001B[0m \u001B[0mtld\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mis_eager\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3118\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3119\u001B[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001B[0m\u001B[0;32m   3120\u001B[0m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001B[0;32m   3121\u001B[0m       \u001B[1;32mreturn\u001B[0m \u001B[0m_result\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "years = 2010 \n",
    "\n",
    "pol_2010_data = pd.read_csv(\"./Congressmen+Speech/POL_RES/polres_\"+str(years)+\".csv\",encoding=\"euc-kr\")\n",
    "\n",
    "# 정제와 단어 토큰화\n",
    "vocab = {} # 파이썬의 dictionary 자료형\n",
    "sentences = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "pol_2010_data[\"textlens\"] = 0\n",
    "\n",
    "for te in range(0,len(pol_2010_data)):\n",
    "    text = sent_tokenize(pol_2010_data[\"V2\"][te])\n",
    "    del text[0]\n",
    "    pol_2010_data[\"textlens\"][te] = len(text)\n",
    "    for i in text:\n",
    "        \n",
    "        sentence = word_tokenize(i) # 단어 토큰화를 수행합니다.\n",
    "        result = []\n",
    "\n",
    "        for word in sentence: \n",
    "            word = word.lower() # 모든 단어를 소문자화하여 단어의 개수를 줄입니다.\n",
    "            if word not in stop_words: # 단어 토큰화 된 결과에 대해서 불용어를 제거합니다.\n",
    "                if len(word) > 2: # 단어 길이가 2이하인 경우에 대하여 추가로 단어를 제거합니다.\n",
    "                    result.append(word)\n",
    "                    if word not in vocab:\n",
    "                        vocab[word] = 0 \n",
    "                    vocab[word] += 1\n",
    "        sentences.append(result) \n",
    "        \n",
    "vocab_sorted = sorted(vocab.items(), key = lambda x:x[1], reverse = True)\n",
    "\n",
    "word_to_index = {}\n",
    "i=0\n",
    "for (word, frequency) in vocab_sorted :\n",
    "    if frequency > 1 : # \n",
    "        i=i+1\n",
    "        word_to_index[word] = i\n",
    "        \n",
    "\n",
    "vocab_size = 99999\n",
    "words_frequency = [w for w,c in word_to_index.items() if c >= vocab_size + 1] \n",
    "for w in words_frequency:\n",
    "    del word_to_index[w] # 해당 단어에 대한 인덱스 정보를 삭제\n",
    "    \n",
    "    \n",
    "##########################################################################################\n",
    "##########################################################################################\n",
    "##########################################################################################\n",
    "##########################################################################################\n",
    "##########################################################################################\n",
    "\n",
    "loaded_model = load_model('best_model_'+str(years)+'.h5')\n",
    "pathis = './CEO earnings call/'+str(years)+'_COMPANY/'\n",
    "fn = os.listdir(pathis)\n",
    "\n",
    "for f in fn:\n",
    "    try:\n",
    "        col_data = pd.read_csv(pathis+f,encoding=\"euc-kr\")\n",
    "    except:\n",
    "        continue\n",
    "    # 정제와 단어 토큰화\n",
    "    vocab = {} # 파이썬의 dictionary 자료형\n",
    "    sentences = []\n",
    "\n",
    "    for te in range(0,len(col_data)):\n",
    "        \n",
    "        try:\n",
    "            text = sent_tokenize(col_data[\"text\"][te])\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        for i in text:\n",
    "            sentence = word_tokenize(i) # 단어 토큰화를 수행합니다.\n",
    "            result = []\n",
    "\n",
    "            for word in sentence: \n",
    "                word = word.lower() # 모든 단어를 소문자화하여 단어의 개수를 줄입니다.\n",
    "                if word not in stop_words: # 단어 토큰화 된 결과에 대해서 불용어를 제거합니다.\n",
    "                    if len(word) > 2: # 단어 길이가 2이하인 경우에 대하여 추가로 단어를 제거합니다.\n",
    "                        result.append(word)\n",
    "                        if word not in vocab:\n",
    "                            vocab[word] = 0 \n",
    "                        vocab[word] += 1\n",
    "            sentences.append(result) \n",
    "\n",
    "    encoded = []\n",
    "    for s in sentences:\n",
    "        temp = []\n",
    "        for w in s:\n",
    "            try:\n",
    "                temp.append(word_to_index[w])\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "        encoded.append(temp)\n",
    "\n",
    "    val_0 = []\n",
    "    val_1 = []\n",
    "    for i in range(0,len(encoded)):\n",
    "        if(len(encoded[i]) >= 10):\n",
    "            aa = loaded_model.predict(encoded[i])\n",
    "\n",
    "            rows = aa.shape[0]\n",
    "            cols = aa.shape[1]\n",
    "            for r in range(0,rows):\n",
    "\n",
    "                if(aa[r][0] >= 0.8 or aa[r][1] >= 0.8): # 정치색이 강한 단어만 선택 (80%)\n",
    "                    val_0.append(aa[r][0])\n",
    "                    val_1.append(aa[r][1])   \n",
    "                    \n",
    "                    \n",
    "    try:\n",
    "        print(\"회사이름 : \" + f)\n",
    "        print(\"공화당적 발언 정도 : \" + str(sum(val_0)/len(val_0)) )\n",
    "        print(\"민주당적 발언 정도 : \" + str(sum(val_1)/len(val_1)) )\n",
    "    except:\n",
    "        continue\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}