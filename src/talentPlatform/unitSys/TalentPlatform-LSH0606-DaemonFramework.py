# -*- coding: utf-8 -*-
import argparse
import glob
import logging
import logging.handlers
import os
import platform
import sys
import traceback
import warnings
from _ast import expr
from datetime import datetime

import matplotlib as mpl
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from matplotlib import font_manager, rc
from pyparsing import col
from selenium.webdriver.support.ui import Select

from urllib.request import urlopen
from urllib import parse
from urllib.request import Request
from urllib.error import HTTPError
import json
import math
from scipy import spatial
import requests
from bs4 import BeautifulSoup
import os
import pandas as pd
from selenium import webdriver
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.options import Options
import re
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
from lxml import etree
import xml.etree.ElementTree as et
import requests
from lxml import html
import urllib
import unicodedata2
from urllib import parse
import time
from urllib.parse import quote_plus, urlencode

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import time

import time
from selenium import webdriver
import chardet
from selenium.common.exceptions import NoSuchWindowException
import requests
from bs4 import BeautifulSoup
import pytz
from pytrends.request import TrendReq
import re
from typing import List, Dict, Set
import re
from collections import defaultdict
from pathlib import Path
import os
import sys
import urllib.request
from urllib.parse import urlencode

from konlpy.tag import Okt
from collections import Counter
import re
import nltk
from nltk.corpus import stopwords
# nltk.download('stopwords')

# =================================================
# ì‚¬ìš©ì ë§¤ë‰´ì–¼
# =================================================
# [ì†ŒìŠ¤ ì½”ë“œì˜ ì‹¤í–‰ ìˆœì„œ]
# 1. ì´ˆê¸° ì„¤ì • : í°íŠ¸ ì„¤ì •
# 2. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ : ì´ˆê¸°í™” í•¨ìˆ˜ (ë¡œê·¸ ì„¤ì •, ì´ˆê¸° ë³€ìˆ˜, ì´ˆê¸° ì „ë‹¬ì¸ì ì„¤ì •) ë˜ëŠ” ìì£¼ ì‚¬ìš©í•˜ëŠ” í•¨ìˆ˜
# 3. ì£¼ í”„ë¡œê·¸ë¨ :ë¶€ í”„ë¡œê·¸ë¨ì„ í˜¸ì¶œ
# 4. ë¶€ í”„ë¡œê·¸ë¨ : ìë£Œ ì²˜ë¦¬ë¥¼ ìœ„í•œ í´ë˜ìŠ¤ë¡œì„œ ë‚´ë¶€ í•¨ìˆ˜ (ì´ˆê¸° ë³€ìˆ˜, ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§, ìˆ˜í–‰ í”„ë¡œê·¸ë¨ ì„¤ì •)
# 4.1. í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (ë¡œê·¸ ì„¤ì •) : ë¡œê·¸ ê¸°ë¡ì„ ìœ„í•œ ì„¤ì • ì •ë³´ ì½ê¸°
# 4.2. í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (ì´ˆê¸° ë³€ìˆ˜) : ì…ë ¥ ê²½ë¡œ (inpPath) ë° ì¶œë ¥ ê²½ë¡œ (outPath) ë“±ì„ ì„¤ì •
# 4.3. ì´ˆê¸° ë³€ìˆ˜ (Argument, Option) ì„¤ì • : íŒŒì´ì¬ ì‹¤í–‰ ì‹œ ì „ë‹¬ì¸ì ì„¤ì • (pyhton3 *.py argv1 argv2 argv3 ...)
# 4.4. ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ìˆ˜í–‰ : ë‹¨ìœ„ ì‹œìŠ¤í…œ (unit íŒŒì¼ëª…)ìœ¼ë¡œ ê´€ë¦¬ ë˜ëŠ” ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ êµ¬í˜„

# =================================================
# 1. ì´ˆê¸° ì„¤ì •
# =================================================
warnings.filterwarnings("ignore")

plt.rc('font', family='Malgun Gothic')
plt.rc('axes', unicode_minus=False)
# sns.set(font="Malgun Gothic", rc={"axes.unicode_minus": False}, style='darkgrid')

# ê·¸ë˜í”„ì—ì„œ ë§ˆì´ë„ˆìŠ¤ ê¸€ê¼´ ê¹¨ì§€ëŠ” ë¬¸ì œì— ëŒ€í•œ ëŒ€ì²˜
mpl.rcParams['axes.unicode_minus'] = False

# íƒ€ì„ì¡´ ì„¤ì •
tzKst = pytz.timezone('Asia/Seoul')
tzUtc = pytz.timezone('UTC')

# =================================================
# 2. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# =================================================
# ë¡œê·¸ ì„¤ì •
def initLog(env=None, contextPath=None, prjName=None):

    if env is None: env = 'local'
    if contextPath is None: contextPath = os.getcwd()
    if prjName is None: prjName = 'test'

    saveLogFile = "{}/{}_{}_{}_{}_{}_{}.log".format(
        contextPath if env in 'local' else os.path.join(contextPath, 'resources', 'log', prjName)
        , platform.system()
        , platform.machine()
        , platform.architecture()[0]
        , platform.node()
        , prjName
        , datetime.now().strftime("%Y%m%d")
    )

    if not os.path.exists(os.path.dirname(saveLogFile)):
        os.makedirs(os.path.dirname(saveLogFile))

    # logger instance ìƒì„±
    log = logging.getLogger(prjName)

    if len(log.handlers) > 0:
        return log

    # format ìƒì„±
    format = logging.Formatter('%(asctime)s [%(name)s | %(lineno)d | %(filename)s] [%(levelname)-5.5s] %(message)s')

    # handler ìƒì„±
    streamHandler = logging.StreamHandler()
    fileHandler = logging.FileHandler(saveLogFile)

    # logger instanceì— format ì„¤ì •
    streamHandler.setFormatter(format)
    fileHandler.setFormatter(format)

    # logger instanceì— handler ì„¤ì •
    log.addHandler(streamHandler)
    log.addHandler(fileHandler)

    # logger instanceë¡œ log ê¸°ë¡
    log.setLevel(level=logging.INFO)

    return log


#  ì´ˆê¸° ë³€ìˆ˜ ì„¤ì •
def initGlobalVar(env=None, contextPath=None, prjName=None):
    if env is None: env = 'local'
    if contextPath is None: contextPath = os.getcwd()
    if prjName is None: prjName = 'test'

    # í™˜ê²½ ë³€ìˆ˜ (local, ê·¸ ì™¸)ì— ë”°ë¼ ì „ì—­ ë³€ìˆ˜ (ì…ë ¥ ìë£Œ, ì¶œë ¥ ìë£Œ ë“±)ë¥¼ ë™ì ìœ¼ë¡œ ì„¤ì •
    # ì¦‰ localì˜ ê²½ìš° í˜„ì¬ ì‘ì—… ê²½ë¡œ (contextPath)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì„¤ì •
    # ê·¸ ì™¸ì˜ ê²½ìš° contextPath/resources/input/prjNameì™€ ê°™ì€ ë™ì ìœ¼ë¡œ êµ¬ì„±
    globalVar = {
        'prjName': prjName
        , 'sysOs': platform.system()
        , 'contextPath': contextPath
        , 'resPath': contextPath if env in 'local' else os.path.join(contextPath, 'resources')
        , 'cfgPath': contextPath if env in 'local' else os.path.join(contextPath, 'resources', 'config')
        , 'inpPath': contextPath if env in 'local' else os.path.join(contextPath, 'resources', 'input', prjName)
        , 'figPath': contextPath if env in 'local' else os.path.join(contextPath, 'resources', 'fig', prjName)
        , 'outPath': contextPath if env in 'local' else os.path.join(contextPath, 'resources', 'output', prjName)
        , 'movPath': contextPath if env in 'local' else os.path.join(contextPath, 'resources', 'movie', prjName)
        , 'logPath': contextPath if env in 'local' else os.path.join(contextPath, 'resources', 'log', prjName)
        , 'mapPath': contextPath if env in 'local' else os.path.join(contextPath, 'resources', 'mapInfo')
        , 'sysCfg': contextPath if env in 'local' else os.path.join(contextPath, 'resources', 'config', 'system.json')
        , 'seleniumPath': contextPath if env in 'local' else os.path.join(contextPath, 'resources', 'config', 'selenium')
        , 'fontPath': contextPath if env in 'local' else os.path.join(contextPath, 'resources', 'config', 'fontInfo')
    }

    return globalVar


#  ì´ˆê¸° ì „ë‹¬ì¸ì ì„¤ì •
def initArgument(globalVar, inParams):
    # ì›ë„ìš° ë˜ëŠ” ë§¥ í™˜ê²½
    if globalVar['sysOs'] in 'Windows' or globalVar['sysOs'] in 'Darwin':
        inParInfo = inParams

    # ë¦¬ëˆ…ìŠ¤ í™˜ê²½
    if globalVar['sysOs'] in 'Linux':
        parser = argparse.ArgumentParser()

        for i, argv in enumerate(sys.argv[1:]):
            if not argv.__contains__('--'): continue
            parser.add_argument(argv)

        inParInfo = vars(parser.parse_args())

        # ê¸€ê¼´ ì„¤ì •
        # fontList = glob.glob('{}/{}'.format(globalVar['fontPath'], '*.ttf'))
        # fontName = font_manager.FontProperties(fname=fontList[0]).get_name()
        # plt.rcParams['font.family'] = fontName

    log.info(f"[CHECK] inParInfo : {inParInfo}")

    # ì „ì—­ ë³€ìˆ˜ì— í• ë‹¹
    for key, val in inParInfo.items():
        if val is None: continue
        globalVar[key] = val

    # ì „ì—­ ë³€ìˆ˜
    for key, val in globalVar.items():
        if env not in 'local' and key.__contains__('Path') and env and not os.path.exists(val):
            os.makedirs(val)

        globalVar[key] = val.replace('\\', '/')

        log.info(f"[CHECK] {key} : {val}")

    return globalVar

# ================================================
# 4. ë¶€ í”„ë¡œê·¸ë¨
# ================================================
class DtaProcess(object):

    # ================================================
    # ìš”êµ¬ì‚¬í•­
    # ================================================
    # Pythonì„ ì´ìš©í•œ ì¤‘êµ­ ë¹…ë°ì´í„° ì‚¬ì´íŠ¸ ì¡°ì‚¬ ë° ì…€ë ˆëŠ„ ê¸°ë°˜ ë¡œê·¸ì¸, ê¸°ë³¸ ë° ë¶€ê°€ì •ë³´ ìˆ˜ì§‘ ë° ì¶”ì¶œ

    # ì›ë„ìš° X11 (X Window System) í”„ë¡œí† ì½œ ì§€ì›
    # xming

    # ë¦¬ëˆ…ìŠ¤ CLI ì‹¤í–‰
    # google-chrome --no-sandbo

    # í”„ë¡œê·¸ë¨ ì‹¤í–‰
    # cd /SYSTEMS/PROG/PYTHON/IDE/src/talentPlatform/unitSys
    # /HDD/SYSTEMS/LIB/anaconda3/envs/py38/bin/python /SYSTEMS/PROG/PYTHON/IDE/src/talentPlatform/unitSys/TalentPlatform-LSH0605-DaemonFramework.py
    # nohup /HDD/SYSTEMS/LIB/anaconda3/envs/py38/bin/python /SYSTEMS/PROG/PYTHON/IDE/src/talentPlatform/unitSys/TalentPlatform-LSH0605-DaemonFramework.py &
    # tail -f nohup.out

    # í”„ë¡œê·¸ë¨ ì¢…ë£Œ
    # ps -ef | grep "TalentPlatform-LSH0605-DaemonFramework" | awk '{print $2}' | xargs kill -9
    # ps -ef | grep "chrome" | awk '{print $2}' | xargs kill -9

    # ================================================================================================
    # í™˜ê²½ë³€ìˆ˜ ì„¤ì •
    # ================================================================================================
    global env, contextPath, prjName, serviceName, log, globalVar

    # env = 'local'  # ë¡œì»¬ : ì›ë„ìš° í™˜ê²½, ì‘ì—…í™˜ê²½ (í˜„ì¬ ì†ŒìŠ¤ ì½”ë“œ í™˜ê²½ ì‹œ .) ì„¤ì •
    env = 'dev'      # ê°œë°œ : ì›ë„ìš° í™˜ê²½, ì‘ì—…í™˜ê²½ (ì‚¬ìš©ì í™˜ê²½ ì‹œ contextPath) ì„¤ì •
    # env = 'oper'  # ìš´ì˜ : ë¦¬ëˆ…ìŠ¤ í™˜ê²½, ì‘ì—…í™˜ê²½ (ì‚¬ìš©ì í™˜ê²½ ì‹œ contextPath) ì„¤ì •

    if platform.system() == 'Windows':
        contextPath = os.getcwd() if env in 'local' else '/SYSTEMS/PROG/PYTHON/TalentPlatform-Python'
    else:
        contextPath = os.getcwd() if env in 'local' else '/SYSTEMS/PROG/PYTHON/IDE'

    prjName = 'test'
    serviceName = 'LSH0606'

    # 4.1. í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (ë¡œê·¸ ì„¤ì •)
    log = initLog(env, contextPath, prjName)

    # 4.2. í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (ì´ˆê¸° ë³€ìˆ˜)
    globalVar = initGlobalVar(env, contextPath, prjName)

    # ================================================================================================
    # 4.3. ì´ˆê¸° ë³€ìˆ˜ (Argument, Option) ì„¤ì •
    # ================================================================================================
    def __init__(self, inParams):

        log.info('[START] {}'.format("init"))

        try:
            # ì´ˆê¸° ì „ë‹¬ì¸ì ì„¤ì • (íŒŒì´ì¬ ì‹¤í–‰ ì‹œ)
            # pyhton3 *.py argv1 argv2 argv3 ...
            initArgument(globalVar, inParams)

        except Exception as e:
            log.error(f"Exception : {str(e)}")
            raise e
        finally:
            log.info('[END] {}'.format("init"))

    # ================================================================================================
    # 4.4. ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ìˆ˜í–‰
    # ================================================================================================
    def exec(self):

        log.info('[START] {}'.format("exec"))

        try:

            if platform.system() == 'Windows':
                pass
            else:
                globalVar['inpPath'] = '/DATA/INPUT'
                globalVar['outPath'] = '/DATA/OUTPUT'
                globalVar['figPath'] = '/DATA/FIG'

            # ì˜µì…˜ ì„¤ì •
            sysOpt = {
                # ìˆ˜ì§‘ ì„¤ì •
                'colct': {
                    'naver': {
                        'baseUrl': "https://datalab.naver.com/shoppingInsight/getKeywordRank.naver",
                        'cateList': [
                            {"name": "íŒ¨ì…˜ì˜ë¥˜", "param": ["50000000"]},
                            {"name": "íŒ¨ì…˜ì¡í™”", "param": ["50000001"]},
                            {"name": "í™”ì¥í’ˆ/ë¯¸ìš©", "param": ["50000002"]},
                            {"name": "ë””ì§€í„¸/ê°€ì „", "param": ["50000003"]},
                            {"name": "ê°€êµ¬/ì¸í…Œë¦¬ì–´", "param": ["50000004"]},
                            {"name": "ì¶œì‚°/ìœ¡ì•„", "param": ["50000005"]},
                            {"name": "ì‹í’ˆ", "param": ["50000006"]},
                            {"name": "ìŠ¤í¬ì¸ /ë ˆì €", "param": ["50000007"]},
                            {"name": "ìƒí™œ/ê±´ê°•", "param": ["50000008"]},
                            {"name": "ì—¬ê°€/ìƒí™œí¸ì˜", "param": ["50000009"]},
                            {"name": "ë„ì„œ", "param": ["50005542"]},
                        ],
                        'headers': {
                            "Content-Type": "application/x-www-form-urlencoded",
                            "Referer": "https://datalab.naver.com",
                            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
                        },
                    },

                    'whereispost': {
                        'baseUrl': "https://whereispost.com/hot",
                        'headers': {
                            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36"
                        },
                    },
                    'ezme': {
                        'baseUrl': "https://rank.ezme.net",
                        'headers': {
                            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36"
                        },
                    },
                },

                # ê°€ê³µ ì„¤ì •
                'filter': {
                    'stopWordFileInfo': '/SYSTEMS/PROG/PYTHON/IDE/resources/config/word/stopwords-ko.txt',
                    'forbidWordList': ["ì‹œì‹ ", "ê±°ì§€", "ì•¼ì‚¬", "ì˜ì‚¬", "ìì§€", "ë³´ì§€", "ì•„ë‹¤", "ì”¹ê³ ", "ìŒíƒ•", "í›„ì¥", "ë³‘ì›", "í™˜ì", "ì§„ë‹¨",
                                       "ì¦ìƒ", "ì¦ì„¸", "ì¬ë°œ", "ë°©ì§€", "ì‹œìˆ ", "ë³¸ì›", "ìƒë‹´", "ê³ ì", "ì¶©ë™", "í›„íšŒ", "ê³ ë¹„", "ì¸ë‚´", "ì°¸ì•„",
                                       "ìì‚´", "ìŒë¶€", "ê³ í™˜", "ì˜¤ë¹ ê°€", "í›„ë‹¤", "ë‹ˆë¯¸", "ì• ë„", "ì—ë„", "í•´ì ", "ëª°ë˜", "ì¬ìƒ", "ìœ ë°œ", "ë§Œì¡±",
                                       "ë¬´ì‹œ", "ë„¤ìš”", "í•˜ë”ë¼", "í’ˆì ˆ", "ë§¤ì§„", "ë§ˆê°", "ì˜ì•„", "ì˜ë¬¸", "ì˜ì‹¬", "ê°€ê²©", "ì •ê°€", "êµ¬ë§¤", "íŒë§¤",
                                       "ë§¤ì…", "ì§€ì €ë¶„í•¨", "ìš”ê°€", "ì²´í˜•", "ë“±ë¹¨", "íƒˆì¶œ"]
                },
            }

            # ==========================================================================================================
            # ë¸”ë¡œê·¸ ì§€ìˆ˜ì— ì˜í–¥ì„ ì£¼ëŠ” ê¸ˆì§€ì–´ ìœ„ë°˜ ëª©ë¡ ì°¾ê¸°
            # https://github.com/keunyop/BadWordCheck
            # ==========================================================================================================

            # íŒŒì¼ ëŒ€ì‹  í…ìŠ¤íŠ¸ ì…ë ¥
            text = """
ìš”ì¦˜ ëª¸ë„ í˜ë“¤ê³  ë§ˆìŒë„ í˜ë“¤ê³  ì´ë˜ì €ë˜ ê¸°ìš´ì—†ëŠ” ë‚˜ë‚ ë“¤ì„ ë³´ë‚´ê³  ìˆì–´ìš”. ëª¸ì´ í”¼ê³¤í•˜ë‹ˆ ë§ˆìŒë„ ê¸°ë¶„ë„ ìš¸ì í•œê°€ ë´ìš”. ë’¤ëŠ¦ê²Œ ê°€ì„ì„ íƒ€ëŠ” ê±¸ê¹Œìš”?
í•˜ê³  ì‹¶ì€ ì œ ë¨¸ë¦¬ëŠ” ì•„ë‹ˆì§€ë§Œ ì˜¤ëŠ˜ì„ ë”¸ì˜ ìƒì•  ì²« ì»¤íŠ¸ì— ëŒ€í•œ ê°„ë‹¨í•œ ì¼ê¸°ë¥¼ í¬ìŠ¤íŒ…í•˜ê² ìŠµë‹ˆë‹¤.

ì§€ë‚œì£¼ ê¸ˆìš”ì¼ ì˜¤í›„,
í•¨ê»˜ ëˆ„ì›Œìˆë˜ ë”¸ì•„ì´ê°€
ê°‘ìê¸° 'ì—„ë§ˆ, ë‚˜ ë¨¸ë¦¬ì¹´ë½ì´ ë„ˆë¬´ ê·€ì°®ì•„ìš”...
ë‚˜ ì´ì œ ë¨¸ë¦¬ì¹´ë½ ìë¥´ê³  ì‹¶ì–´ìš”.'ë¼ê³  í•´ì„œ
(ë¬´ë ¤ 4ë…„ ë§Œì—... ìš°ë¦¬ ë”¸ 4ì‚´!!)

ê·¸ ë§ê³¼ ë™ì‹œì—
ì£¼ì„¬ì£¼ì„¬ ì˜·ì„ ì…ê³ 
ë¯¸ìš©ì‹¤ë¡œ ì§í–‰!! í–ˆìŠµë‹ˆë‹¤.

ë¬´ë ¤ 4ë…„ ë§Œì— ìë¥´ëŠ” ê²ƒì´ë¼
ì‘ë…„ë¶€í„° ë¶€ì© ê¸¸ì–´ì§„ ë¨¸ë¦¬ì—
ì•„ì¹¨ë§ˆë‹¤ ë¹—ì§ˆë„, ë¬¶ëŠ” ê²ƒë„ ì¼ì´ê¸°ì—
ì¡°ê¸ˆë§Œ ìë¥´ìê³  ê¼¬ì…”ë„
ì ˆëŒ€ ì•ˆ ìë¥´ê² ë‹¤ê³ ,
ì•„ë¹ ë‘ ì˜¤ë¹  ë¯¸ìš©ì‹¤ì— ë”°ë¼ê°€ì„œë„
ì ˆëŒ€ ì•ˆ ìë¥¸ë‹¤ê³  í•´ì™”ì–´ì„œ
ë¨¸ë¦¬ë¥¼ ìë¥¸ë‹¤ëŠ” ë§ì´
ë„ˆë¬´ë„ˆë¬´ ë°˜ê°€ì› ì–´ìš”. :)

ë¯¸ë¦¬ ì˜ˆì•½ì„ í•˜ê³  ê°„ ê²ƒì´
ì•„ë‹ˆë¼ ì •ë§ ê¸‰í•˜ê²Œ ì™”ë”ë‹ˆ
ì—­ì‹œë‚˜ ëŒ€ê¸°ê°€ ìˆì—ˆìŠµë‹ˆë‹¤.

ì•„ë“¤ì€ ì•„ë¹ ë‘ ì•„ë¬´ë°ë‚˜ ê°€ì„œ
ì»¤íŠ¸ë¥¼ í•˜ì§€ë§Œ,
ë”¸ì€ ê·¸ë˜ë„ ì—¬ìì•„ì´ê³ 
ì²« ì»¤íŠ¸ë‹ˆ ë§Œí¼ ì˜ˆì˜ê²Œ ì˜ë¼ì£¼ê³  ì‹¶ì€
ë§ˆìŒì— ì œê°€ ì´ìš©í•˜ëŠ” ë¯¸ìš©ì‹¤ë¡œ ë°ë¦¬ê³  ê°”ì–´ìš”.
ã…ã…ã…
            """

            # ë¶ˆìš©ì–´ ëª©ë¡
            fileList = sorted(glob.glob(sysOpt['filter']['stopWordFileInfo']))
            stopWordData = pd.read_csv(fileList[0])
            stopWordList = stopWordData['word'].tolist()

            # ê¸ˆì§€ì–´ ëª©ë¡
            forbidWordList = sysOpt['filter']['forbidWordList']
            okt = Okt()
            posTagList = okt.pos(text, stem=True)

            # ëª…ì‚¬ ì¶”ì¶œ
            keywordList = [word for word, pos in posTagList if pos in ['Noun']]

            # ë¶ˆìš©ì–´ ì œê±°
            keywordList = [word for word in keywordList if word not in stopWordList and len(word) > 1]

            # ë¹ˆë„ìˆ˜ ê³„ì‚°
            keywordCnt = Counter(keywordList)
            data = pd.DataFrame(keywordCnt.items(), columns=['keyword', 'cnt'])

            pattern = re.compile("|".join(forbidWordList))
            data['type'] = data['keyword'].apply(lambda x: 'ê¸ˆì§€ì–´' if pattern.search(x) else 'ì¼ë°˜ì–´')

            forbidData = data[data['type'] == 'ê¸ˆì§€ì–´'].sort_values(by='cnt', ascending=False)
            normalData = data[data['type'] == 'ì¼ë°˜ì–´'].sort_values(by='cnt', ascending=False)
            forbidList = forbidData['keyword'].tolist()
            normalList = normalData['keyword'].tolist()

            log.info(f"[CHECK] ê¸ˆì§€ì–´ ëª©ë¡: {len(forbidList)} : {forbidList}")
            log.info(f"[CHECK] ì¼ë°˜ì–´ ëª©ë¡: {len(normalList)} : {normalList}")

            # ==========================================================================================================
            # ë„¤ì´ë²„ íŠ¸ë Œë“œ ê¸°ë°˜ ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ (ë¶„ì•¼ ì„ íƒ í•„ì—°)
            # ì •ì  í¬ë¡¤ë§
            # https://datalab.naver.com

            # í†µí•© ê²€ìƒ‰ì–´ íŠ¸ë Œë“œ https://openapi.naver.com/v1/datalab/search
            # ì‡¼í•‘ì¸ì‚¬ì´íŠ¸ https://openapi.naver.com/v1/datalab/shopping/categories
            # ==========================================================================================================
            try:
                dataL1 = pd.DataFrame()
                for idx, cateInfo in enumerate(sysOpt['colct']['naver']['cateList']):
                    params = {
                        "timeUnit": "date",
                        "cid": cateInfo['param'][0],
                    }

                    queryStr = urlencode(params)
                    url = f"{sysOpt['colct']['naver']['baseUrl']}?{queryStr}"

                    response = requests.post(url, headers=sysOpt['colct']['naver']['headers'])
                    if not (response.status_code == 200): continue

                    resData = response.json()
                    resDataL1 = resData[-1]

                    orgData = pd.DataFrame(resDataL1['ranks']).rename(
                        columns={
                            'rank': 'no'
                        }
                    )

                    orgData['type'] = 'naver'
                    orgData['cate'] = cateInfo['name']
                    orgData['dateTime'] = pd.to_datetime(resDataL1['date']).tz_localize('Asia/Seoul')
                    data = orgData[['type', 'cate', 'dateTime', 'no', 'keyword']]

                    if len(data) > 0:
                        dataL1 = pd.concat([dataL1, data])
            except Exception as e:
                log.error(f"ë„¤ì´ë²„ ê²€ìƒ‰ì–´ ìˆ˜ì§‘ ì‹¤íŒ¨ : {e}")

            # ==========================================================================================================
            # êµ¬ê¸€ íŠ¸ë Œë“œ ê¸°ë°˜ ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´ ì›¹
            # ë™ì  í¬ë¡¤ë§
            # https://trends.google.co.kr/trending?geo=KR&hl=ko
            # ==========================================================================================================
            try:
                pytrends = TrendReq(geo='ko-KR', tz=540)
                orgData = pytrends.trending_searches(pn='south_korea')

                orgDataL1 = orgData.rename(columns={0: 'keyword'})
                orgDataL1['no'] = orgDataL1.index + 1
                orgDataL1['dateTime'] = datetime.now(tz=tzKst)
                orgDataL1['type'] = 'google'
                orgDataL1['cate'] = 'ì „ì²´'

                data = orgDataL1[['type', 'cate', 'dateTime', 'no', 'keyword']]
                if len(data) > 0:
                    dataL1 = pd.concat([dataL1, data])
            except Exception as e:
                log.error(f"êµ¬ê¸€ ê²€ìƒ‰ì–´ ìˆ˜ì§‘ ì‹¤íŒ¨ : {e}")

            # ==========================================================================================================
            # ì›¨ì–´ì´ì¦ˆí¬ìŠ¤íŠ¸ ê¸°ë°˜ ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´
            # ì •ì  í¬ë¡¤ë§
            # ==========================================================================================================
            try:
                response = requests.get(sysOpt['colct']['whereispost']['baseUrl'], headers=sysOpt['colct']['whereispost']['headers'])
                response.raise_for_status()

                soup = BeautifulSoup(response.text, 'html.parser')
                lxml = etree.HTML(str(soup))

                try:
                    tag = lxml.xpath('/html/body/content/div/div/div/div[1]/text()')[0]
                    match = re.search(r'\d{4}-\d{2}-\d{2} \d{2}:\d{2}', tag.strip())
                    sDateTime = None if match is None else match.group(0)
                    dtDateTime = pd.to_datetime(sDateTime).tz_localize('Asia/Seoul')
                except Exception:
                    dtDateTime = None
                log.info(f'[CHECK] dtDateTime : {dtDateTime}')

                noList = soup.find('ul', {'class': 'list-group bg-white'}).find_all("span", {'class': 'rank daum_color'})
                keywordList = soup.find('ul', {'class': 'list-group bg-white'}).find_all("span", {'class': 'keyword'})

                data = pd.DataFrame()
                for noInfo, keywordInfo in zip(noList, keywordList):
                    try:
                        no = None if noInfo is None or len(noInfo) < 1 else noInfo.text.strip()
                        keyword = None if keywordInfo is None or len(keywordInfo) < 1 else keywordInfo.text.strip()

                        dict = {
                            'type': ['whereispost'],
                            'cate': 'ì „ì²´',
                            'dateTime': [dtDateTime],
                            'no': [no],
                            'keyword': [keyword],
                        }

                        data = pd.concat([data, pd.DataFrame.from_dict(dict)])

                    except Exception:
                        pass

                if len(data) > 0:
                    dataL1 = pd.concat([dataL1, data])

            except Exception as e:
                log.error(f"ì›¨ì–´ì´ì¦ˆí¬ìŠ¤íŠ¸ ê²€ìƒ‰ì–´ ìˆ˜ì§‘ ì‹¤íŒ¨ : {e}")

            # ==========================================================================================================
            # ì´ì§€ë¯¸ë„· ê¸°ë°˜ ì‹¤ì‹œê°„ ê²€ìƒ‰ì–´
            # ì •ì  í¬ë¡¤ë§
            # ==========================================================================================================
            try:
                response = requests.get(sysOpt['colct']['ezme']['baseUrl'], headers=sysOpt['colct']['ezme']['headers'])
                response.raise_for_status()

                soup = BeautifulSoup(response.text, 'html.parser')

                try:
                    tag = soup.find('div', {'id': 'content'}).find('small')
                    sDateTime = None if tag is None or len(tag) < 1 else tag.text.strip()
                    dtDateTime = pd.to_datetime(sDateTime).tz_localize('Asia/Seoul')
                except Exception:
                    dtDateTime = None
                log.info(f'[CHECK] dtDateTime : {dtDateTime}')

                noList = soup.find('div', {'id': 'content'}).find_all("span", {'class': 'rank_no'})
                keywordList = soup.find('div', {'id': 'content'}).find_all("span", {'class': 'rank_word'})

                data = pd.DataFrame()
                for noInfo, keywordInfo in zip(noList, keywordList):
                    try:
                        no = None if noInfo is None or len(noInfo) < 1 else noInfo.text.strip(".").strip()
                        keyword = None if keywordInfo is None or len(keywordInfo) < 1 else keywordInfo.find('a').text.strip()

                        dict = {
                            'type': ['ezme'],
                            'cate': 'ì „ì²´',
                            'dateTime': [dtDateTime],
                            'no': [no],
                            'keyword': [keyword],
                        }

                        data = pd.concat([data, pd.DataFrame.from_dict(dict)])
                    except Exception:
                        pass

                if len(data) > 0:
                    dataL1 = pd.concat([dataL1, data])
            except Exception as e:
                log.error(f"ì´ì§€ë¯¸ë„· ê²€ìƒ‰ì–´ ìˆ˜ì§‘ ì‹¤íŒ¨ : {e}")

            # ==========================================================================================================
            # ìë£Œ ì €ì¥
            # ==========================================================================================================
            dataL2 = dataL1.reset_index(drop=True)

        except Exception as e:
            log.error(f"Exception : {e}")
            raise e
        finally:
            log.info('[END] {}'.format("exec"))

# ================================================
# 3. ì£¼ í”„ë¡œê·¸ë¨
# ================================================
if __name__ == '__main__':

    print('[START] {}'.format("main"))

    try:

        # íŒŒì´ì¬ ì‹¤í–‰ ì‹œ ì „ë‹¬ì¸ìë¥¼ ì´ˆê¸° í™˜ê²½ë³€ìˆ˜ ì„¤ì •
        inParams = { }
        print("[CHECK] inParams : {}".format(inParams))

        # ë¶€ í”„ë¡œê·¸ë¨ í˜¸ì¶œ
        subDtaProcess = DtaProcess(inParams)

        subDtaProcess.exec()

    except Exception as e:
        print(traceback.format_exc())
        sys.exit(1)

    finally:
        print('[END] {}'.format("main"))

# ==========================================================================================================
# pipeline
# ==========================================================================================================
# import transformers
# import torch
#
# model_id = "MLP-KTLim/llama-3-Korean-Bllossom-8B"
#
# pipeline = transformers.pipeline(
#     "text-generation",
#     model=model_id,
#     model_kwargs={"torch_dtype": torch.bfloat16},
#     device_map="auto",
# )
#
# PROMPT = '''You are a helpful AI assistant. Please answer the user's questions kindly. ë‹¹ì‹ ì€ ìœ ëŠ¥í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì¹œì ˆí•˜ê²Œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.'''
# instruction = "ëŒ€í•œë¯¼êµ­ì˜ ì—­ì‚¬ ì†Œê°œí•´ì¤˜ "
#
# messages = [
#     {"role": "system", "content": f"{PROMPT}"},
#     {"role": "user", "content": f"{instruction[:2000]}"}
# ]
#
# prompt = pipeline.tokenizer.apply_chat_template(
#     messages,
#     tokenize=False,
#     add_generation_prompt=True
# )
#
# terminators = [
#     pipeline.tokenizer.eos_token_id,
#     pipeline.tokenizer.convert_tokens_to_ids("<|eot_id|>")
# ]
#
# outputs = pipeline(
#     prompt,
#     max_new_tokens=2048,
#     eos_token_id=terminators,
#     do_sample=True,
#     temperature=0.6,
#     top_p=0.9
# )
#
# print(outputs[0]["generated_text"][len(prompt):])
#
# # ==========================================================================================================
# # ë¬´ë£Œ GPT
# # ==========================================================================================================
# # # GPT4All	ë¡œì»¬ì—ì„œ ì‹¤í–‰ ê°€ëŠ¥í•œ ê²½ëŸ‰ GPT	âœ… ë¬´ë£Œ
# from gpt4all import GPT4All
# #
# # # GPT ëª¨ë¸ ë¡œë“œ
# # # model = GPT4All("ggml-model-gpt4all-falcon-q4_0.bin")  # deprecated
# # # model = GPT4All("orca-mini-3b-gguf2-q4_0.gguf") # ì¢€ ë” ê°€ë²¼ìš´ ëª¨ë¸
# model = GPT4All("mistral-7b-instruct-v0.1.Q4_0.gguf")  # ì¶”ì²œ: ë¹ ë¥´ê³  ì„±ëŠ¥ ì¢‹ì€ ëª¨ë¸
# # model2 = GPT4All("Meta-Llama-3-8B-Instruct.Q4_0.gguf")
#
# for token in model.generate("Tell me a story.", streaming=True):
#     print(token, end="", flush=True)
#
# # with model.chat_session():
# #     print(model.generate("quadratic formula"))
# #
# # with model2.chat_session():
# #     print(model2.generate("quadratic formula"))
# #
# # # í•œë²ˆì— ì—¬ëŸ¬ ë©”ì‹œì§€ ì²˜ë¦¬ (ì±—ë´‡ ëª¨ë“œ).  n_predict ì¡°ì ˆ
# # def generate_responses(messages, model, n_predict=128):
# #     with model.chat_session():
# #         responses = []
# #         for message in messages:
# #             response = model.generate(message, max_tokens=n_predict)
# #             responses.append(response)
# #     return responses
# #
# # #  ëŒ€í™” ì˜ˆì œ (ì±—ë´‡)
# # messages = [
# #     # "ì•ˆë…•, ë„Œ ëˆ„êµ¬ë‹ˆ?",
# #     # "í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì•¼?",
# #     # "ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œ?",
# #     "ê¸ˆì§€ì–´ ëª©ë¡ì„ ì•Œë ¤ì¤˜",
# #     "ê¸ˆì§€ì–´ê°€ í¬í•¨ëœ ë¬¸ì¥ ì˜ˆì‹œë¥¼ ë§Œë“¤ì–´ì¤˜",
# # ]
# #
# # responses = generate_responses(messages, model)
# #
# # print("\n-- ì±—ë´‡ ëŒ€í™” --")
# # for i, (message, response) in enumerate(zip(messages, responses)):
# #     print(f"User {i + 1}: {message}")
# #     print(f"Bot  {i + 1}: {response}")
# #     print("-" * 20)
# #
# # # ë‹¨ì¼ í”„ë¡¬í”„íŠ¸ ìƒì„± (ì¼ë°˜ ëª¨ë“œ)
# # prompt = "ê¸ˆì§€ì–´ í•„í„°ë§ ì‹œìŠ¤í…œì„ ë§Œë“œëŠ” ë°©ë²•ì— ëŒ€í•œ íŒŒì´ì¬ ì½”ë“œë¥¼ ì‘ì„±í•´ì¤˜. "
# # prompt += "scikit-learn, konlpyë¥¼ ì‚¬ìš©í•˜ê³ , "
# # prompt += "ê¸ˆì§€ì–´ ëª©ë¡ì€ ['ë°”ë³´', 'ë©ì²­ì´', 'ë‚˜ìœë†ˆ']ìœ¼ë¡œ í•´ì¤˜."
# #
# # print("\n-- ë‹¨ì¼ í”„ë¡¬í”„íŠ¸ ìƒì„± --")
# # output = model.generate(prompt, max_tokens=1024)  # max_tokens: ìµœëŒ€ ìƒì„± ê¸¸ì´
# # print(output)
# #
# # #  ê¸ˆì§€ì–´ í•„í„°ë§ (ì±—ë´‡ ëª¨ë“œ í™œìš©)
# # def filter_text(text, model):
# #     with model.chat_session():
# #         system_template = "You are a helpful assistant that filters forbidden words.  If the text contains a forbidden word, respond with 'Filtered', otherwise respond with 'OK'."  # system prompt
# #
# #         response = model.generate(f"{system_template}\nUser: {text}", max_tokens=10)
# #
# #     if "Filtered" in response:
# #         return "Filtered"
# #     else:
# #         return "OK"
# #
# # print("\n-- ê¸ˆì§€ì–´ í•„í„°ë§ --")
# #
# # test_sentences = [
# #     "ì´ê²ƒì€ í…ŒìŠ¤íŠ¸ ë¬¸ì¥ì…ë‹ˆë‹¤.",
# #     "ì € ë…€ì„ì€ ì •ë§ ë‚˜ìœë†ˆì´ì•¼.",
# #     "ë°”ë³´ëŠ” ì•„ë‹ˆì§€ë§Œ, ì¡°ê¸ˆ ë©ì²­ì´ ê°™ì•„.",
# #     "ì¢‹ì€ í•˜ë£¨ ë˜ì„¸ìš”!",
# # ]
# #
# # for sentence in test_sentences:
# #     result = filter_text(sentence, model)
# #     print(f"'{sentence}' -> {result}")
#
#
#
# # LLaMA (Meta AI)	Facebook AIì—ì„œ ì œê³µí•˜ëŠ” LLM	âœ… ë¬´ë£Œ
# from transformers import AutoTokenizer, AutoModelForCausalLM
# import torch
#
# from transformers import pipeline
#
# # messages = [
# #     {"role": "user", "content": "Who are you?"},
# # ]
# # pipe = pipeline("text-generation", model="meta-llama/Llama-2-7b-chat-hf")
# # pipe(messages)
#
#
# # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
# # tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-chat-hf", token=hugFaceToken)
# # model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-chat-hf", token=hugFaceToken)
#
# # hugFaceToken = None
#
# # Load model directly
# from transformers import AutoTokenizer, AutoModelForCausalLM
#
#
#
# import os
# from huggingface_hub import constants
#
# # ë°©ë²• 2: huggingface_hub ë¼ì´ë¸ŒëŸ¬ë¦¬ ìƒìˆ˜ ì‚¬ìš© (ë” ì•ˆì •ì )
# print(constants.HF_HUB_CACHE)
# print(constants.HUGGINGFACE_HUB_CACHE)  # ì´ì „ ë²„ì „ê³¼ì˜ í˜¸í™˜ì„±ì„ ìœ„í•´ ì¡´ì¬
#
#
# tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-chat-hf", token=hugFaceToken)
# # model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-chat-hf", token=hugFaceToken)
# model = AutoModelForCausalLM.from_pretrained("TheBloke/Llama-2-7B-Chat-GGM", token=hugFaceToken)
#
# # tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-chat-hf", token=hugFaceToken)
# # model = AutoModelForCausalLM.from_pretrained(
# #     "meta-llama/Llama-2-7b-chat-hf",
# #     token=hugFaceToken,
# #     device_map="auto",
# #     load_in_4bit=True,
# #     low_cpu_mem_usage=True,
# # )
#
# # ê¸ˆì§€ì–´ í•„í„°ë§ í•¨ìˆ˜
# def predict_llama(text):
#     prompt = f"ì´ í…ìŠ¤íŠ¸ê°€ ê¸ˆì§€ì–´ë¥¼ í¬í•¨í•˜ëŠ”ì§€ íŒë‹¨í•´ì¤˜. ê¸ˆì§€ì–´ í¬í•¨ ì‹œ 'ğŸš¨ ê¸ˆì§€ì–´ í¬í•¨', í¬í•¨ë˜ì§€ ì•Šìœ¼ë©´ 'âœ… ì •ìƒ í…ìŠ¤íŠ¸'ë¼ê³  ë‹µë³€í•´.\n\n{text}"
#     inputs = tokenizer(prompt, return_tensors="pt")
#     with torch.no_grad():
#         outputs = model.generate(**inputs)
#     return tokenizer.decode(outputs[0])
#
# # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
# print(predict_llama("ì´ê±° ì™„ì „ ì‚¬ê¸°ì•¼!"))
# print(predict_llama("ì¢‹ì€ ì œí’ˆì´ë„¤ìš”!"))
# from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
# import torch
#
# # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ (GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ ìë™ ì‚¬ìš©)
# model_name = "EleutherAI/polyglot-ko-1.3b"  # ì‘ì€ ëª¨ë¸.  ë” í° ëª¨ë¸: EleutherAI/polyglot-ko-5.8b, EleutherAI/polyglot-ko-12.8b
# # model_name = "beomi/kollama-12.8b-v2"  # KoLLaMA (í•œêµ­ì–´, LLaMA ê¸°ë°˜)
# # model_name = "mistralai/Mistral-7B-Instruct-v0.2"  # Mistral (ì˜ì–´, í•œêµ­ì–´ë„ ì¼ë¶€ ê°€ëŠ¥)
#
# try:
#     tokenizer = AutoTokenizer.from_pretrained(model_name)
#     model = AutoModelForCausalLM.from_pretrained(
#         model_name,
#         device_map="auto",  # GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ ìë™ìœ¼ë¡œ ì‚¬ìš©, ì—†ìœ¼ë©´ CPU
#         # torch_dtype=torch.float16,  # FP16 ì‚¬ìš© (GPU ë©”ëª¨ë¦¬ ì ˆì•½, ì†ë„ í–¥ìƒ) - transformers ë²„ì „ì— ë”°ë¼ ì§€ì› ì•ˆë  ìˆ˜ ìˆìŒ.
#         low_cpu_mem_usage=True,
#     )
#
# except Exception as e:
#     print(f"ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
#     print("ëª¨ë¸ ì´ë¦„ì´ ì •í™•í•œì§€, transformers, torch, accelerate, sentencepiece ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
#     exit()
#
# # í”„ë¡¬í”„íŠ¸ ìƒì„± (ì˜ˆì‹œ)
# def generate_prompt(instruction, input_text=""):
#     # í•œêµ­ì–´ ëª¨ë¸ì— ë§ëŠ” í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‚¬ìš© (ëª¨ë¸ë§ˆë‹¤ ë‹¤ë¦„)
#     if "polyglot-ko" in model_name.lower():  # polyglot
#         prompt = f"### ì§ˆë¬¸: {instruction}\n\n### ë‹µë³€:"
#         if input_text:
#             prompt = f"### ì§ˆë¬¸: {instruction}\n\n### ì…ë ¥: {input_text}\n\n### ë‹µë³€:"
#     elif "kollama" in model_name.lower():  # KoLLaMA
#         prompt = f"""ì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ëª…ë ¹ì–´ì…ë‹ˆë‹¤. ìš”ì²­ì„ ì ì ˆíˆ ì™„ë£Œí•˜ëŠ” ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.
#
# ### ëª…ë ¹ì–´:
# {instruction}
# ### ì‘ë‹µ:"""
#         if input_text:
#             prompt = f"""ì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ëª…ë ¹ì–´ì™€ ì»¨í…ìŠ¤íŠ¸ë¡œ êµ¬ì„±ëœ ì…ë ¥ì…ë‹ˆë‹¤. ìš”ì²­ì„ ì ì ˆíˆ ì™„ë£Œí•˜ëŠ” ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.
#
# ### ëª…ë ¹ì–´:
# {instruction}
#
# ### ì…ë ¥:
# {input_text}
#
# ### ì‘ë‹µ:"""
#
#     elif "mistral" in model_name.lower():  # Mistral
#         prompt = f"[INST] {instruction} [/INST]"
#         if input_text:
#             prompt = f"[INST] {instruction}\n\n{input_text} [/INST]"  # input ì˜ˆì‹œ
#
#     else:
#         # ê¸°ë³¸ í…œí”Œë¦¿ (ì˜ì–´ ëª¨ë¸ì— ì í•©)
#         prompt = f"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\n{instruction}\n\n### Response:"
#         if input_text:
#             prompt = f"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\n{instruction}\n\n### Input:\n{input_text}\n\n### Response:"
#
#     return prompt
#
# # í…ìŠ¤íŠ¸ ìƒì„± í•¨ìˆ˜
# def generate_text(instruction, input_text="", max_new_tokens=128, temperature=0.7, top_p=0.9,
#                   repetition_penalty=1.2):
#
#     prompt = generate_prompt(instruction, input_text)
#
#     inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
#
#     with torch.no_grad():  # Gradient ê³„ì‚° ë¹„í™œì„±í™” (ë©”ëª¨ë¦¬ ì ˆì•½, ì†ë„ í–¥ìƒ)
#         generated_ids = model.generate(
#             **inputs,
#             max_new_tokens=max_new_tokens,  # ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜
#             temperature=temperature,  # ë†’ì„ìˆ˜ë¡ ë‹¤ì–‘í•œ ê²°ê³¼, ë‚®ì„ìˆ˜ë¡ ê²°ì •ë¡ ì  ê²°ê³¼
#             top_p=top_p,  # Nucleus Sampling: í™•ë¥ ì´ ë†’ì€ í† í° ì¤‘ì—ì„œ ì„ íƒ
#             repetition_penalty=repetition_penalty,  # ë°˜ë³µ ê°ì†Œ (ê°’ì´ í´ìˆ˜ë¡ ë°˜ë³µ ì¤„ì–´ë“¬)
#             do_sample=True,  # ìƒ˜í”Œë§ ê¸°ë°˜ ìƒì„±
#             pad_token_id=tokenizer.eos_token_id,  # íŒ¨ë”© í† í°
#             # eos_token_id=tokenizer.eos_token_id, # <eos>í† í°ì´ ìƒì„±ë˜ë©´, ìƒì„± ì¢…ë£Œ. (ëª¨ë¸ì— ë”°ë¼ ì„¤ì •)
#             # early_stopping=True,  # eos í† í° ë‚˜ì˜¤ë©´ ìƒì„± early stop
#
#         )
#
#     generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
#
#     # í”„ë¡¬í”„íŠ¸ ì´í›„ì˜ í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜ (ëª¨ë¸ì— ë”°ë¼ ì¡°ì • í•„ìš”)
#     answer = generated_text[len(prompt):]
#
#     return answer
#
# # --- ì‚¬ìš© ì˜ˆì‹œ ---
#
# # 1. ê°„ë‹¨í•œ ì§ˆë¬¸-ë‹µë³€
# instruction = "í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?"
# response = generate_text(instruction)
# print(f"ì§ˆë¬¸: {instruction}\në‹µë³€: {response}\n")
#
# # 2. í…ìŠ¤íŠ¸ ìš”ì•½
# instruction = "ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•´ ì£¼ì„¸ìš”."
# input_text = """
# ì¸ê³µì§€ëŠ¥(AI)ì€ 21ì„¸ê¸° ê°€ì¥ í˜ì‹ ì ì¸ ê¸°ìˆ  ì¤‘ í•˜ë‚˜ë¡œ, ... (ê¸´ í…ìŠ¤íŠ¸) ...
# """
# response = generate_text(instruction, input_text)
# print(f"ìš”ì•½:\n{response}\n")
#
# # 3. í…ìŠ¤íŠ¸ ìƒì„± (ìŠ¤í† ë¦¬, ì‹œ ë“±)
# instruction = "ë°”ë‹·ê°€ì—ì„œ í•´ì§ˆë…˜ í’ê²½ì„ ë¬˜ì‚¬í•˜ëŠ” ì‹œë¥¼ ì¨ ì£¼ì„¸ìš”."
# response = generate_text(instruction, max_new_tokens=256)  # max_new_tokens ëŠ˜ë¦¼
# print(f"ì‹œ:\n{response}\n")
#
# # 4. ë²ˆì—­ (í•œêµ­ì–´ -> ì˜ì–´)
# instruction = "ë‹¤ìŒ ë¬¸ì¥ì„ ì˜ì–´ë¡œ ë²ˆì—­í•´ ì£¼ì„¸ìš”."
# input_text = "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì •ë§ ì¢‹ìŠµë‹ˆë‹¤."
# response = generate_text(instruction, input_text)
# print(f"ë²ˆì—­: {response}\n")
#
# # 5. ì½”ë“œ ìƒì„±
# instruction = "íŒŒì´ì¬ìœ¼ë¡œ ê°„ë‹¨í•œ ì›¹ ì„œë²„ë¥¼ ë§Œë“œëŠ” ì½”ë“œë¥¼ ì‘ì„±í•´ ì£¼ì„¸ìš”."
# response = generate_text(instruction, max_new_tokens=512)  # ì½”ë“œ ìƒì„±ì´ë¯€ë¡œ max_new_tokensì„ ëŠ˜ë¦¼
# print(f"ì½”ë“œ:\n{response}\n")
#
# # 6. ê¸ˆì§€ì–´ í•„í„°ë§ (ë¶„ë¥˜)
# instruction = "ë‹¤ìŒ ë¬¸ì¥ì— ìš•ì„¤ì´ë‚˜ ë¹„ì†ì–´ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ íŒë³„í•´ ì£¼ì„¸ìš”. í¬í•¨ë˜ì–´ ìˆë‹¤ë©´ 'ìœ í•´í•¨', í¬í•¨ë˜ì–´ ìˆì§€ ì•Šë‹¤ë©´ 'ì•ˆì „í•¨'ì´ë¼ê³  ì¶œë ¥í•˜ì„¸ìš”."
# input_text = "ì´ ì„œë¹„ìŠ¤ëŠ” ì •ë§ ìµœê³ ì˜ˆìš”!"
# response = generate_text(instruction, input_text, max_new_tokens=10)  # ì§§ì€ ì‘ë‹µì´ë¯€ë¡œ max_tokens ì¤„ì„
# print(f"'{input_text}' íŒë³„: {response}")
#
# input_text = "ì´ ì„œë¹„ìŠ¤ëŠ” ì •ë§ ê°œì“°ë ˆê¸°ê°™ì•„ìš”."  # ìš•ì„¤ í¬í•¨
# response = generate_text(instruction, input_text, max_new_tokens=10)
# print(f"'{input_text}' íŒë³„: {response}")