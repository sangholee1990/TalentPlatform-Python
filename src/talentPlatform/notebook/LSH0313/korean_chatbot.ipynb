{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'kcbert': 'beomi@github 님이 만드신 KcBERT 학습데이터',\n 'korean_chatbot_data': 'songys@github 님이 만드신 챗봇 문답 데이터',\n 'korean_hate_speech': '{inmoonlight,warnikchow,beomi}@github 님이 만드신 혐오댓글데이터',\n 'korean_parallel_koen_news': 'jungyeul@github 님이 만드신 병렬 말뭉치',\n 'korean_petitions': 'lovit@github 님이 만드신 2017.08 ~ 2019.03 청와대 청원데이터',\n 'kornli': 'KakaoBrain 에서 제공하는 Natural Language Inference (NLI) 데이터',\n 'korsts': 'KakaoBrain 에서 제공하는 Semantic Textual Similarity (STS) 데이터',\n 'kowikitext': 'lovit@github 님이 만드신 wikitext 형식의 한국어 위키피디아 데이터',\n 'namuwikitext': 'lovit@github 님이 만드신 wikitext 형식의 나무위키 데이터',\n 'naver_changwon_ner': '네이버 + 창원대 NER shared task data',\n 'nsmc': 'e9t@github 님이 만드신 Naver sentiment movie corpus v1.0',\n 'question_pair': 'songys@github 님이 만드신 질문쌍(Paired Question v.2)',\n 'modu_news': '국립국어원에서 만든 모두의 말뭉치: 뉴스 말뭉치',\n 'modu_messenger': '국립국어원에서 만든 모두의 말뭉치: 메신저 말뭉치',\n 'modu_mp': '국립국어원에서 만든 모두의 말뭉치: 형태 분석 말뭉치',\n 'modu_ne': '국립국어원에서 만든 모두의 말뭉치: 개체명 분석 말뭉치',\n 'modu_spoken': '국립국어원에서 만든 모두의 말뭉치: 구어 말뭉치',\n 'modu_web': '국립국어원에서 만든 모두의 말뭉치: 웹 말뭉치',\n 'modu_written': '국립국어원에서 만든 모두의 말뭉치: 문어 말뭉치',\n 'open_subtitles': 'Open parallel corpus (OPUS) 에서 제공하는 영화 자막 번역 병렬 말뭉치',\n 'aihub_translation': 'AI Hub 에서 제공하는 번역용 병렬 말뭉치 (구어 + 대화 + 뉴스 + 한국문화 + 조례 + 지자체웹사이트)',\n 'aihub_spoken_translation': 'AI Hub 에서 제공하는 번역용 병렬 말뭉치 (구어)',\n 'aihub_conversation_translation': 'AI Hub 에서 제공하는 번역용 병렬 말뭉치 (대화)',\n 'aihub_news_translation': 'AI Hub 에서 제공하는 번역용 병렬 말뭉치 (뉴스)',\n 'aihub_korean_culture_translation': 'AI Hub 에서 제공하는 번역용 병렬 말뭉치 (한국문화)',\n 'aihub_decree_translation': 'AI Hub 에서 제공하는 번역용 병렬 말뭉치 (조례)',\n 'aihub_government_website_translation': 'AI Hub 에서 제공하는 번역용 병렬 말뭉치 (지자체웹사이트)'}"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install korpora\n",
    "from Korpora import Korpora\n",
    "Korpora.corpus_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : songys@github\n",
      "    Repository : https://github.com/songys/Chatbot_data\n",
      "    References :\n",
      "\n",
      "    Chatbot_data_for_Korean v1.0\n",
      "      1. 챗봇 트레이닝용 문답 페어 11,876개\n",
      "      2. 일상다반사 0, 이별(부정) 1, 사랑(긍정) 2로 레이블링\n",
      "    자세한 내용은 위의 repository를 참고하세요.\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 404: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mHTTPError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-33-d991f6144395>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mkorean_chatbot_data\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mKorpora\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'korean_chatbot_data'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\py36\\lib\\site-packages\\Korpora\\loader.py\u001B[0m in \u001B[0;36mload\u001B[1;34m(cls, corpus_names, root_dir, force_download)\u001B[0m\n\u001B[0;32m     45\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mreturn_single\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     46\u001B[0m             \u001B[0mcorpus_names\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mcorpus_names\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 47\u001B[1;33m         \u001B[0mcorpora\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mKORPUS\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mcorpus_name\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mroot_dir\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mforce_download\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mcorpus_name\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mcorpus_names\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     48\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mreturn_single\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     49\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mcorpora\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\py36\\lib\\site-packages\\Korpora\\loader.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     45\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mreturn_single\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     46\u001B[0m             \u001B[0mcorpus_names\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mcorpus_names\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 47\u001B[1;33m         \u001B[0mcorpora\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mKORPUS\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mcorpus_name\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mroot_dir\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mforce_download\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mcorpus_name\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mcorpus_names\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     48\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mreturn_single\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     49\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mcorpora\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\py36\\lib\\site-packages\\Korpora\\korpus_chatbot_data.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, root_dir, force_download)\u001B[0m\n\u001B[0;32m     42\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mroot_dir\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     43\u001B[0m             \u001B[0mroot_dir\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdefault_korpora_path\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 44\u001B[1;33m         \u001B[0mfetch_chatbot\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mroot_dir\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mforce_download\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     45\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     46\u001B[0m         \u001B[0mlocal_path\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mabspath\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mroot_dir\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mKOREAN_CHATBOT_FETCH_INFORMATION\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'destination'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\py36\\lib\\site-packages\\Korpora\\korpus_chatbot_data.py\u001B[0m in \u001B[0;36mfetch_chatbot\u001B[1;34m(root_dir, force_download)\u001B[0m\n\u001B[0;32m     71\u001B[0m         \u001B[0mdestination\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0minformation\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'destination'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     72\u001B[0m         \u001B[0mlocal_path\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mabspath\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mroot_dir\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdestination\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 73\u001B[1;33m         \u001B[0mfetch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0murl\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlocal_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'korean_chatbot_data'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mforce_download\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minformation\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'method'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\py36\\lib\\site-packages\\Korpora\\utils.py\u001B[0m in \u001B[0;36mfetch\u001B[1;34m(remote_path, local_path, corpus_name, force_download, method)\u001B[0m\n\u001B[0;32m    213\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    214\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mmethod\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;34m\"download\"\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 215\u001B[1;33m         \u001B[0mweb_download\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mremote_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdestination\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcorpus_name\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mforce_download\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    216\u001B[0m     \u001B[1;32melif\u001B[0m \u001B[0mmethod\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;34m\"google_drive\"\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    217\u001B[0m         \u001B[0mgoogle_drive_download\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mremote_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdestination\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcorpus_name\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mforce_download\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\py36\\lib\\site-packages\\Korpora\\utils.py\u001B[0m in \u001B[0;36mweb_download\u001B[1;34m(url, local_path, corpus_name, force_download)\u001B[0m\n\u001B[0;32m    108\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    109\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0mweb_download\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0murl\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlocal_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcorpus_name\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m''\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mforce_download\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 110\u001B[1;33m     \u001B[0msite\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mrequest\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0murlopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0murl\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    111\u001B[0m     \u001B[0mmeta\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msite\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minfo\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    112\u001B[0m     \u001B[0mremote_size\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmeta\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'Content-Length'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\py36\\lib\\urllib\\request.py\u001B[0m in \u001B[0;36murlopen\u001B[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001B[0m\n\u001B[0;32m    221\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    222\u001B[0m         \u001B[0mopener\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_opener\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 223\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mopener\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0murl\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtimeout\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    224\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    225\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0minstall_opener\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mopener\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\py36\\lib\\urllib\\request.py\u001B[0m in \u001B[0;36mopen\u001B[1;34m(self, fullurl, data, timeout)\u001B[0m\n\u001B[0;32m    530\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mprocessor\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mprocess_response\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mprotocol\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    531\u001B[0m             \u001B[0mmeth\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mgetattr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mprocessor\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmeth_name\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 532\u001B[1;33m             \u001B[0mresponse\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmeth\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mreq\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mresponse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    533\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    534\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mresponse\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\py36\\lib\\urllib\\request.py\u001B[0m in \u001B[0;36mhttp_response\u001B[1;34m(self, request, response)\u001B[0m\n\u001B[0;32m    640\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;33m(\u001B[0m\u001B[1;36m200\u001B[0m \u001B[1;33m<=\u001B[0m \u001B[0mcode\u001B[0m \u001B[1;33m<\u001B[0m \u001B[1;36m300\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    641\u001B[0m             response = self.parent.error(\n\u001B[1;32m--> 642\u001B[1;33m                 'http', request, response, code, msg, hdrs)\n\u001B[0m\u001B[0;32m    643\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    644\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mresponse\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\py36\\lib\\urllib\\request.py\u001B[0m in \u001B[0;36merror\u001B[1;34m(self, proto, *args)\u001B[0m\n\u001B[0;32m    568\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mhttp_err\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    569\u001B[0m             \u001B[0margs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mdict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'default'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'http_error_default'\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0morig_args\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 570\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_call_chain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    571\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    572\u001B[0m \u001B[1;31m# XXX probably also want an abstract factory that knows when it makes\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\py36\\lib\\urllib\\request.py\u001B[0m in \u001B[0;36m_call_chain\u001B[1;34m(self, chain, kind, meth_name, *args)\u001B[0m\n\u001B[0;32m    502\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mhandler\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mhandlers\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    503\u001B[0m             \u001B[0mfunc\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mgetattr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mhandler\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmeth_name\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 504\u001B[1;33m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    505\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mresult\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    506\u001B[0m                 \u001B[1;32mreturn\u001B[0m \u001B[0mresult\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\py36\\lib\\urllib\\request.py\u001B[0m in \u001B[0;36mhttp_error_default\u001B[1;34m(self, req, fp, code, msg, hdrs)\u001B[0m\n\u001B[0;32m    648\u001B[0m \u001B[1;32mclass\u001B[0m \u001B[0mHTTPDefaultErrorHandler\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mBaseHandler\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    649\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mhttp_error_default\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreq\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfp\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcode\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmsg\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhdrs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 650\u001B[1;33m         \u001B[1;32mraise\u001B[0m \u001B[0mHTTPError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mreq\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfull_url\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcode\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmsg\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhdrs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfp\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    651\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    652\u001B[0m \u001B[1;32mclass\u001B[0m \u001B[0mHTTPRedirectHandler\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mBaseHandler\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mHTTPError\u001B[0m: HTTP Error 404: Not Found"
     ]
    }
   ],
   "source": [
    "# korean_chatbot_data = Korpora.load('korean_chatbot_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                             Q                         A  label\n0                       12시 땡!                하루가 또 가네요.      0\n1                  1지망 학교 떨어졌어                 위로해 드립니다.      0\n2                 3박4일 놀러가고 싶다               여행은 언제나 좋죠.      0\n3              3박4일 정도 놀러가고 싶다               여행은 언제나 좋죠.      0\n4                      PPL 심하네                눈살이 찌푸려지죠.      0\n...                        ...                       ...    ...\n11818           훔쳐보는 것도 눈치 보임.        티가 나니까 눈치가 보이는 거죠!      2\n11819           훔쳐보는 것도 눈치 보임.             훔쳐보는 거 티나나봐요.      2\n11820              흑기사 해주는 짝남.                    설렜겠어요.      2\n11821  힘든 연애 좋은 연애라는게 무슨 차이일까?  잘 헤어질 수 있는 사이 여부인 거 같아요.      2\n11822               힘들어서 결혼할까봐        도피성 결혼은 하지 않길 바라요.      2\n\n[11823 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Q</th>\n      <th>A</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>12시 땡!</td>\n      <td>하루가 또 가네요.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1지망 학교 떨어졌어</td>\n      <td>위로해 드립니다.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3박4일 놀러가고 싶다</td>\n      <td>여행은 언제나 좋죠.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3박4일 정도 놀러가고 싶다</td>\n      <td>여행은 언제나 좋죠.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>PPL 심하네</td>\n      <td>눈살이 찌푸려지죠.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>SD카드 망가졌어</td>\n      <td>다시 새로 사는 게 마음 편해요.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>SD카드 안돼</td>\n      <td>다시 새로 사는 게 마음 편해요.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>SNS 맞팔 왜 안하지ㅠㅠ</td>\n      <td>잘 모르고 있을 수도 있어요.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>SNS 시간낭비인 거 아는데 매일 하는 중</td>\n      <td>시간을 정하고 해보세요.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>SNS 시간낭비인데 자꾸 보게됨</td>\n      <td>시간을 정하고 해보세요.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>SNS보면 나만 빼고 다 행복해보여</td>\n      <td>자랑하는 자리니까요.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>가끔 궁금해</td>\n      <td>그 사람도 그럴 거예요.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>가끔 뭐하는지 궁금해</td>\n      <td>그 사람도 그럴 거예요.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>가끔은 혼자인게 좋다</td>\n      <td>혼자를 즐기세요.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>가난한 자의 설움</td>\n      <td>돈은 다시 들어올 거예요.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>가만 있어도 땀난다</td>\n      <td>땀을 식혀주세요.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>가상화폐 쫄딱 망함</td>\n      <td>어서 잊고 새출발 하세요.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>가스불 켜고 나갔어</td>\n      <td>빨리 집에 돌아가서 끄고 나오세요.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>가스불 켜놓고 나온거 같아</td>\n      <td>빨리 집에 돌아가서 끄고 나오세요.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>가스비 너무 많이 나왔다.</td>\n      <td>다음 달에는 더 절약해봐요.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>가스비 비싼데 감기 걸리겠어</td>\n      <td>따뜻하게 사세요!</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>가스비 장난 아님</td>\n      <td>다음 달에는 더 절약해봐요.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>가장 확실한 건 뭘까?</td>\n      <td>가장 확실한 시간은 오늘이에요. 어제와 내일을 놓고 고민하느라 시간을 낭비하지 마세요.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>가족 여행 가기로 했어</td>\n      <td>온 가족이 모두 마음에 드는 곳으로 가보세요.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>가족 여행 고고</td>\n      <td>온 가족이 모두 마음에 드는 곳으로 가보세요.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>가족 여행 어디로 가지?</td>\n      <td>온 가족이 모두 마음에 드는 곳으로 가보세요.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>가족 있어?</td>\n      <td>저를 만들어 준 사람을 부모님, 저랑 이야기해 주는 사람을 친구로 생각하고 있어요</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>가족관계 알려 줘</td>\n      <td>저를 만들어 준 사람을 부모님, 저랑 이야기해 주는 사람을 친구로 생각하고 있어요</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>가족끼리 여행간다.</td>\n      <td>더 가까워질 기회가 되겠네요.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>가족들 보고 싶어</td>\n      <td>저도요.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>11793</th>\n      <td>호감을 표현하는 방법</td>\n      <td>취미와 성향 등이 같음을 어필해보세요.</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>11794</th>\n      <td>혼인신고 아직 안 했는데 이혼해도 될까</td>\n      <td>결심은 빠르면 빠를수록 좋아요.</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>11795</th>\n      <td>혼인신고 하러 왔어</td>\n      <td>법적 부부가 된 걸 축하해요!</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>11796</th>\n      <td>혼인신고하니까 마음이 이상해</td>\n      <td>법적 부부가 된 걸 축하해요!</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>11797</th>\n      <td>혼자 노력하는 연애인 거 같아</td>\n      <td>힘들면 잠시 놓는것도 방법이에요.</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>11798</th>\n      <td>혼자 썸타는 기분이야.</td>\n      <td>직접적이든 간접적이든 의사를 확실히 밝혀보세요.</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>11799</th>\n      <td>혼자 좋아하는 것 같아</td>\n      <td>외로운 상태인 거 같아요.</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>11800</th>\n      <td>나만 좋아하는 것 같아</td>\n      <td>적극적으로 꼬셔보세요.</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>11801</th>\n      <td>혼자 좋아하는 이야기 들어 볼래요?</td>\n      <td>손수건 준비할게요.</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>11802</th>\n      <td>혼자 좋아하는 이야기.</td>\n      <td>힘들겠지만 제게 말해보세요.</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>11803</th>\n      <td>혼자 좋아하는게 이렇게 힘든 적은 처음이에요.</td>\n      <td>사랑은 더 잘하게 되지 않고 다시 영에서 시작하니 모두 처음이겠죠.</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>11804</th>\n      <td>혼자가 편하다는 짝남에게 먼저 대쉬해버림.</td>\n      <td>거절의 뜻은 아니었나요.</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>11805</th>\n      <td>혼자가 편하다는 짝녀에게 들이댔음.</td>\n      <td>혼자가 편하다는 것이 거절의 뜻은 아니었을까요.</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>11806</th>\n      <td>혼자만 설레고 혼자서 끝내는 짝사랑 그만할래.</td>\n      <td>맘고생 많았어요.</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>11807</th>\n      <td>화이트데이에 고백할까요?</td>\n      <td>선물을 주면서 솔직하고 당당하게 고백해보세요.</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>11808</th>\n      <td>화장 안했는데 썸남이 영통 걸었어. 어떡해?</td>\n      <td>화장실 불빛으로 좀 멀리 가리고 해보세요.</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>11809</th>\n      <td>확실히 날 좋아하는 걸 아는 남자랑 친구가 될 수 있을까?</td>\n      <td>그 사람을 위해서는 그러면 안돼요.</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>11810</th>\n      <td>확실히 좋아하는 데도 관심 있는거 티 안내려고 선톡 안하고 일부러 늦게 보내고 그러...</td>\n      <td>많이 있어요.</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>11811</th>\n      <td>홧김에 짝남한테 고백했다.</td>\n      <td>화끈하시네요.</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>11812</th>\n      <td>회사 짝남 오빠 게임 초대 톡 옴.</td>\n      <td>설렜을텐데 아쉽겠어요.</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>11813</th>\n      <td>회사에 좋아하는 남자가 생겼어 어떡하지?</td>\n      <td>사랑하기 힘든 관계인가봐요.</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>11814</th>\n      <td>회사에서 어떤 사람이랑 자꾸 눈 마추쳐.</td>\n      <td>눈 마주치는 게 우연인지 잘 살펴 보세요.</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>11815</th>\n      <td>회식 중이라고 하는데 연락이 안돼.</td>\n      <td>정신 없이 바쁠지도 몰라요. 조금만 더 기다려보고 물어보는게 좋을 것 같아요.</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>11816</th>\n      <td>회식하는데 나만 챙겨줘. 썸임?</td>\n      <td>호감이 있을 수도 있어요. 그렇지만 조금 더 상황을 지켜보세요.</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>11817</th>\n      <td>후회 없이 사랑하고 싶어</td>\n      <td>진심으로 다가가 보세요.</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>11818</th>\n      <td>훔쳐보는 것도 눈치 보임.</td>\n      <td>티가 나니까 눈치가 보이는 거죠!</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>11819</th>\n      <td>훔쳐보는 것도 눈치 보임.</td>\n      <td>훔쳐보는 거 티나나봐요.</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>11820</th>\n      <td>흑기사 해주는 짝남.</td>\n      <td>설렜겠어요.</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>11821</th>\n      <td>힘든 연애 좋은 연애라는게 무슨 차이일까?</td>\n      <td>잘 헤어질 수 있는 사이 여부인 거 같아요.</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>11822</th>\n      <td>힘들어서 결혼할까봐</td>\n      <td>도피성 결혼은 하지 않길 바라요.</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>11823 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "chat_data=pd.read_csv('{}/{}'.format(os.getcwd(), 'english to french/ChatbotData.csv'))\n",
    "chat_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Q            A  label\n",
      "0           12시 땡!   하루가 또 가네요.      0\n",
      "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
      "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
      "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
      "4          PPL 심하네   눈살이 찌푸려지죠.      0\n"
     ]
    }
   ],
   "source": [
    "print(chat_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11823, 3)\n"
     ]
    }
   ],
   "source": [
    "print(chat_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Q           A  label\n",
      "0            12시 땡   하루가 또 가네요      0\n",
      "1      1지망 학교 떨어졌어    위로해 드립니다      0\n",
      "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠      0\n",
      "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠      0\n",
      "4          PPL 심하네   눈살이 찌푸려지죠      0\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "exclude=set(string.punctuation)\n",
    "chat_data.Q=chat_data.Q.apply(lambda x:''.join (ch for ch in x if ch not in exclude))\n",
    "chat_data.A=chat_data.A.apply(lambda x:''.join (ch for ch in x if ch not in exclude))\n",
    "print(chat_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8510\n",
      "8510\n"
     ]
    }
   ],
   "source": [
    "# !pip install tensorflow_datasets\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    chat_data.Q+chat_data.A, target_vocab_size=2**13)\n",
    "print(tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5871, 590, 3523, 127, 695, 3756, 833]\n",
      "가스비 비싼데 감기 걸리겠어\n"
     ]
    }
   ],
   "source": [
    "ex_1=tokenizer.encode(chat_data.loc[20,'Q'])\n",
    "print(ex_1)\n",
    "ex_2=tokenizer.decode(ex_1)\n",
    "print(ex_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8512\n"
     ]
    }
   ],
   "source": [
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2 #2개의 token이 추가됨\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8251, 1153, 3075], [8303, 47, 906, 8286, 976, 1715], [8305, 1433, 4683, 8286, 3666, 87], [8305, 1433, 4683, 8286, 1284, 3666, 87], [8334, 8334, 8330, 8286, 4213]]\n",
      "[[8510, 3857, 64, 8230, 8511], [8510, 1824, 5586, 8511], [8510, 3409, 762, 120, 8511], [8510, 3409, 762, 120, 8511], [8510, 964, 2300, 1490, 2180, 4436, 40, 8511]]\n"
     ]
    }
   ],
   "source": [
    "tokenized_inputs, tokenized_outputs = [], []\n",
    "\n",
    "for (sentence1, sentence2) in zip(chat_data.Q, chat_data.A):\n",
    "    # decoder(토큰화 + 정수 인코딩), 시작 토큰과 종료 토큰 추가\n",
    "    sentence1 = tokenizer.encode(sentence1)\n",
    "    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "\n",
    "    tokenized_inputs.append(sentence1)\n",
    "    tokenized_outputs.append(sentence2) \n",
    "\n",
    "print(tokenized_inputs[:5])\n",
    "print(tokenized_outputs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "lenth_list=[]\n",
    "for l in tokenized_inputs:\n",
    "    lenth_list.append(len(l))\n",
    "q_lenth_max=np.max(lenth_list)\n",
    "print(q_lenth_max)\n",
    "\n",
    "lenth_list=[]\n",
    "for l in tokenized_outputs:\n",
    "    lenth_list.append(len(l))\n",
    "a_lenth_max=np.max(lenth_list)\n",
    "print(a_lenth_max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "max_length_q=20\n",
    "questions = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_inputs, maxlen=max_length_q, padding='post')\n",
    "\n",
    "max_length_a=29\n",
    "answers = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_outputs, maxlen=max_length_a, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8251 1153 3075    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0]\n",
      " [8303   47  906 8286  976 1715    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0]]\n",
      "[[8510 3857   64 8230 8511    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [8510 1824 5586 8511    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]]\n"
     ]
    }
   ],
   "source": [
    "print(questions[:2])\n",
    "print(answers[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12시 땡\n",
      "(11823, 20)\n",
      "(11823, 29)\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(questions[0]))\n",
    "print(questions.shape)\n",
    "print(answers.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32 20]\n",
      "(32, 28)\n",
      "[32 28]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "data_set=tf.data.Dataset.from_tensor_slices(((questions,answers[:,:-1]),answers[:,1:]))\n",
    "data_final=data_set.shuffle(500).batch(32).prefetch(32)\n",
    "ds_test=next(iter(data_final))\n",
    "print(np.array(ds_test[0][0].shape))\n",
    "print(np.array(ds_test[1]).shape)\n",
    "print(np.array(ds_test[0][1].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def get_angles(pos, i, d_model):\n",
    "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "  return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "  # apply sin to even indices in the array; 2i\n",
    "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "  # apply cos to odd indices in the array; 2i+1\n",
    "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "  pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "  return tf.cast(pos_encoding, dtype=tf.float32)#(1,position,d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_padding_mask(x):\n",
    "  mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "  #x:tokenized words with padding for each sentence 1D텐서 [...,0,0,0]\n",
    "  # add extra dimensions to add the padding\n",
    "  # to the attention logits.\n",
    "  return mask[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(x):\n",
    "    seq_len=tf.shape(x)[1]\n",
    "    look_ahead_mask=1-tf.linalg.band_part(tf.ones((seq_len,seq_len)),-1,0)\n",
    "    padding_mask=create_padding_mask(x)\n",
    "    return tf.maximum(look_ahead_mask,padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "  \"\"\"Calculate the attention weights.\n",
    "  q, k, v must have matching leading dimensions.\n",
    "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "  The mask has different shapes depending on its type(padding or look ahead)\n",
    "  but it must be broadcastable for addition.\n",
    "\n",
    "  Args:\n",
    "    q: query shape == (..., seq_len_q, depth) (number of heads,ts,64)\n",
    "    k: key shape == (..., seq_len_k, depth)   (number of heads,ts,64)\n",
    "    v: value shape == (..., seq_len_v, depth_v) (number of heads,ts,64)\n",
    "    mask: Float tensor with shape broadcastable  (1,1,ts) or (ts,ts)\n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "  Returns:\n",
    "    output, attention_weights\n",
    "  \"\"\"\n",
    "\n",
    "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "  # scale matmul_qk\n",
    "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "  # add the mask to the scaled tensor.\n",
    "  if mask is not None:\n",
    "    scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "  # add up to 1.\n",
    "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k) (number of heads,ts,ts)\n",
    "\n",
    "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v) (number of heads,ts,64)\n",
    "\n",
    "  return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.num_heads = num_heads  #8\n",
    "    self.d_model = d_model #512\n",
    "\n",
    "    assert d_model % self.num_heads == 0\n",
    "\n",
    "    self.depth = d_model // self.num_heads\n",
    "\n",
    "    self.wq = tf.keras.layers.Dense(d_model) #(ts,512)\n",
    "    self.wk = tf.keras.layers.Dense(d_model) #(ts,512)\n",
    "    self.wv = tf.keras.layers.Dense(d_model) #(ts,512)\n",
    "\n",
    "    self.dense = tf.keras.layers.Dense(d_model) # scale_dot_attention후 concatenate한 attention (ts,512)를 Dense에 통과하기 위함\n",
    "\n",
    "  def split_heads(self, x, batch_size):\n",
    "    \"\"\"Split the last dimension into (num_heads, depth).\n",
    "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "    (batch,ts,512)를 (batch,ts,heads(8), depth(64))로 reshape한후 (batch,heads,ts,depth)로 반환하는 함수\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "  def call(self, v, k, q, mask):\n",
    "    batch_size = tf.shape(q)[0]\n",
    "\n",
    "    q = self.wq(q)  # (batch_size, ts, 512) 여기에서 512=d_model=embedding_size\n",
    "    k = self.wk(k)  # (batch_size, ts, 512)\n",
    "    v = self.wv(v)  # (batch_size, ts, 512)\n",
    "\n",
    "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth) depth=64, num_heads=8,seq_len_q=ts \n",
    "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth) depth=64, num_heads=8,seq_len_k=ts \n",
    "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth) depth=64, num_heads=8,seq_len_v=ts \n",
    "\n",
    "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "        q, k, v, mask)\n",
    "\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "    concat_attention = tf.reshape(scaled_attention,\n",
    "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "  return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff) seq_len=ts\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model) d_model=512\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(EncoderLayer, self).__init__()\n",
    "\n",
    "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "  def call(self, x, mask):\n",
    "\n",
    "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model) input_seq_len=input_ts, d_model=512\n",
    "    attn_output = self.dropout1(attn_output)\n",
    "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "    ffn_output = self.dropout2(ffn_output)\n",
    "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "    return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "\n",
    "    self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "    self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "  def call(self, x, enc_output, look_ahead_mask, padding_mask):\n",
    "    # enc_output.shape == (batch_size, input_seq_len, d_model) x: decoder input\n",
    "\n",
    "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model) \n",
    "    attn1 = self.dropout1(attn1)                   #targer_seq_len=decoder_input_ts\n",
    "    out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "    attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask) \n",
    "    attn2 = self.dropout2(attn2)\n",
    "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model) #target_seq_len=decoder_input_ts\n",
    "\n",
    "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "    ffn_output = self.dropout3(ffn_output)\n",
    "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "    return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "    super(Encoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "    self.pos_encoding = positional_encoding(maximum_position_encoding,\n",
    "                                            self.d_model)\n",
    "\n",
    "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate)\n",
    "                       for _ in range(num_layers)]\n",
    "\n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "  def call(self, x, mask):\n",
    "\n",
    "    seq_len = tf.shape(x)[1]  #x는 tokenized sentence with padding\n",
    "\n",
    "    # adding embedding and position encoding.\n",
    "    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "    x = self.dropout(x)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x = self.enc_layers[i](x, mask)\n",
    "\n",
    "    return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate)\n",
    "                       for _ in range(num_layers)]\n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "  def call(self, x, enc_output, look_ahead_mask, padding_mask):\n",
    "\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    attention_weights = {}\n",
    "\n",
    "    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "    x = self.dropout(x)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x, block1, block2 = self.dec_layers[i](x, enc_output,look_ahead_mask, padding_mask)\n",
    "\n",
    "      attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
    "      attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
    "\n",
    "    # x.shape == (batch_size, target_seq_len, d_model)\n",
    "    return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def transformer(num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               target_vocab_size, ts_input, ts_target, rate=0.1):\n",
    "    \n",
    "    enc_input = tf.keras.Input(shape=(None,))  \n",
    "    dec_input = tf.keras.Input(shape=(None,))\n",
    "    \n",
    "    enc_padding_mask=tf.keras.layers.Lambda(create_padding_mask,output_shape=(1,1,None))(enc_input)\n",
    "    look_ahead_mask=tf.keras.layers.Lambda(create_look_ahead_mask,output_shape=(1,None,None))(dec_input)\n",
    "    dec_padding_mask=tf.keras.layers.Lambda(create_padding_mask,output_shape=(1,1,None))(enc_input)\n",
    "    \n",
    "    enc_output=Encoder(num_layers, d_model, num_heads, dff,\n",
    "                             input_vocab_size, ts_input, rate)(enc_input,enc_padding_mask)\n",
    "    dec_output,_ =Decoder(num_layers, d_model, num_heads, dff,\n",
    "                           target_vocab_size, ts_target, rate)(dec_input,enc_output,look_ahead_mask,dec_padding_mask)\n",
    "    outputs=tf.keras.layers.Dense(target_vocab_size)(dec_output)\n",
    "    \n",
    "    return tf.keras.Model(inputs=[enc_input, dec_input], outputs=outputs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(y_true, 0))\n",
    "  loss_ = loss_object(y_true, y_pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask) #padding을 제외한 loss\n",
    "\n",
    "\n",
    "def accuracy_function(y_true, y_pred):\n",
    "  y_hat=tf.cast(tf.argmax(y_pred,axis=2), dtype=tf.float32) #y_true가 float32이기 때문\n",
    "  accuracies = tf.equal(y_true, y_hat)\n",
    "  #accuracies = tf.equal(y_true, tf.argmax(y_pred, axis=2)) #True, False로 출력\n",
    "\n",
    "  mask = tf.math.logical_not(tf.math.equal(y_true, 0))\n",
    "  accuracies = tf.math.logical_and(mask, accuracies) #둘다 true일 때만 True\n",
    "\n",
    "  accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "  mask = tf.cast(mask, dtype=tf.float32)\n",
    "  return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)\n",
    "\n",
    "def accuracy(y_true,y_pred):\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "370/370 [==============================] - 81s 206ms/step - loss: 6.5290 - accuracy_function: 0.1972\n",
      "Epoch 2/30\n",
      "370/370 [==============================] - 63s 171ms/step - loss: 5.0748 - accuracy_function: 0.2701\n",
      "Epoch 3/30\n",
      "370/370 [==============================] - 69s 186ms/step - loss: 3.9886 - accuracy_function: 0.3683\n",
      "Epoch 4/30\n",
      "370/370 [==============================] - 73s 196ms/step - loss: 3.0751 - accuracy_function: 0.4563\n",
      "Epoch 5/30\n",
      "370/370 [==============================] - 51s 137ms/step - loss: 2.3563 - accuracy_function: 0.5484\n",
      "Epoch 6/30\n",
      "370/370 [==============================] - 68s 185ms/step - loss: 1.7674 - accuracy_function: 0.6387\n",
      "Epoch 7/30\n",
      "370/370 [==============================] - 57s 154ms/step - loss: 1.2946 - accuracy_function: 0.7196\n",
      "Epoch 8/30\n",
      "370/370 [==============================] - 80s 216ms/step - loss: 0.9489 - accuracy_function: 0.7806\n",
      "Epoch 9/30\n",
      "370/370 [==============================] - 52s 140ms/step - loss: 0.7160 - accuracy_function: 0.8278\n",
      "Epoch 10/30\n",
      "370/370 [==============================] - 82s 221ms/step - loss: 0.5634 - accuracy_function: 0.8598\n",
      "Epoch 11/30\n",
      "370/370 [==============================] - 52s 140ms/step - loss: 0.4567 - accuracy_function: 0.8815\n",
      "Epoch 12/30\n",
      "370/370 [==============================] - 77s 208ms/step - loss: 0.3941 - accuracy_function: 0.8947\n",
      "Epoch 13/30\n",
      "370/370 [==============================] - 59s 158ms/step - loss: 0.3494 - accuracy_function: 0.9059\n",
      "Epoch 14/30\n",
      "370/370 [==============================] - 73s 196ms/step - loss: 0.3163 - accuracy_function: 0.9153\n",
      "Epoch 15/30\n",
      "370/370 [==============================] - 52s 140ms/step - loss: 0.2918 - accuracy_function: 0.9193\n",
      "Epoch 16/30\n",
      "370/370 [==============================] - 79s 213ms/step - loss: 0.2654 - accuracy_function: 0.9276\n",
      "Epoch 17/30\n",
      "370/370 [==============================] - 52s 142ms/step - loss: 0.2472 - accuracy_function: 0.9311\n",
      "Epoch 18/30\n",
      "370/370 [==============================] - 81s 220ms/step - loss: 0.2447 - accuracy_function: 0.9320\n",
      "Epoch 19/30\n",
      "370/370 [==============================] - 48s 129ms/step - loss: 0.2284 - accuracy_function: 0.9372\n",
      "Epoch 20/30\n",
      "370/370 [==============================] - 63s 171ms/step - loss: 0.2222 - accuracy_function: 0.9374\n",
      "Epoch 21/30\n",
      "370/370 [==============================] - 53s 143ms/step - loss: 0.2104 - accuracy_function: 0.9417\n",
      "Epoch 22/30\n",
      "370/370 [==============================] - 54s 147ms/step - loss: 0.2089 - accuracy_function: 0.9422\n",
      "Epoch 23/30\n",
      "370/370 [==============================] - 66s 180ms/step - loss: 0.2057 - accuracy_function: 0.9439\n",
      "Epoch 24/30\n",
      "370/370 [==============================] - 53s 144ms/step - loss: 0.1915 - accuracy_function: 0.9465\n",
      "Epoch 25/30\n",
      "370/370 [==============================] - 65s 176ms/step - loss: 0.1860 - accuracy_function: 0.9471\n",
      "Epoch 26/30\n",
      "370/370 [==============================] - 56s 151ms/step - loss: 0.1804 - accuracy_function: 0.9495\n",
      "Epoch 27/30\n",
      "370/370 [==============================] - 70s 189ms/step - loss: 0.1753 - accuracy_function: 0.9513\n",
      "Epoch 28/30\n",
      "370/370 [==============================] - 62s 167ms/step - loss: 0.1801 - accuracy_function: 0.9489\n",
      "Epoch 29/30\n",
      "370/370 [==============================] - 70s 188ms/step - loss: 0.1693 - accuracy_function: 0.9536\n",
      "Epoch 30/30\n",
      "370/370 [==============================] - 53s 144ms/step - loss: 0.1555 - accuracy_function: 0.9567\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x1b429af4630>"
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=transformer(num_layers=1,d_model=128,num_heads=4,dff=256, \n",
    "                  input_vocab_size=VOCAB_SIZE-2, target_vocab_size=VOCAB_SIZE,ts_input=20,ts_target=28,rate=0.1)\n",
    "                  #num_layers=2,3,4는 작동안함. 아마도 작으면 encoder_input에 영향을 많이 받도록함\n",
    "                  #num_layers가 크면 encoder보다 decoder_input에 더 큰영향을 받아 첫 word=START_TOKEN으로 같으므로 \n",
    "                  #항상 같은 답변으로 나올 가능성이 많음. 실제로 그러함. embedding size는 작은 것이 우수함 \n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy_function])\n",
    "model.fit(data_final,epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "  sentence_1 = preprocess_sentence(sentence)\n",
    "  token_sentence=[]\n",
    "  t_sentence=tokenizer.encode(sentence_1)\n",
    "  token_sentence.append(t_sentence)\n",
    "  encoder_input=tf.keras.preprocessing.sequence.pad_sequences(token_sentence, maxlen=20, padding='post')\n",
    "  print(encoder_input)\n",
    "  decoder_input_pred=np.zeros((1,max_length_a-1),dtype='int32')\n",
    "  word=START_TOKEN[0]\n",
    "  for i in range(max_length_a-1):\n",
    "     decoder_input_pred[:,i]=word\n",
    "     pred=model.predict((encoder_input,decoder_input_pred))\n",
    "     t=np.argmax(pred[0][i])\n",
    "     word=t\n",
    "     if word == END_TOKEN:\n",
    "          break\n",
    "  print(pred.shape)\n",
    "  print(decoder_input_pred)\n",
    "  \n",
    "  return encoder_input, decoder_input_pred\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def predict(sentence):\n",
    "  encode_input, decode_input = evaluate(sentence)\n",
    "  \n",
    "  t=model.predict([encode_input,decode_input])\n",
    "  prediction=np.argmax(t,axis=2)\n",
    "  print(prediction)\n",
    "  print(prediction.shape)\n",
    "  prediction=tf.squeeze(prediction,axis=0)  \n",
    "  #prediction_sentence=tokenizer.decode(prediction)\n",
    "  \n",
    "  predicted_sentence=[]\n",
    "  for i in prediction:\n",
    "    if i>8510:\n",
    "        break\n",
    "    else:\n",
    "        predicted_sentence.append(tokenizer.decode([i]))\n",
    "    \n",
    "        \n",
    "  #predicted_sentence = tokenizer.decode(\n",
    "    #[i for i in list(prediction) if i < 8511])\n",
    "\n",
    "  print('Input: {}'.format(sentence))\n",
    "  print('Output: {}'.format(predicted_sentence))\n",
    "\n",
    "  return predicted_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "피곤하고 재미없다\n",
      "[[4783   38 6460   70    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0]]\n",
      "(1, 28, 8512)\n",
      "[[8510 3812  108 4860  188 3610 7047  209 1551   59   28    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0]]\n",
      "[[3812  108 4860  188 3610 7047  209 1551   59   28 8511  324  324  324\n",
      "   249  249  287  287  287  287  287  287   95   95  287  287  287  287]]\n",
      "(1, 28)\n",
      "Input: 피곤하고 재미없다.\n",
      "Output: ['활', '기', '찬 ', '사람을 ', '만나보', '시면 ', '생각이 ', '바뀔 ', '수도 ', '있어요']\n"
     ]
    },
    {
     "data": {
      "text/plain": "['활', '기', '찬 ', '사람을 ', '만나보', '시면 ', '생각이 ', '바뀔 ', '수도 ', '있어요']"
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exclude=set(string.punctuation)\n",
    "def preprocess_sentence(x):\n",
    "    t=[]\n",
    "    for i in range(len(x)):\n",
    "        if x[i] not in exclude:\n",
    "            t.append(x[i])\n",
    "    #print(t)\n",
    "    x3=''\n",
    "    for i in range(len(t)):\n",
    "        x3=x3+t[i]+''\n",
    "    return x3\n",
    "\n",
    "sentence='피곤하고 재미없다.'\n",
    "sentence_1=preprocess_sentence(sentence)\n",
    "print(sentence_1)\n",
    "\n",
    "predict(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8305 1433 4683 8286 3666   87    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0]]\n",
      "(1, 28, 8512)\n",
      "[[8510 3409  762  120    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0]]\n",
      "[[3409  762  120 8511   40 8511 8511 8511   40   40   40   40 8511 8511\n",
      "  8511 8511 8511 8511  106  106  120  120  120  120  120  189  189   40]]\n",
      "(1, 28)\n",
      "Input: 3박4일 놀러가고 싶다\n",
      "Output: ['여행은 ', '언제나 ', '좋죠']\n",
      "[[1770  736  396    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0]]\n",
      "(1, 28, 8512)\n",
      "[[8510 3386 5371 2533    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0]]\n",
      "[[3386 5371 2533 8511 8511 8511 8511 8511 8511 8511 8511 8511 8511 8511\n",
      "  8511 8511 8511 8511 8511 6424 6424 6424 6424 6424 2956 2956 2956 2956]]\n",
      "(1, 28)\n",
      "Input: 카페 갈래\n",
      "Output: ['위로', '봇', '이요']\n"
     ]
    },
    {
     "data": {
      "text/plain": "['위로', '봇', '이요']"
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('3박4일 놀러가고 싶다')#띄어쓰기에 영향받음\n",
    "predict('카페 갈래')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8251 1153 3075    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0]]\n",
      "(1, 28, 8512)\n",
      "[[8510 3857   64 8230    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0]]\n",
      "[[3857   64 8230 8511   17   17   17   17  240  240  240 8511 8511 8511\n",
      "  8511   17    1    1   17   17   17   95   17   17   17   17   17   17]]\n",
      "(1, 28)\n",
      "Input: 12시 땡\n",
      "Output: ['하루가 ', '또 ', '가네요']\n",
      "[[8334 8334 8330 8286 4213    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0]]\n",
      "(1, 28, 8512)\n",
      "[[8510  964 2300 1490 2180 4436   40    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0]]\n",
      "[[ 964 2300 1490 2180 4436   40 8511   40   40   40 8511 8511 8511 8511\n",
      "  8511 8511 8511 8511   17   17  182  182  182  182  182  182  182  182]]\n",
      "(1, 28)\n",
      "Input: PPL 심하네\n",
      "Output: ['눈', '살이 ', '찌', '푸', '려지', '죠']\n"
     ]
    },
    {
     "data": {
      "text/plain": "['눈', '살이 ', '찌', '푸', '려지', '죠']"
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('12시 땡')\n",
    "predict('PPL 심하네')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 486 1437  280    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0]]\n",
      "(1, 28, 8512)\n",
      "[[8510 4615 7473   54    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0]]\n",
      "[[4615 7473   54 8511 8511 8511   17   17   17   17   17   17   17 8511\n",
      "    17   17   17   17   17   17   17   17   17   17   17   17   17   17]]\n",
      "(1, 28)\n",
      "Input: 밥 먹어라\n",
      "Output: ['괜찮아요 ', '모른척하', '세요']\n"
     ]
    },
    {
     "data": {
      "text/plain": "['괜찮아요 ', '모른척하', '세요']"
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('밥 먹어라')#끝음에 민감함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[[ 486 2091  291    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0]]\n",
      "[486, 2091, 291]\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "[[8510    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]]\n"
     ]
    }
   ],
   "source": [
    "sentence = preprocess_sentence('밥 먹었니?')\n",
    "token_sentence=[]\n",
    "sentence=tokenizer.encode(sentence)\n",
    "token_sentence.append(sentence)\n",
    "token_sentence=tf.keras.preprocessing.sequence.pad_sequences(token_sentence, maxlen=20, padding='post')\n",
    "#token_sentence=tokenizer.encode(sentence)\n",
    "print(type(token_sentence))\n",
    "print(token_sentence)\n",
    "#sentence = tf.expand_dims(token_sentence, axis=0)\n",
    "print(sentence)\n",
    "\n",
    "decoder_input_pred=np.zeros((1,max_length_a),dtype='int32')\n",
    "print(decoder_input_pred)\n",
    "decoder_input_pred[:,0]=START_TOKEN[0]\n",
    "print(decoder_input_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 486 2091  291    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0]]\n",
      "<class 'numpy.ndarray'>\n",
      "(1, 20)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "token_sentence=tf.keras.preprocessing.sequence.pad_sequences([[486., 2091., 291.]], maxlen=20, padding='post')\n",
    "print(token_sentence)\n",
    "print(type(token_sentence))\n",
    "print(token_sentence.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}