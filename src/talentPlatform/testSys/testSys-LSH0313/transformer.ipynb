{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                                      eng  \\\n0                                                     Go.   \n1                                                    Run!   \n2                                                    Run!   \n3                                                    Wow!   \n4                                                   Fire!   \n...                                                   ...   \n149856  A carbon footprint is the amount of carbon dio...   \n149857  Death is something that we're often discourage...   \n149858  Since there are usually multiple websites on a...   \n149859  If someone who doesn't know your background sa...   \n149860  It may be impossible to get a completely error...   \n\n                                                       fr  \n0                                                    Va !  \n1                                                 Cours !  \n2                                                Courez !  \n3                                              Ça alors !  \n4                                                Au feu !  \n...                                                   ...  \n149856  Une empreinte carbone est la somme de pollutio...  \n149857  La mort est une chose qu'on nous décourage sou...  \n149858  Puisqu'il y a de multiples sites web sur chaqu...  \n149859  Si quelqu'un qui ne connaît pas vos antécédent...  \n149860  Il est peut-être impossible d'obtenir un Corpu...  \n\n[149861 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>eng</th>\n      <th>fr</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Go.</td>\n      <td>Va !</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Run!</td>\n      <td>Cours !</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Run!</td>\n      <td>Courez !</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Wow!</td>\n      <td>Ça alors !</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Fire!</td>\n      <td>Au feu !</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Help!</td>\n      <td>À l'aide !</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Jump.</td>\n      <td>Saute.</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Stop!</td>\n      <td>Ça suffit !</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Stop!</td>\n      <td>Stop !</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Stop!</td>\n      <td>Arrête-toi !</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Wait!</td>\n      <td>Attends !</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>Wait!</td>\n      <td>Attendez !</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>Go on.</td>\n      <td>Poursuis.</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>Go on.</td>\n      <td>Continuez.</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>Go on.</td>\n      <td>Poursuivez.</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>I see.</td>\n      <td>Je comprends.</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>I try.</td>\n      <td>J'essaye.</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>I won!</td>\n      <td>J'ai gagné !</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>I won!</td>\n      <td>Je l'ai emporté !</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>Oh no!</td>\n      <td>Oh non !</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>Attack!</td>\n      <td>Attaque !</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>Attack!</td>\n      <td>Attaquez !</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>Cheers!</td>\n      <td>Santé !</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>Cheers!</td>\n      <td>À votre santé !</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>Cheers!</td>\n      <td>Merci !</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>Cheers!</td>\n      <td>Tchin-tchin !</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>Get up.</td>\n      <td>Lève-toi.</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>Go now.</td>\n      <td>Va, maintenant.</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>Go now.</td>\n      <td>Allez-y maintenant.</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>Go now.</td>\n      <td>Vas-y maintenant.</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>149831</th>\n      <td>Since it will be cold soon, it might be nice t...</td>\n      <td>Puisqu'il fera bientôt froid, ça serait chouet...</td>\n    </tr>\n    <tr>\n      <th>149832</th>\n      <td>A man who has never gone to school may steal f...</td>\n      <td>Un homme qui n'a jamais été à l'école peut vol...</td>\n    </tr>\n    <tr>\n      <th>149833</th>\n      <td>One way to lower the number of errors in the T...</td>\n      <td>Un moyen de diminuer le nombre d’erreurs dans ...</td>\n    </tr>\n    <tr>\n      <th>149834</th>\n      <td>What is old age? First you forget names, then ...</td>\n      <td>Qu'est l'âge ? D'abord on oublie les noms, et ...</td>\n    </tr>\n    <tr>\n      <th>149835</th>\n      <td>What is old age? First you forget names, then ...</td>\n      <td>Ce qu'est l'âge ? D'abord on oublie les noms, ...</td>\n    </tr>\n    <tr>\n      <th>149836</th>\n      <td>He and I have a near-telepathic understanding ...</td>\n      <td>Lui et moi avons une compréhension quasi-télép...</td>\n    </tr>\n    <tr>\n      <th>149837</th>\n      <td>Although rainforests make up only two percent ...</td>\n      <td>Bien que les forêts tropicales ne couvrent que...</td>\n    </tr>\n    <tr>\n      <th>149838</th>\n      <td>If you translate from your second language int...</td>\n      <td>Si vous traduisez de votre seconde langue dans...</td>\n    </tr>\n    <tr>\n      <th>149839</th>\n      <td>I love trying out new things, so I always buy ...</td>\n      <td>J'adore essayer de nouvelles choses, alors j'a...</td>\n    </tr>\n    <tr>\n      <th>149840</th>\n      <td>A good theory is characterized by the fact tha...</td>\n      <td>Une bonne théorie se caractérise par le fait d...</td>\n    </tr>\n    <tr>\n      <th>149841</th>\n      <td>The more time you spend speaking a foreign lan...</td>\n      <td>Plus l'on passe de temps à parler une langue é...</td>\n    </tr>\n    <tr>\n      <th>149842</th>\n      <td>The enquiry concluded that, despite his denial...</td>\n      <td>L'enquête conclut qu'en dépit de ses dénégatio...</td>\n    </tr>\n    <tr>\n      <th>149843</th>\n      <td>The Tatoeba Project, which can be found online...</td>\n      <td>Le projet Tatoeba, que l'on peut trouver en li...</td>\n    </tr>\n    <tr>\n      <th>149844</th>\n      <td>You may not learn to speak as well as a native...</td>\n      <td>Peut-être n'apprendrez-vous pas à parler comme...</td>\n    </tr>\n    <tr>\n      <th>149845</th>\n      <td>And the good news is that today the economy is...</td>\n      <td>Et la bonne nouvelle est qu'aujourd'hui l'écon...</td>\n    </tr>\n    <tr>\n      <th>149846</th>\n      <td>E-cigarettes are being promoted as a healthy a...</td>\n      <td>La cigarette électronique est mise en avant co...</td>\n    </tr>\n    <tr>\n      <th>149847</th>\n      <td>It's still too hard to find a job. And even if...</td>\n      <td>C'est encore trop difficile de trouver un empl...</td>\n    </tr>\n    <tr>\n      <th>149848</th>\n      <td>As you contribute more sentences to the Tatoeb...</td>\n      <td>Au fur et à mesure que vous ajoutez davantage ...</td>\n    </tr>\n    <tr>\n      <th>149849</th>\n      <td>Even at the end of the nineteenth century, sai...</td>\n      <td>Même à la fin du dix-neuvième siècle, les mari...</td>\n    </tr>\n    <tr>\n      <th>149850</th>\n      <td>Five tremors in excess of magnitude 5.0 on the...</td>\n      <td>Cinq secousses dépassant la magnitude cinq sur...</td>\n    </tr>\n    <tr>\n      <th>149851</th>\n      <td>No matter how much you try to convince people ...</td>\n      <td>Peu importe le temps que tu passeras à essayer...</td>\n    </tr>\n    <tr>\n      <th>149852</th>\n      <td>We need to uphold laws against discrimination ...</td>\n      <td>Nous devons faire respecter les lois contre la...</td>\n    </tr>\n    <tr>\n      <th>149853</th>\n      <td>A child who is a native speaker usually knows ...</td>\n      <td>Un enfant qui est un locuteur natif connaît ha...</td>\n    </tr>\n    <tr>\n      <th>149854</th>\n      <td>There are four main causes of alcohol-related ...</td>\n      <td>Il y a quatre causes principales de décès liés...</td>\n    </tr>\n    <tr>\n      <th>149855</th>\n      <td>Top-down economics never works, said Obama. \"T...</td>\n      <td>« L'économie en partant du haut vers le bas, ç...</td>\n    </tr>\n    <tr>\n      <th>149856</th>\n      <td>A carbon footprint is the amount of carbon dio...</td>\n      <td>Une empreinte carbone est la somme de pollutio...</td>\n    </tr>\n    <tr>\n      <th>149857</th>\n      <td>Death is something that we're often discourage...</td>\n      <td>La mort est une chose qu'on nous décourage sou...</td>\n    </tr>\n    <tr>\n      <th>149858</th>\n      <td>Since there are usually multiple websites on a...</td>\n      <td>Puisqu'il y a de multiples sites web sur chaqu...</td>\n    </tr>\n    <tr>\n      <th>149859</th>\n      <td>If someone who doesn't know your background sa...</td>\n      <td>Si quelqu'un qui ne connaît pas vos antécédent...</td>\n    </tr>\n    <tr>\n      <th>149860</th>\n      <td>It may be impossible to get a completely error...</td>\n      <td>Il est peut-être impossible d'obtenir un Corpu...</td>\n    </tr>\n  </tbody>\n</table>\n<p>149861 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = pd.read_table('{}/{}'.format(os.getcwd(), 'english to french/english to french.txt'), names=['eng', 'fr'])\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    eng        fr\n",
      "0   Go.      Va !\n",
      "1  Run!   Cours !\n",
      "2  Run!  Courez !\n",
      "                            eng                              fr\n",
      "49997  They go to work on foot.     Ils vont au travail à pied.\n",
      "49998  They got into the train.    Ils montèrent dans le train.\n",
      "49999  They got into the train.  Elles montèrent dans le train.\n"
     ]
    }
   ],
   "source": [
    "lines = lines[0:50000]\n",
    "print(lines.head(3))\n",
    "print(lines.tail(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\ProgramData\\Anaconda3\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "                            eng                                 fr\n0                           go.                               va !\n1                          run!                            cours !\n2                          run!                           courez !\n3                          wow!                         ça alors !\n4                         fire!                           au feu !\n...                         ...                                ...\n49995  they gave us their word.    ils nous donnèrent leur parole.\n49996  they gave us their word.  elles nous donnèrent leur parole.\n49997  they go to work on foot.        ils vont au travail à pied.\n49998  they got into the train.       ils montèrent dans le train.\n49999  they got into the train.     elles montèrent dans le train.\n\n[50000 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>eng</th>\n      <th>fr</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>go.</td>\n      <td>va !</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>run!</td>\n      <td>cours !</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>run!</td>\n      <td>courez !</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>wow!</td>\n      <td>ça alors !</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>fire!</td>\n      <td>au feu !</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>help!</td>\n      <td>à l'aide !</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>jump.</td>\n      <td>saute.</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>stop!</td>\n      <td>ça suffit !</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>stop!</td>\n      <td>stop !</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>stop!</td>\n      <td>arrête-toi !</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>wait!</td>\n      <td>attends !</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>wait!</td>\n      <td>attendez !</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>go on.</td>\n      <td>poursuis.</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>go on.</td>\n      <td>continuez.</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>go on.</td>\n      <td>poursuivez.</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>i see.</td>\n      <td>je comprends.</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>i try.</td>\n      <td>j'essaye.</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>i won!</td>\n      <td>j'ai gagné !</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>i won!</td>\n      <td>je l'ai emporté !</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>oh no!</td>\n      <td>oh non !</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>attack!</td>\n      <td>attaque !</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>attack!</td>\n      <td>attaquez !</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>cheers!</td>\n      <td>santé !</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>cheers!</td>\n      <td>à votre santé !</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>cheers!</td>\n      <td>merci !</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>cheers!</td>\n      <td>tchin-tchin !</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>get up.</td>\n      <td>lève-toi.</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>go now.</td>\n      <td>va, maintenant.</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>go now.</td>\n      <td>allez-y maintenant.</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>go now.</td>\n      <td>vas-y maintenant.</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>49970</th>\n      <td>they chased others away.</td>\n      <td>ils en chassèrent d'autres.</td>\n    </tr>\n    <tr>\n      <th>49971</th>\n      <td>they crossed the border.</td>\n      <td>ils traversèrent la frontière.</td>\n    </tr>\n    <tr>\n      <th>49972</th>\n      <td>they crossed the border.</td>\n      <td>elles traversèrent la frontière.</td>\n    </tr>\n    <tr>\n      <th>49973</th>\n      <td>they crossed the border.</td>\n      <td>ils ont traversé la frontière.</td>\n    </tr>\n    <tr>\n      <th>49974</th>\n      <td>they crossed the border.</td>\n      <td>elles ont traversé la frontière.</td>\n    </tr>\n    <tr>\n      <th>49975</th>\n      <td>they didn't act quickly.</td>\n      <td>ils n'agirent pas rapidement.</td>\n    </tr>\n    <tr>\n      <th>49976</th>\n      <td>they don't have to know.</td>\n      <td>il n'est pas nécessaire qu'ils sachent.</td>\n    </tr>\n    <tr>\n      <th>49977</th>\n      <td>they don't have to know.</td>\n      <td>il n'est pas nécessaire qu'elles sachent.</td>\n    </tr>\n    <tr>\n      <th>49978</th>\n      <td>they don't know my name.</td>\n      <td>ils ne savent pas mon nom.</td>\n    </tr>\n    <tr>\n      <th>49979</th>\n      <td>they don't know my name.</td>\n      <td>ils ne savent pas mon prénom.</td>\n    </tr>\n    <tr>\n      <th>49980</th>\n      <td>they don't listen to me.</td>\n      <td>ils ne m'écoutent pas.</td>\n    </tr>\n    <tr>\n      <th>49981</th>\n      <td>they don't listen to me.</td>\n      <td>elles ne m'écoutent pas.</td>\n    </tr>\n    <tr>\n      <th>49982</th>\n      <td>they drank way too much.</td>\n      <td>ils ont beaucoup trop bu.</td>\n    </tr>\n    <tr>\n      <th>49983</th>\n      <td>they enjoyed themselves.</td>\n      <td>ils s'amusèrent.</td>\n    </tr>\n    <tr>\n      <th>49984</th>\n      <td>they enjoyed themselves.</td>\n      <td>ils se sont amusés.</td>\n    </tr>\n    <tr>\n      <th>49985</th>\n      <td>they enjoyed themselves.</td>\n      <td>elles se sont amusées.</td>\n    </tr>\n    <tr>\n      <th>49986</th>\n      <td>they enjoyed themselves.</td>\n      <td>elles s'amusèrent.</td>\n    </tr>\n    <tr>\n      <th>49987</th>\n      <td>they formed a swim team.</td>\n      <td>ils ont formé une équipe de nageurs.</td>\n    </tr>\n    <tr>\n      <th>49988</th>\n      <td>they formed a swim team.</td>\n      <td>elles ont formé une équipe de nageuses.</td>\n    </tr>\n    <tr>\n      <th>49989</th>\n      <td>they formed a swim team.</td>\n      <td>ce sont eux qui formèrent une équipe de nageurs.</td>\n    </tr>\n    <tr>\n      <th>49990</th>\n      <td>they formed a swim team.</td>\n      <td>ce sont elles qui formèrent une équipe de nage...</td>\n    </tr>\n    <tr>\n      <th>49991</th>\n      <td>they formed a swim team.</td>\n      <td>ils ont formé une équipe de natation.</td>\n    </tr>\n    <tr>\n      <th>49992</th>\n      <td>they fought for freedom.</td>\n      <td>ils se sont battus pour la liberté.</td>\n    </tr>\n    <tr>\n      <th>49993</th>\n      <td>they gave us their word.</td>\n      <td>ils nous ont donné leur parole.</td>\n    </tr>\n    <tr>\n      <th>49994</th>\n      <td>they gave us their word.</td>\n      <td>elles nous ont donné leur parole.</td>\n    </tr>\n    <tr>\n      <th>49995</th>\n      <td>they gave us their word.</td>\n      <td>ils nous donnèrent leur parole.</td>\n    </tr>\n    <tr>\n      <th>49996</th>\n      <td>they gave us their word.</td>\n      <td>elles nous donnèrent leur parole.</td>\n    </tr>\n    <tr>\n      <th>49997</th>\n      <td>they go to work on foot.</td>\n      <td>ils vont au travail à pied.</td>\n    </tr>\n    <tr>\n      <th>49998</th>\n      <td>they got into the train.</td>\n      <td>ils montèrent dans le train.</td>\n    </tr>\n    <tr>\n      <th>49999</th>\n      <td>they got into the train.</td>\n      <td>elles montèrent dans le train.</td>\n    </tr>\n  </tbody>\n</table>\n<p>50000 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines['eng'] = lines['eng'].apply(lambda x: x.lower())\n",
    "lines['fr'] = lines['fr'].apply(lambda x: x.lower())\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\py36\\lib\\site-packages\\pandas\\core\\generic.py:5209: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "exclude = set(string.punctuation)\n",
    "lines.eng = lines.eng.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "lines.fr = lines.fr.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   eng       fr\n",
      "0   go      va \n",
      "1  run   cours \n",
      "2  run  courez \n",
      "                           eng                             fr\n",
      "49997  they go to work on foot     ils vont au travail à pied\n",
      "49998  they got into the train    ils montèrent dans le train\n",
      "49999  they got into the train  elles montèrent dans le train\n"
     ]
    }
   ],
   "source": [
    "print(lines.head(3))\n",
    "print(lines.tail(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lines.fr = lines.fr.apply(lambda x: 'start ' + x + ' end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   eng                 fr\n",
      "0   go      start va  end\n",
      "1  run   start cours  end\n",
      "2  run  start courez  end\n",
      "                           eng                                       fr\n",
      "49997  they go to work on foot     start ils vont au travail à pied end\n",
      "49998  they got into the train    start ils montèrent dans le train end\n",
      "49999  they got into the train  start elles montèrent dans le train end\n"
     ]
    }
   ],
   "source": [
    "print(lines.head(3))\n",
    "print(lines.tail(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(50000, 2)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# fit a tokenizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "eng_tokenizer = create_tokenizer(lines['eng'])\n",
    "eng_dict = json.loads(json.dumps(eng_tokenizer.word_counts))\n",
    "df = pd.DataFrame([eng_dict.keys(), eng_dict.values()]).T\n",
    "df.columns = ['word', 'count']\n",
    "\n",
    "df = df.sort_values(by='count', ascending=False)\n",
    "df['cum_count'] = df['count'].cumsum()\n",
    "df['cum_perc'] = df['cum_count'] / df['cum_count'].max()\n",
    "final_eng_words = df[df['cum_perc'] < 0.8]['word'].values\n",
    "#final_eng_words=df['word'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fr_tokenizer = create_tokenizer(lines['fr'])\n",
    "fr_dict = json.loads(json.dumps(fr_tokenizer.word_counts))\n",
    "df = pd.DataFrame([fr_dict.keys(), fr_dict.values()]).T\n",
    "df.columns = ['word', 'count']\n",
    "df = df.sort_values(by='count', ascending=False)\n",
    "df['cum_count'] = df['count'].cumsum()\n",
    "df['cum_perc'] = df['cum_count'] / df['cum_count'].max()\n",
    "final_fr_words = df[df['cum_perc'] < 0.8]['word'].values\n",
    "#final_fr_words=df['word'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384 357\n"
     ]
    }
   ],
   "source": [
    "print(len(final_eng_words), len(final_fr_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def filter_eng_words(x):\n",
    "    t = []\n",
    "    x = x.split()\n",
    "    for i in range(len(x)):\n",
    "        if x[i] in final_eng_words:\n",
    "            t.append(x[i])\n",
    "        else:\n",
    "            t.append('unk')\n",
    "    x3 = ''\n",
    "    for i in range(len(t)):\n",
    "        x3 = x3 + t[i] + ' '\n",
    "    return x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'he is unk good '"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_eng_words('he is extremely good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def filter_fr_words(x):\n",
    "    t = []\n",
    "    x = x.split()\n",
    "    for i in range(len(x)):\n",
    "        if x[i] in final_fr_words:\n",
    "            t.append(x[i])\n",
    "        else:\n",
    "            t.append('unk')\n",
    "    x3 = ''\n",
    "    for i in range(len(t)):\n",
    "        x3 = x3 + t[i] + ' '\n",
    "    return x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\ProgramData\\Anaconda3\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "lines['eng'] = lines['eng'].apply(filter_eng_words)\n",
    "lines['fr'] = lines['fr'].apply(filter_fr_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_eng_words = set()\n",
    "for eng in lines.eng:\n",
    "    for word in eng.split():\n",
    "        if word not in all_eng_words:\n",
    "            all_eng_words.add(word)\n",
    "\n",
    "all_french_words = set()\n",
    "for fr in lines.fr:\n",
    "    for word in fr.split():\n",
    "        if word not in all_french_words:\n",
    "            all_french_words.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_words = sorted(list(all_eng_words))\n",
    "target_words = sorted(list(all_french_words))\n",
    "num_encoder_tokens = len(all_eng_words)\n",
    "num_decoder_tokens = len(all_french_words)\n",
    "# del all_eng_words, all_french_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'unk'}"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(all_french_words) - set(final_fr_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "385"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_eng_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "358"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_token_index = dict(\n",
    "    [(word, i + 1) for i, word in enumerate(input_words)])\n",
    "target_token_index = dict(\n",
    "    [(word, i + 1) for i, word in enumerate(target_words)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "358"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_decoder_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "284\n",
      "89\n",
      "youve\n",
      "start\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "print(target_token_index['start'])\n",
    "print(target_token_index['end'])\n",
    "print(list(input_token_index.keys())[384])\n",
    "print(list(target_token_index.keys())[283])\n",
    "print(list(target_token_index.keys())[88])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "length_list = []\n",
    "for l in lines.fr:\n",
    "    length_list.append(len(l.split(' ')))\n",
    "fr_max_length = np.max(length_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "length_list = []\n",
    "for l in lines.eng:\n",
    "    length_list.append(len(l.split(' ')))\n",
    "eng_max_length = np.max(length_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "print(eng_max_length)\n",
    "print(fr_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "encoder_input_data = np.zeros(\n",
    "    (len(lines['eng']), eng_max_length),\n",
    "    dtype='float32')\n",
    "decoder = np.zeros(\n",
    "    (len(lines['fr']), fr_max_length + 1),\n",
    "    dtype='float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(50000, 18)"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i, (input_text, target_text) in enumerate(zip(lines['eng'], lines['fr'])):\n",
    "    for t, word in enumerate(input_text.split()):\n",
    "        encoder_input_data[i, t] = input_token_index[word]\n",
    "    for t, word in enumerate(target_text.split()):\n",
    "        decoder[i, t] = target_token_index[word]\n",
    "decoder_input_data = decoder[:, :-1]\n",
    "decoder_target_data = decoder[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 8) (50000, 17) (50000, 17)\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input_data.shape, decoder_input_data.shape, decoder_target_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[63.  8.  0.  0.  0.  0.  0.  0.]\n",
      "[284. 320. 274.  89.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.]\n",
      "[320. 274.  89.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.]\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input_data[1000])\n",
    "print(decoder_input_data[1000])\n",
    "print(decoder_target_data[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                            np.arange(d_model)[np.newaxis, :],\n",
    "                            d_model)\n",
    "\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)  #(1,position,d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_padding_mask(x):\n",
    "    mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "    #x:tokenized words with padding for each sentence 1D텐서 [...,0,0,0]\n",
    "    # add extra dimensions to add the padding\n",
    "    # to the attention logits.\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(x):\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    padding_mask = create_padding_mask(x)\n",
    "    return tf.maximum(look_ahead_mask, padding_mask)\n",
    "\n",
    "##x:tokenized words with padding for each sentence 1D텐서 [...,0,0,0](ts,), return (ts,ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "    q, k, v must have matching leading dimensions.\n",
    "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "    The mask has different shapes depending on its type(padding or look ahead)\n",
    "    but it must be broadcastable for addition.\n",
    "\n",
    "    Args:\n",
    "      q: query shape == (..., seq_len_q, depth) (number of heads,ts,64)\n",
    "      k: key shape == (..., seq_len_k, depth)   (number of heads,ts,64)\n",
    "      v: value shape == (..., seq_len_v, depth_v) (number of heads,ts,64)\n",
    "      mask: Float tensor with shape broadcastable  (1,1,ts) or (ts,ts)\n",
    "            to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "      output, attention_weights\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits,\n",
    "                                      axis=-1)  # (..., seq_len_q, seq_len_k) (number of heads,ts,ts)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v) (number of heads,ts,64)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads  #8\n",
    "        self.d_model = d_model  #512\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)  #(ts,512)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)  #(ts,512)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)  #(ts,512)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(\n",
    "            d_model)  # scale_dot_attention후 concatenate한 attention (ts,512)를 Dense에 통과하기 위함\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        (batch,ts,512)를 (batch,ts,heads(8), depth(64))로 reshape한후 (batch,heads,ts,depth)로 반환하는 함수\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, ts, 512) 여기에서 512=d_model=embedding_size\n",
    "        k = self.wk(k)  # (batch_size, ts, 512)\n",
    "        v = self.wv(v)  # (batch_size, ts, 512)\n",
    "\n",
    "        q = self.split_heads(q,\n",
    "                             batch_size)  # (batch_size, num_heads, seq_len_q, depth) depth=64, num_heads=8,seq_len_q=ts\n",
    "        k = self.split_heads(k,\n",
    "                             batch_size)  # (batch_size, num_heads, seq_len_k, depth) depth=64, num_heads=8,seq_len_k=ts\n",
    "        v = self.split_heads(v,\n",
    "                             batch_size)  # (batch_size, num_heads, seq_len_v, depth) depth=64, num_heads=8,seq_len_v=ts\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention,\n",
    "                                        perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff) seq_len=ts\n",
    "        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model) d_model=512\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, mask):\n",
    "        attn_output, _ = self.mha(x, x, x,\n",
    "                                  mask)  # (batch_size, input_seq_len, d_model) input_seq_len=input_ts, d_model=512\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, look_ahead_mask, padding_mask):\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model) x: decoder input\n",
    "\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1)  #targer_seq_len=decoder_input_ts\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
    "        attn2 = self.dropout2(attn2)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model) #target_seq_len=decoder_input_ts\n",
    "\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 지금까지 인코더 층의 내부 아키텍처에 대해서 이해해보았습니다. 이러한 인코더 층을 enc_num_layers개만큼 쌓고, 마지막 인코더 층에서 얻는 (seq_len, d_model) 크기의 행렬을 디코더로 보내주므로서 트랜스포머 인코더의 인코딩 연산이 끝나게 됩니다.\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "                 maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding,\n",
    "                                                self.d_model)\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate)\n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, mask):\n",
    "        seq_len = tf.shape(x)[1]  #x는 tokenized sentence with padding\n",
    "\n",
    "        # adding embedding and position encoding.\n",
    "        # 포지셔널 인코딩 + 드롭아웃\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # 아래의 코드는 인코더 층을 enc_num_layers개 만큼 쌓는 코드입니다.\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a_1': 0, 'b_1': 0, 'a_2': 1, 'b_2': 1}\n"
     ]
    }
   ],
   "source": [
    "a = {}\n",
    "for i in range(2):\n",
    "    block1 = i ** 2\n",
    "    block2 = i ** 3\n",
    "    a[\"a_{}\".format(i + 1)] = block1\n",
    "    a[\"b_{}\".format(i + 1)] = block2\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 디코더도 인코더와 동일하게 임베딩 층과 포지셔널 인코딩을 거친 후의 문장 행렬이 입력됩니다. 트랜스포머 또한 seq2seq와 마찬가지로 교사 강요(Teacher Forcing)을 사용하여 훈련되므로 학습 과정에서 디코더는 번역할 문장에 해당되는 <sos> je suis étudiant의 문장 행렬을 한 번에 입력받습니다. 그리고 디코더는 이 문장 행렬로부터 각 시점의 단어를 예측하도록 훈련됩니다.\n",
    "# 여기서 문제가 있습니다. seq2seq의 디코더에 사용되는 RNN 계열의 신경망은 입력 단어를 매 시점마다 순차적으로 입력받으므로 다음 단어 예측에 현재 시점을 포함한 이전 시점에 입력된 단어들만 참고할 수 있습니다. 반면, 트랜스포머는 문장 행렬로 입력을 한 번에 받으므로 현재 시점의 단어를 예측하고자 할 때, 입력 문장 행렬로부터 미래 시점의 단어까지도 참고할 수 있는 현상이 발생합니다. 가령, suis를 예측해야 하는 시점이라고 해봅시다. RNN 계열의 seq2seq의 디코더라면 현재까지 디코더에 입력된 단어는 <sos>와 je뿐일 것입니다. 반면, 트랜스포머는 이미 문장 행렬로 <sos> je suis étudiant를 입력받았습니다.\n",
    "# 이를 위해 트랜스포머의 디코더에서는 현재 시점의 예측에서 현재 시점보다 미래에 있는 단어들을 참고하지 못하도록 룩-어헤드 마스크(look-ahead mask)를 도입했습니다. 직역하면 '미리보기에 대한 마스크'입니다.\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "                 maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate)\n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    # 인코더의 입력으로 들어가는 문장에는 패딩이 있을 수 있으므로, 어텐션 시 패딩 토큰을 제외하도록 패딩 마스크를 사용합니다. 이는 MultiHeadAttention 함수의 mask의 인자값으로 padding_mask가 사용되는 이유입니다. 인코더는 총 두 개의 서브층으로 이루어지는데, 멀티 헤드 어텐션과 피드 포워드 신경망입니다. 각 서브층 이후에는 드롭 아웃, 잔차 연결과 층 정규화가 수행됩니다.\n",
    "    def call(self, x, enc_output, look_ahead_mask, padding_mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, look_ahead_mask, padding_mask)\n",
    "\n",
    "            attention_weights[f'decoder_layer{i + 1}_block1'] = block1\n",
    "            attention_weights[f'decoder_layer{i + 1}_block2'] = block2\n",
    "\n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def transformer(enc_num_layers, dec_num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "                target_vocab_size, ts_input, ts_target, rate=0.1):\n",
    "    enc_input = tf.keras.Input(shape=(None,))\n",
    "    dec_input = tf.keras.Input(shape=(None,))\n",
    "\n",
    "    enc_padding_mask = tf.keras.layers.Lambda(create_padding_mask, output_shape=(1, 1, None))(enc_input)\n",
    "    look_ahead_mask = tf.keras.layers.Lambda(create_look_ahead_mask, output_shape=(1, None, None))(dec_input)\n",
    "    dec_padding_mask = tf.keras.layers.Lambda(create_padding_mask, output_shape=(1, 1, None))(enc_input)\n",
    "\n",
    "    # enc_output=Encoder(num_layers, d_model, num_heads, dff,\n",
    "    #                          input_vocab_size, ts_input, rate)(enc_input,enc_padding_mask)\n",
    "    # dec_output,_ =Decoder(num_layers, d_model, num_heads, dff,\n",
    "    #                        target_vocab_size, ts_target, rate)(dec_input,enc_output,look_ahead_mask,dec_padding_mask)\n",
    "\n",
    "    # 트랜스포머는 하이퍼파라미터인 enc_num_layers 개수의 인코더 층을 쌓습니다. 해당 코드에서는 총 20개의 인코더 층을 사용하였습니다.\n",
    "    # 이러한 인코더를 하나의 층이라는 개념으로 생각한다면, 하나의 인코더 층은 크게 총 2개의 서브층(sublayer)으로 나뉘어집니다.\n",
    "    # 셀프 어텐션과 피드 포워드 신경망입니다. 즉 멀티 헤드 셀프 어텐션과 포지션 와이즈 피드 포워드 신경망이라고 적혀있으나 멀티 헤드 셀프 어텐션은 셀프 어텐션을 병렬적으로 사용하였다는 의미고, 포지션 와이즈 피드 포워드 신경망은 우리가 알고있는 일반적인 피드 포워드 신경망입니다.\n",
    "    enc_output = Encoder(enc_num_layers, d_model, num_heads, dff,\n",
    "                         input_vocab_size, ts_input, rate)(enc_input, enc_padding_mask)\n",
    "\n",
    "    # 앞선 인코더는 총 enc_num_layers 만큼의 층 연산을 순차적으로 한 후에 마지막 층의 인코더의 출력을 디코너에게 전달합니다.\n",
    "    # 인코더 연산이 끝났으면 디코더 연산이 시작되어 디코더 또한 만큼의 연산을 하는데, 이때마다 인코더가 보낸 출력을 각 디코더 층 연산 (dec_num_layers, 총 40개)를 사용합니다.\n",
    "    dec_output, _ = Decoder(dec_num_layers, d_model, num_heads, dff,\n",
    "                            target_vocab_size, ts_target, rate)(dec_input, enc_output, look_ahead_mask,\n",
    "                                                                dec_padding_mask)\n",
    "    outputs = tf.keras.layers.Dense(target_vocab_size)(dec_output)\n",
    "\n",
    "    return tf.keras.Model(inputs=[enc_input, dec_input], outputs=outputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([False False  True], shape=(3,), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "a = [False, False, True]\n",
    "b = [False, True, True]\n",
    "c = tf.math.logical_and(a, b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(y_true, 0))\n",
    "    loss_ = loss_object(y_true, y_pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)  #padding을 제외한 loss\n",
    "\n",
    "\n",
    "def accuracy_function(y_true, y_pred):\n",
    "    y_hat = tf.cast(tf.argmax(y_pred, axis=2), dtype=tf.float32)  #y_true가 float32이기 때문\n",
    "    accuracies = tf.equal(y_true, y_hat)\n",
    "    #accuracies = tf.equal(y_true, tf.argmax(y_pred, axis=2)) #True, False로 출력\n",
    "\n",
    "    mask = tf.math.logical_not(tf.math.equal(y_true, 0))\n",
    "    accuracies = tf.math.logical_and(mask, accuracies)  #둘다 true일 때만 True\n",
    "\n",
    "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(accuracies) / tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "data_set = tf.data.Dataset.from_tensor_slices(((encoder_input_data, decoder_input_data), decoder_target_data))\n",
    "data_final = data_set.shuffle(1000).batch(64).prefetch(64)  #memory를 차지하지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "transformer() got an unexpected keyword argument 'num_layers'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-52-1b986b97a240>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m model = transformer(num_layers=1, d_model=128, num_heads=4, dff=256,\n\u001B[1;32m----> 2\u001B[1;33m                     input_vocab_size=386, target_vocab_size=359, ts_input=8, ts_target=17, rate=0.2)\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0moptimizer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moptimizers\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mAdam\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcompile\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moptimizer\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0moptimizer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mloss\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mloss_function\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmetrics\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0maccuracy_function\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: transformer() got an unexpected keyword argument 'num_layers'"
     ]
    }
   ],
   "source": [
    "# model = transformer(num_layers=1, d_model=128, num_heads=4, dff=256,\n",
    "#                     input_vocab_size=386, target_vocab_size=359, ts_input=8, ts_target=17, rate=0.2)\n",
    "#\n",
    "# optimizer = tf.keras.optimizers.Adam()\n",
    "# model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy_function])\n",
    "# model.fit(data_final, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "724/782 [==========================>...] - ETA: 6:16 - loss: 3.6270 - accuracy_function: 0.2746"
     ]
    }
   ],
   "source": [
    "# model=transformer(num_layers=1,d_model=128,num_heads=4,dff=256,\n",
    "#                   input_vocab_size=386, target_vocab_size=359,ts_input=8,ts_target=17,rate=0.2)\n",
    "\n",
    "# encoder 의 time_step=20\n",
    "# decoder 의 time_step=40\n",
    "# embedding 노드수는 128. 그리고 head 수는 8개\n",
    "model = transformer(enc_num_layers=20, dec_num_layers=40, d_model=128, num_heads=8, dff=256, input_vocab_size=386,\n",
    "                    target_vocab_size=359, ts_input=8, ts_target=17, rate=0.2)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy_function])\n",
    "model.fit(data_final, epochs=30)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}