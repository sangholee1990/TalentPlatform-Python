{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4020e982",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from tcn import TCN\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error, r2_score,mean_absolute_error\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential , load_model , Model\n",
    "from keras.layers import Dense, Dropout , LSTM , Bidirectional ,GRU ,Flatten,Add,BatchNormalization\n",
    "from keras_layer_normalization import LayerNormalization\n",
    "from keras.initializers import  glorot_normal, RandomUniform\n",
    "from keras import optimizers,Input\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d705cb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(735, 19) (662, 19) (97, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:32: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e3ee63bd34e4e14a40e80d7ce0bd6ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/638 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:44: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb8c425e746a4cde8b5f4b2cea32469d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:\n",
      "(638, 24, 18) (638,)\n",
      "Test size:\n",
      "(73, 24, 18) (73,)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"Mode2_bike_all.csv\")\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "df = df.set_index(\"timestamp\")\n",
    "#df.head()\n",
    "\n",
    "df[\"hour\"] = df.index.hour\n",
    "df[\"day_of_month\"] = df.index.day\n",
    "df[\"day_of_week\"]  = df.index.dayofweek\n",
    "df[\"month\"] = df.index.month\n",
    "\n",
    "training_data_len = math.ceil(len(df) * 0.9) # taking 90% of data to train and 10% of data to test\n",
    "testing_data_len = len(df) - training_data_len\n",
    "\n",
    "time_steps = 24\n",
    "train, test = df.iloc[0:training_data_len], df.iloc[(training_data_len-time_steps):len(df)]\n",
    "print(df.shape, train.shape, test.shape)\n",
    "train_trans = train[['t1','t2', 'hum', 'wind_speed']].to_numpy()\n",
    "test_trans = test[['t1','t2', 'hum', 'wind_speed']].to_numpy()\n",
    "\n",
    "scaler = RobustScaler() # Handles outliers\n",
    "#scaler = MinMaxScaler(feature_range=(0, 1)) # scale to (0,1)\n",
    "train.loc[:, ['t1','t2','hum', 'wind_speed']]=scaler.fit_transform(train_trans)\n",
    "test.loc[:, ['t1','t2', 'hum', 'wind_speed']]=scaler.fit_transform(test_trans)\n",
    "\n",
    "train['cnt'] = scaler.fit_transform(train[['cnt']])\n",
    "test['cnt'] = scaler.fit_transform(test[['cnt']])\n",
    "\n",
    "#Split the data into x_train and y_train data sets\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in tqdm(range(len(train) - time_steps)):\n",
    "    x_train.append(train.drop(columns='cnt').iloc[i:i + time_steps].to_numpy())\n",
    "    y_train.append(train.loc[:,'cnt'].iloc[i + time_steps])\n",
    "\n",
    "#Convert x_train and y_train to numpy arrays\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "#Create the x_test and y_test data sets\n",
    "x_test = []\n",
    "y_test = df.loc[:,'cnt'].iloc[training_data_len:len(df)]\n",
    "\n",
    "for i in tqdm(range(len(test) - time_steps)):\n",
    "    x_test.append(test.drop(columns='cnt').iloc[i:i + time_steps].to_numpy())\n",
    "    # y_test.append(test.loc[:,'cnt'].iloc[i + time_steps])\n",
    "\n",
    "#Convert x_test and y_test to numpy arrays\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# All 12 columns of the data\n",
    "print('Train size:')\n",
    "print(x_train.shape, y_train.shape)\n",
    "print('Test size:')\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3eef8352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 24, 18)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 24, 32)       3360        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 24, 18)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "seq_self_attention_2 (SeqSelfAt (None, 24, 32)       1025        bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 24, 64)       9792        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 24, 32)       0           seq_self_attention_2[0][0]       \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "seq_self_attention_1 (SeqSelfAt (None, 24, 64)       4097        bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_2 (LayerNor (None, 24, 32)       64          add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 24, 64)       0           seq_self_attention_1[0][0]       \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "seq_self_attention_3 (SeqSelfAt (None, 24, 32)       1025        layer_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 24, 64)       128         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_3 (LayerNor (None, 24, 32)       64          seq_self_attention_3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 1536)         0           layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 768)          0           layer_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           24592       flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           12304       flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 16)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 16)           0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1552)         0           dropout_1[0][0]                  \n",
      "                                                                 flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 784)          0           dropout_2[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            1553        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            785         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 1)            0           dense_2[0][0]                    \n",
      "                                                                 dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            2           add_3[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 58,791\n",
      "Trainable params: 58,791\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Concatenate\n",
    "init = glorot_normal(seed=None) # 給 GRU\n",
    "init_d = RandomUniform(minval=-0.05, maxval=0.05) # 給 Dense layer\n",
    "\n",
    "def Encoder(layer):\n",
    "    shortcut = layer\n",
    "    layer = SeqSelfAttention(\n",
    "                     attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n",
    "                     kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     bias_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     attention_regularizer_weight=0.0001)(layer)\n",
    "    layer = Add()([layer,shortcut])\n",
    "    layer = LayerNormalization()(layer)\n",
    "    layer = Flatten()(layer)\n",
    "    \n",
    "    shortcut2 = layer\n",
    "    layer = Dense(16,kernel_initializer=init_d)(layer)\n",
    "    layer = Dropout(0.5)(layer)\n",
    "    layer = Concatenate()([layer,shortcut2])\n",
    "    output = Dense(1,kernel_initializer=init_d)(layer)\n",
    "    return output\n",
    "\n",
    "def Decoder(layer):\n",
    "    shortcut = layer\n",
    "    layer = SeqSelfAttention(\n",
    "                     attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n",
    "                     kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     bias_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     attention_regularizer_weight=0.0001)(layer)\n",
    "    layer = Add()([layer,shortcut])\n",
    "    layer = LayerNormalization()(layer)\n",
    "    layer = SeqSelfAttention(\n",
    "                     attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n",
    "                     kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     bias_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     attention_regularizer_weight=0.0001)(layer)\n",
    "    layer = LayerNormalization()(layer)\n",
    "    \n",
    "    layer = Flatten()(layer)\n",
    "    shortcut2 = layer\n",
    "    layer = Dense(16,kernel_initializer=init_d)(layer)\n",
    "    layer = Dropout(0.3)(layer)\n",
    "    layer = Concatenate()([layer,shortcut2])\n",
    "    output = Dense(1,kernel_initializer=init_d)(layer)\n",
    "    return output\n",
    "\n",
    "def Bi_GRU(layer,unit):\n",
    "    output = Bidirectional(GRU(unit, dropout=0.25, recurrent_dropout=0.25, return_sequences=True,\n",
    "                            kernel_initializer=init))(layer)\n",
    "    return output\n",
    "\n",
    "#start = Input(shape = (x_train.shape[1],x_train.shape[2]))\n",
    "start = Input(shape = (x_train.shape[1:]))\n",
    "start2 = Input(shape = (x_train.shape[1:]))\n",
    "x = Bi_GRU(start,32)\n",
    "x = Encoder(x)\n",
    "\n",
    "y = Bi_GRU(start2,16)\n",
    "y = Decoder(y)\n",
    "\n",
    "Merge = Add()([x,y])\n",
    "Last = Dense(1)(Merge)\n",
    "model = Model([start,start2] , Last)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e1eab9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 574 samples, validate on 64 samples\n",
      "Epoch 1/500\n",
      "574/574 [==============================] - 4s 7ms/step - loss: 3.2204 - val_loss: 2.3456\n",
      "Epoch 2/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 2.3516 - val_loss: 3.6486\n",
      "Epoch 3/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 2.1195 - val_loss: 5.2479\n",
      "Epoch 4/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 2.0044 - val_loss: 2.6993\n",
      "Epoch 5/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.9607 - val_loss: 5.1321\n",
      "Epoch 6/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.9123 - val_loss: 2.3410\n",
      "Epoch 7/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.9509 - val_loss: 3.1571\n",
      "Epoch 8/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.8807 - val_loss: 3.4162\n",
      "Epoch 9/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.7456 - val_loss: 2.5486\n",
      "Epoch 10/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.7373 - val_loss: 3.5057\n",
      "Epoch 11/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.7191 - val_loss: 4.0128\n",
      "Epoch 12/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.7413 - val_loss: 3.7443\n",
      "Epoch 13/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.6542 - val_loss: 4.8351\n",
      "Epoch 14/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.6474 - val_loss: 4.9544\n",
      "Epoch 15/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.5478 - val_loss: 5.1913\n",
      "Epoch 16/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.5597 - val_loss: 3.5136\n",
      "Epoch 17/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.5685 - val_loss: 4.6601\n",
      "Epoch 18/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.5165 - val_loss: 4.1278\n",
      "Epoch 19/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.5506 - val_loss: 7.4972\n",
      "Epoch 20/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.5052 - val_loss: 4.9448\n",
      "Epoch 21/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.4386 - val_loss: 5.2790\n",
      "Epoch 22/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.4239 - val_loss: 4.3769\n",
      "Epoch 23/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.4093 - val_loss: 3.8180\n",
      "Epoch 24/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.4140 - val_loss: 6.0834\n",
      "Epoch 25/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.3744 - val_loss: 5.0906\n",
      "Epoch 26/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.3834 - val_loss: 3.2924\n",
      "Epoch 27/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.3577 - val_loss: 5.1963\n",
      "Epoch 28/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.3981 - val_loss: 4.3419\n",
      "Epoch 29/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.3069 - val_loss: 4.3394\n",
      "Epoch 30/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.3184 - val_loss: 5.8625\n",
      "Epoch 31/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.2769 - val_loss: 5.7934\n",
      "Epoch 32/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.2824 - val_loss: 8.4002\n",
      "Epoch 33/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.2369 - val_loss: 7.9654\n",
      "Epoch 34/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.2159 - val_loss: 5.7525\n",
      "Epoch 35/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.2674 - val_loss: 5.2064\n",
      "Epoch 36/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.2078 - val_loss: 3.3681\n",
      "Epoch 37/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.2435 - val_loss: 4.6161\n",
      "Epoch 38/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.2453 - val_loss: 3.1609\n",
      "Epoch 39/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.2276 - val_loss: 3.8107\n",
      "Epoch 40/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.1600 - val_loss: 4.0604\n",
      "Epoch 41/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.1419 - val_loss: 4.0381\n",
      "Epoch 42/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.1594 - val_loss: 4.3187\n",
      "Epoch 43/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.1226 - val_loss: 5.2620\n",
      "Epoch 44/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.1112 - val_loss: 4.5796\n",
      "Epoch 45/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.1145 - val_loss: 4.4971\n",
      "Epoch 46/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.1191 - val_loss: 4.5561\n",
      "Epoch 47/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.0925 - val_loss: 4.1747\n",
      "Epoch 48/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.0974 - val_loss: 3.2470\n",
      "Epoch 49/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.0804 - val_loss: 3.0312\n",
      "Epoch 50/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.0679 - val_loss: 4.2712\n",
      "Epoch 51/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.0087 - val_loss: 3.4280\n",
      "Epoch 52/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.0691 - val_loss: 4.5840\n",
      "Epoch 53/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.0575 - val_loss: 3.9993\n",
      "Epoch 54/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.0356 - val_loss: 5.8879\n",
      "Epoch 55/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.0248 - val_loss: 4.4668\n",
      "Epoch 56/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.9767 - val_loss: 4.9125\n",
      "Epoch 57/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.9998 - val_loss: 3.9795\n",
      "Epoch 58/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.0095 - val_loss: 2.5244\n",
      "Epoch 59/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.0085 - val_loss: 3.4209\n",
      "Epoch 60/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.9785 - val_loss: 4.6993\n",
      "Epoch 61/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.9753 - val_loss: 4.1672\n",
      "Epoch 62/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.9467 - val_loss: 4.8395\n",
      "Epoch 63/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.9783 - val_loss: 4.3002\n",
      "Epoch 64/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.9720 - val_loss: 4.6229\n",
      "Epoch 65/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.9277 - val_loss: 6.6345\n",
      "Epoch 66/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.9313 - val_loss: 6.2647\n",
      "Epoch 67/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.9339 - val_loss: 4.5272\n",
      "Epoch 68/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.9371 - val_loss: 3.8987\n",
      "Epoch 69/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.9094 - val_loss: 6.3062\n",
      "Epoch 70/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.8853 - val_loss: 7.2073\n",
      "Epoch 71/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.9098 - val_loss: 6.2072\n",
      "Epoch 72/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.9059 - val_loss: 5.5044\n",
      "Epoch 73/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.8926 - val_loss: 6.0832\n",
      "Epoch 74/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.9145 - val_loss: 5.3275\n",
      "Epoch 75/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.8820 - val_loss: 6.2748\n",
      "Epoch 76/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.9009 - val_loss: 5.6449\n",
      "Epoch 77/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.8638 - val_loss: 6.0808\n",
      "Epoch 78/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.8271 - val_loss: 3.5943\n",
      "Epoch 79/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.9146 - val_loss: 4.7655\n",
      "Epoch 80/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.8158 - val_loss: 6.3686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.8055 - val_loss: 5.6102\n",
      "Epoch 82/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7908 - val_loss: 4.8602\n",
      "Epoch 83/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7853 - val_loss: 5.6376\n",
      "Epoch 84/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.8402 - val_loss: 4.7262\n",
      "Epoch 85/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7902 - val_loss: 4.7730\n",
      "Epoch 86/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.8104 - val_loss: 5.6916\n",
      "Epoch 87/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7760 - val_loss: 6.4442\n",
      "Epoch 88/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.8136 - val_loss: 5.1550\n",
      "Epoch 89/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7764 - val_loss: 6.4742\n",
      "Epoch 90/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7868 - val_loss: 5.1260\n",
      "Epoch 91/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7971 - val_loss: 5.4867\n",
      "Epoch 92/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7278 - val_loss: 7.0718\n",
      "Epoch 93/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7601 - val_loss: 5.9681\n",
      "Epoch 94/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7412 - val_loss: 4.7157\n",
      "Epoch 95/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7627 - val_loss: 4.3309\n",
      "Epoch 96/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7514 - val_loss: 4.2380\n",
      "Epoch 97/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7372 - val_loss: 4.2890\n",
      "Epoch 98/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7313 - val_loss: 5.5839\n",
      "Epoch 99/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7606 - val_loss: 5.3136\n",
      "Epoch 100/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7453 - val_loss: 5.1306\n",
      "Epoch 101/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7408 - val_loss: 4.0977\n",
      "Epoch 102/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6901 - val_loss: 5.1735\n",
      "Epoch 103/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7664 - val_loss: 7.0474\n",
      "Epoch 104/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7474 - val_loss: 5.2342\n",
      "Epoch 105/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7179 - val_loss: 5.2378\n",
      "Epoch 106/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6828 - val_loss: 6.2081\n",
      "Epoch 107/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7215 - val_loss: 5.3203\n",
      "Epoch 108/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7321 - val_loss: 4.7914\n",
      "Epoch 109/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6995 - val_loss: 6.5091\n",
      "Epoch 110/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7068 - val_loss: 5.5109\n",
      "Epoch 111/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6895 - val_loss: 4.9117\n",
      "Epoch 112/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6979 - val_loss: 3.9570\n",
      "Epoch 113/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6839 - val_loss: 3.0505\n",
      "Epoch 114/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6572 - val_loss: 3.4574\n",
      "Epoch 115/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6741 - val_loss: 3.9925\n",
      "Epoch 116/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6768 - val_loss: 3.8218\n",
      "Epoch 117/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6817 - val_loss: 4.7616\n",
      "Epoch 118/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6694 - val_loss: 4.6737\n",
      "Epoch 119/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6595 - val_loss: 5.4567\n",
      "Epoch 120/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7108 - val_loss: 6.5547\n",
      "Epoch 121/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6692 - val_loss: 5.8486\n",
      "Epoch 122/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6296 - val_loss: 6.4666\n",
      "Epoch 123/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6347 - val_loss: 5.2196\n",
      "Epoch 124/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6665 - val_loss: 3.5634\n",
      "Epoch 125/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6225 - val_loss: 5.8248\n",
      "Epoch 126/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6290 - val_loss: 4.6417\n",
      "Epoch 127/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6305 - val_loss: 6.7144\n",
      "Epoch 128/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6374 - val_loss: 4.6980\n",
      "Epoch 129/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6149 - val_loss: 4.9430\n",
      "Epoch 130/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6120 - val_loss: 5.4592\n",
      "Epoch 131/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6270 - val_loss: 4.8033\n",
      "Epoch 132/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5822 - val_loss: 4.9906\n",
      "Epoch 133/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5855 - val_loss: 4.2325\n",
      "Epoch 134/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6038 - val_loss: 3.8243\n",
      "Epoch 135/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6155 - val_loss: 3.4531\n",
      "Epoch 136/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5848 - val_loss: 4.2533\n",
      "Epoch 137/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5912 - val_loss: 4.7855\n",
      "Epoch 138/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6051 - val_loss: 3.4004\n",
      "Epoch 139/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6005 - val_loss: 3.9440\n",
      "Epoch 140/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5875 - val_loss: 3.3331\n",
      "Epoch 141/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5565 - val_loss: 3.3752\n",
      "Epoch 142/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5842 - val_loss: 4.6085\n",
      "Epoch 143/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5607 - val_loss: 4.9632\n",
      "Epoch 144/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5550 - val_loss: 3.7161\n",
      "Epoch 145/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5671 - val_loss: 3.2021\n",
      "Epoch 146/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5590 - val_loss: 5.1198\n",
      "Epoch 147/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5232 - val_loss: 6.1018\n",
      "Epoch 148/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5841 - val_loss: 6.8653\n",
      "Epoch 149/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5676 - val_loss: 4.5359\n",
      "Epoch 150/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5282 - val_loss: 3.8802\n",
      "Epoch 151/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5562 - val_loss: 4.1641\n",
      "Epoch 152/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5352 - val_loss: 4.1357\n",
      "Epoch 153/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5242 - val_loss: 4.3429\n",
      "Epoch 154/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5182 - val_loss: 4.8085\n",
      "Epoch 155/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5141 - val_loss: 3.7705\n",
      "Epoch 156/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5305 - val_loss: 3.8912\n",
      "Epoch 157/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5162 - val_loss: 4.1498\n",
      "Epoch 158/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5401 - val_loss: 4.0345\n",
      "Epoch 159/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5154 - val_loss: 4.4028\n",
      "Epoch 160/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5100 - val_loss: 3.9104\n",
      "Epoch 161/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5045 - val_loss: 4.0525\n",
      "Epoch 162/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5151 - val_loss: 5.3184\n",
      "Epoch 163/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4991 - val_loss: 4.0759\n",
      "Epoch 164/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5166 - val_loss: 4.4016\n",
      "Epoch 165/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5120 - val_loss: 2.7990\n",
      "Epoch 166/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4737 - val_loss: 2.9117\n",
      "Epoch 167/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4698 - val_loss: 3.2576\n",
      "Epoch 168/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4860 - val_loss: 2.7894\n",
      "Epoch 169/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5007 - val_loss: 2.7728\n",
      "Epoch 170/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4913 - val_loss: 3.5310\n",
      "Epoch 171/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4777 - val_loss: 3.0351\n",
      "Epoch 172/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4717 - val_loss: 2.9896\n",
      "Epoch 173/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4848 - val_loss: 3.3354\n",
      "Epoch 174/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4814 - val_loss: 3.7818\n",
      "Epoch 175/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5028 - val_loss: 4.0060\n",
      "Epoch 176/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4997 - val_loss: 3.3495\n",
      "Epoch 177/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4612 - val_loss: 2.8546\n",
      "Epoch 178/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4404 - val_loss: 3.1637\n",
      "Epoch 179/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4682 - val_loss: 3.5124\n",
      "Epoch 180/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4420 - val_loss: 3.8106\n",
      "Epoch 181/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4382 - val_loss: 3.5254\n",
      "Epoch 182/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4589 - val_loss: 2.3535\n",
      "Epoch 183/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4562 - val_loss: 2.5344\n",
      "Epoch 184/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4555 - val_loss: 2.3698\n",
      "Epoch 185/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4539 - val_loss: 2.9527\n",
      "Epoch 186/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4410 - val_loss: 3.7025\n",
      "Epoch 187/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4493 - val_loss: 3.1682\n",
      "Epoch 188/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4337 - val_loss: 2.7232\n",
      "Epoch 189/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4357 - val_loss: 3.4312\n",
      "Epoch 190/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4233 - val_loss: 3.1818\n",
      "Epoch 191/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4354 - val_loss: 3.6916\n",
      "Epoch 192/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4424 - val_loss: 3.0370\n",
      "Epoch 193/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4165 - val_loss: 2.7211\n",
      "Epoch 194/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4288 - val_loss: 3.0809\n",
      "Epoch 195/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4193 - val_loss: 2.1803\n",
      "Epoch 196/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4201 - val_loss: 2.6225\n",
      "Epoch 197/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4154 - val_loss: 2.4804\n",
      "Epoch 198/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4260 - val_loss: 2.2528\n",
      "Epoch 199/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4131 - val_loss: 2.3004\n",
      "Epoch 200/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3950 - val_loss: 2.3473\n",
      "Epoch 201/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4191 - val_loss: 2.0662\n",
      "Epoch 202/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4155 - val_loss: 2.3138\n",
      "Epoch 203/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4232 - val_loss: 1.8392\n",
      "Epoch 204/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4101 - val_loss: 2.5706\n",
      "Epoch 205/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4068 - val_loss: 2.5772\n",
      "Epoch 206/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4014 - val_loss: 1.8198\n",
      "Epoch 207/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3979 - val_loss: 1.7852\n",
      "Epoch 208/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4074 - val_loss: 1.9895\n",
      "Epoch 209/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3966 - val_loss: 2.4693\n",
      "Epoch 210/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3850 - val_loss: 2.1567\n",
      "Epoch 211/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4232 - val_loss: 1.6713\n",
      "Epoch 212/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3933 - val_loss: 1.0918\n",
      "Epoch 213/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4086 - val_loss: 1.3699\n",
      "Epoch 214/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4097 - val_loss: 1.3118\n",
      "Epoch 215/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3920 - val_loss: 1.0897\n",
      "Epoch 216/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4015 - val_loss: 0.9343\n",
      "Epoch 217/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3783 - val_loss: 1.3536\n",
      "Epoch 218/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3662 - val_loss: 1.1941\n",
      "Epoch 219/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3717 - val_loss: 1.0162\n",
      "Epoch 220/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3562 - val_loss: 1.1228\n",
      "Epoch 221/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3789 - val_loss: 1.0691\n",
      "Epoch 222/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3831 - val_loss: 1.2167\n",
      "Epoch 223/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3731 - val_loss: 1.2116\n",
      "Epoch 224/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3780 - val_loss: 1.1117\n",
      "Epoch 225/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3649 - val_loss: 1.1464\n",
      "Epoch 226/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3808 - val_loss: 1.1105\n",
      "Epoch 227/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3713 - val_loss: 0.9406\n",
      "Epoch 228/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3636 - val_loss: 0.8877\n",
      "Epoch 229/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3634 - val_loss: 1.1118\n",
      "Epoch 230/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3532 - val_loss: 1.1372\n",
      "Epoch 231/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3690 - val_loss: 1.4192\n",
      "Epoch 232/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3334 - val_loss: 0.9089\n",
      "Epoch 233/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3702 - val_loss: 0.8518\n",
      "Epoch 234/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3566 - val_loss: 0.9386\n",
      "Epoch 235/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3323 - val_loss: 1.0361\n",
      "Epoch 236/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3505 - val_loss: 1.2930\n",
      "Epoch 237/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3584 - val_loss: 1.2041\n",
      "Epoch 238/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3331 - val_loss: 1.2832\n",
      "Epoch 239/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3385 - val_loss: 0.9113\n",
      "Epoch 240/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3597 - val_loss: 0.9399\n",
      "Epoch 241/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3337 - val_loss: 1.0602\n",
      "Epoch 242/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3151 - val_loss: 1.1589\n",
      "Epoch 243/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3494 - val_loss: 0.9814\n",
      "Epoch 244/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3421 - val_loss: 0.8389\n",
      "Epoch 245/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3220 - val_loss: 0.9719\n",
      "Epoch 246/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3238 - val_loss: 0.9466\n",
      "Epoch 247/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3207 - val_loss: 0.8916\n",
      "Epoch 248/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3394 - val_loss: 0.8207\n",
      "Epoch 249/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3149 - val_loss: 1.2794\n",
      "Epoch 250/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3169 - val_loss: 0.9751\n",
      "Epoch 251/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2936 - val_loss: 0.8355\n",
      "Epoch 252/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2993 - val_loss: 0.8190\n",
      "Epoch 253/500\n",
      "574/574 [==============================] - ETA: 0s - loss: 0.298 - 2s 3ms/step - loss: 0.3129 - val_loss: 0.7046\n",
      "Epoch 254/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2866 - val_loss: 0.9180\n",
      "Epoch 255/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3027 - val_loss: 1.3210\n",
      "Epoch 256/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3069 - val_loss: 1.3057\n",
      "Epoch 257/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3036 - val_loss: 1.0966\n",
      "Epoch 258/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3162 - val_loss: 1.0281\n",
      "Epoch 259/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3013 - val_loss: 0.9443\n",
      "Epoch 260/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3012 - val_loss: 1.1776\n",
      "Epoch 261/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2925 - val_loss: 1.3662\n",
      "Epoch 262/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2883 - val_loss: 1.1754\n",
      "Epoch 263/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2795 - val_loss: 1.1914\n",
      "Epoch 264/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3000 - val_loss: 1.3938\n",
      "Epoch 265/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2783 - val_loss: 1.4779\n",
      "Epoch 266/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2767 - val_loss: 1.3994\n",
      "Epoch 267/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2706 - val_loss: 1.2169\n",
      "Epoch 268/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2850 - val_loss: 1.2821\n",
      "Epoch 269/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2827 - val_loss: 1.0038\n",
      "Epoch 270/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2928 - val_loss: 0.8853\n",
      "Epoch 271/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2867 - val_loss: 1.0779\n",
      "Epoch 272/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2806 - val_loss: 0.8559\n",
      "Epoch 273/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2829 - val_loss: 0.7212\n",
      "Epoch 274/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2713 - val_loss: 1.0352\n",
      "Epoch 275/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2667 - val_loss: 1.1461\n",
      "Epoch 276/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2963 - val_loss: 0.7783\n",
      "Epoch 277/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2720 - val_loss: 0.8120\n",
      "Epoch 278/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2637 - val_loss: 0.6567\n",
      "Epoch 279/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2761 - val_loss: 0.7694\n",
      "Epoch 280/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2671 - val_loss: 1.1255\n",
      "Epoch 281/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2709 - val_loss: 1.1836\n",
      "Epoch 282/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2521 - val_loss: 0.9177\n",
      "Epoch 283/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2741 - val_loss: 0.8129\n",
      "Epoch 284/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2497 - val_loss: 0.9194\n",
      "Epoch 285/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2561 - val_loss: 0.9514\n",
      "Epoch 286/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2711 - val_loss: 1.0289\n",
      "Epoch 287/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2525 - val_loss: 1.1867\n",
      "Epoch 288/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2540 - val_loss: 1.0819\n",
      "Epoch 289/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2477 - val_loss: 1.3574\n",
      "Epoch 290/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2637 - val_loss: 1.1096\n",
      "Epoch 291/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2588 - val_loss: 1.0117\n",
      "Epoch 292/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2647 - val_loss: 0.9432\n",
      "Epoch 293/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2519 - val_loss: 0.9937\n",
      "Epoch 294/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2428 - val_loss: 1.0423\n",
      "Epoch 295/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2523 - val_loss: 1.1202\n",
      "Epoch 296/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2520 - val_loss: 0.9144\n",
      "Epoch 297/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2393 - val_loss: 0.9220\n",
      "Epoch 298/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2405 - val_loss: 0.8182\n",
      "Epoch 299/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2372 - val_loss: 0.7737\n",
      "Epoch 300/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2297 - val_loss: 0.7363\n",
      "Epoch 301/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2401 - val_loss: 0.9346\n",
      "Epoch 302/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2566 - val_loss: 1.1628\n",
      "Epoch 303/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2298 - val_loss: 0.9055\n",
      "Epoch 304/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2399 - val_loss: 1.0412\n",
      "Epoch 305/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2342 - val_loss: 0.7533\n",
      "Epoch 306/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2410 - val_loss: 1.2542\n",
      "Epoch 307/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2378 - val_loss: 0.6068\n",
      "Epoch 308/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2206 - val_loss: 0.6944\n",
      "Epoch 309/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2382 - val_loss: 0.6975\n",
      "Epoch 310/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2401 - val_loss: 0.6204\n",
      "Epoch 311/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2214 - val_loss: 0.7137\n",
      "Epoch 312/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2287 - val_loss: 0.8085\n",
      "Epoch 313/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2442 - val_loss: 0.7035\n",
      "Epoch 314/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2218 - val_loss: 0.6327\n",
      "Epoch 315/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2307 - val_loss: 0.6267\n",
      "Epoch 316/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2187 - val_loss: 0.6556\n",
      "Epoch 317/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2483 - val_loss: 0.5725\n",
      "Epoch 318/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2308 - val_loss: 0.6214\n",
      "Epoch 319/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2287 - val_loss: 0.6501\n",
      "Epoch 320/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2111 - val_loss: 0.5769\n",
      "Epoch 321/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2066 - val_loss: 0.6429\n",
      "Epoch 322/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2158 - val_loss: 0.5440\n",
      "Epoch 323/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2249 - val_loss: 0.7049\n",
      "Epoch 324/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2290 - val_loss: 0.4805\n",
      "Epoch 325/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2110 - val_loss: 0.4688\n",
      "Epoch 326/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2201 - val_loss: 0.6048\n",
      "Epoch 327/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2406 - val_loss: 0.6087\n",
      "Epoch 328/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2023 - val_loss: 0.5947\n",
      "Epoch 329/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2093 - val_loss: 0.5587\n",
      "Epoch 330/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2241 - val_loss: 0.5374\n",
      "Epoch 331/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2325 - val_loss: 0.5190\n",
      "Epoch 332/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2221 - val_loss: 0.7053\n",
      "Epoch 333/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2165 - val_loss: 0.7696\n",
      "Epoch 334/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2235 - val_loss: 0.6076\n",
      "Epoch 335/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2120 - val_loss: 0.6743\n",
      "Epoch 336/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2111 - val_loss: 0.6281\n",
      "Epoch 337/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1904 - val_loss: 0.8289\n",
      "Epoch 338/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2012 - val_loss: 0.6472\n",
      "Epoch 339/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2057 - val_loss: 0.5234\n",
      "Epoch 340/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2227 - val_loss: 0.9457\n",
      "Epoch 341/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2092 - val_loss: 0.9554\n",
      "Epoch 342/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1939 - val_loss: 0.9026\n",
      "Epoch 343/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2126 - val_loss: 0.7729\n",
      "Epoch 344/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1873 - val_loss: 0.7200\n",
      "Epoch 345/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2041 - val_loss: 0.6125\n",
      "Epoch 346/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2036 - val_loss: 0.6623\n",
      "Epoch 347/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1943 - val_loss: 0.9778\n",
      "Epoch 348/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2123 - val_loss: 0.5343\n",
      "Epoch 349/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2129 - val_loss: 0.6220\n",
      "Epoch 350/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2027 - val_loss: 0.7007\n",
      "Epoch 351/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1932 - val_loss: 0.8626\n",
      "Epoch 352/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2147 - val_loss: 0.7874\n",
      "Epoch 353/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2111 - val_loss: 0.6970\n",
      "Epoch 354/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2040 - val_loss: 0.9040\n",
      "Epoch 355/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1953 - val_loss: 0.6624\n",
      "Epoch 356/500\n",
      "574/574 [==============================] - ETA: 0s - loss: 0.198 - 2s 3ms/step - loss: 0.1988 - val_loss: 0.6710\n",
      "Epoch 357/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2019 - val_loss: 0.5332\n",
      "Epoch 358/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1978 - val_loss: 0.5663\n",
      "Epoch 359/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1903 - val_loss: 0.7872\n",
      "Epoch 360/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1973 - val_loss: 0.6152\n",
      "Epoch 361/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1987 - val_loss: 0.7683\n",
      "Epoch 362/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1931 - val_loss: 0.7002\n",
      "Epoch 363/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1874 - val_loss: 0.7292\n",
      "Epoch 364/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1843 - val_loss: 0.8470\n",
      "Epoch 365/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1811 - val_loss: 0.7245\n",
      "Epoch 366/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2003 - val_loss: 0.6747\n",
      "Epoch 367/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1912 - val_loss: 0.7175\n",
      "Epoch 368/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1915 - val_loss: 0.5811\n",
      "Epoch 369/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1855 - val_loss: 0.7138\n",
      "Epoch 370/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1969 - val_loss: 0.6892\n",
      "Epoch 371/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1763 - val_loss: 0.7911\n",
      "Epoch 372/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1730 - val_loss: 0.6315\n",
      "Epoch 373/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1888 - val_loss: 0.6912\n",
      "Epoch 374/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1925 - val_loss: 0.7240\n",
      "Epoch 375/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1855 - val_loss: 0.6488\n",
      "Epoch 376/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1884 - val_loss: 0.5917\n",
      "Epoch 377/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1743 - val_loss: 0.5832\n",
      "Epoch 378/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1801 - val_loss: 0.6527\n",
      "Epoch 379/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1777 - val_loss: 0.8093\n",
      "Epoch 380/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1829 - val_loss: 0.5767\n",
      "Epoch 381/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1731 - val_loss: 0.7113\n",
      "Epoch 382/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1841 - val_loss: 0.5180\n",
      "Epoch 383/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1815 - val_loss: 0.5257\n",
      "Epoch 384/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1921 - val_loss: 0.8839\n",
      "Epoch 385/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1829 - val_loss: 0.6319\n",
      "Epoch 386/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1978 - val_loss: 0.5911\n",
      "Epoch 387/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1859 - val_loss: 0.7402\n",
      "Epoch 388/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1736 - val_loss: 0.6498\n",
      "Epoch 389/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1697 - val_loss: 0.6107\n",
      "Epoch 390/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1796 - val_loss: 0.4844\n",
      "Epoch 391/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1713 - val_loss: 0.6997\n",
      "Epoch 392/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1636 - val_loss: 0.6785\n",
      "Epoch 393/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1676 - val_loss: 0.5793\n",
      "Epoch 394/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1820 - val_loss: 0.7023\n",
      "Epoch 395/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1781 - val_loss: 0.5634\n",
      "Epoch 396/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1689 - val_loss: 0.5569\n",
      "Epoch 397/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1657 - val_loss: 0.5519\n",
      "Epoch 398/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1772 - val_loss: 0.4649\n",
      "Epoch 399/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1663 - val_loss: 0.6190\n",
      "Epoch 400/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1680 - val_loss: 0.5089\n",
      "Epoch 401/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1558 - val_loss: 0.6352\n",
      "Epoch 402/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1638 - val_loss: 0.5868\n",
      "Epoch 403/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1617 - val_loss: 0.5088\n",
      "Epoch 404/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1738 - val_loss: 0.5213\n",
      "Epoch 405/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1822 - val_loss: 0.5175\n",
      "Epoch 406/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1590 - val_loss: 0.4953\n",
      "Epoch 407/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1679 - val_loss: 0.4541\n",
      "Epoch 408/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1571 - val_loss: 0.4888\n",
      "Epoch 409/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1540 - val_loss: 0.4576\n",
      "Epoch 410/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1676 - val_loss: 0.4265\n",
      "Epoch 411/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1628 - val_loss: 0.5124\n",
      "Epoch 412/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1461 - val_loss: 0.5305\n",
      "Epoch 413/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1690 - val_loss: 0.5357\n",
      "Epoch 414/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1810 - val_loss: 0.5277\n",
      "Epoch 415/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1590 - val_loss: 0.5601\n",
      "Epoch 416/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1657 - val_loss: 0.5250\n",
      "Epoch 417/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1510 - val_loss: 0.4819\n",
      "Epoch 418/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1543 - val_loss: 0.7212\n",
      "Epoch 419/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1547 - val_loss: 0.6102\n",
      "Epoch 420/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1542 - val_loss: 0.6843\n",
      "Epoch 421/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1483 - val_loss: 0.5585\n",
      "Epoch 422/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1559 - val_loss: 0.6629\n",
      "Epoch 423/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1649 - val_loss: 0.5920\n",
      "Epoch 424/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1463 - val_loss: 0.4570\n",
      "Epoch 425/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1538 - val_loss: 0.5159\n",
      "Epoch 426/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1559 - val_loss: 0.5382\n",
      "Epoch 427/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1462 - val_loss: 0.5947\n",
      "Epoch 428/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1616 - val_loss: 0.5815\n",
      "Epoch 429/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1579 - val_loss: 0.6133\n",
      "Epoch 430/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1460 - val_loss: 0.7602\n",
      "Epoch 431/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1580 - val_loss: 0.6164\n",
      "Epoch 432/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1583 - val_loss: 0.6485\n",
      "Epoch 433/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1610 - val_loss: 0.4566\n",
      "Epoch 434/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1531 - val_loss: 0.5632\n",
      "Epoch 435/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1483 - val_loss: 0.7899\n",
      "Epoch 436/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1487 - val_loss: 0.4497\n",
      "Epoch 437/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1579 - val_loss: 0.5914\n",
      "Epoch 438/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1478 - val_loss: 0.5795\n",
      "Epoch 439/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1547 - val_loss: 0.4737\n",
      "Epoch 440/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1580 - val_loss: 0.7849\n",
      "Epoch 441/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1547 - val_loss: 0.5110\n",
      "Epoch 442/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1526 - val_loss: 0.6158\n",
      "Epoch 443/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1430 - val_loss: 0.5359\n",
      "Epoch 444/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1424 - val_loss: 0.6182\n",
      "Epoch 445/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1637 - val_loss: 0.5455\n",
      "Epoch 446/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1616 - val_loss: 0.7867\n",
      "Epoch 447/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1629 - val_loss: 0.4788\n",
      "Epoch 448/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1611 - val_loss: 0.4373\n",
      "Epoch 449/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1516 - val_loss: 0.4454\n",
      "Epoch 450/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1578 - val_loss: 0.4538\n",
      "Epoch 451/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1556 - val_loss: 0.6259\n",
      "Epoch 452/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1491 - val_loss: 0.6563\n",
      "Epoch 453/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1591 - val_loss: 0.5573\n",
      "Epoch 454/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1348 - val_loss: 0.4887\n",
      "Epoch 455/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1466 - val_loss: 0.5817\n",
      "Epoch 456/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1489 - val_loss: 0.5864\n",
      "Epoch 457/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1381 - val_loss: 0.5320\n",
      "Epoch 458/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1511 - val_loss: 0.7088\n",
      "Epoch 459/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1435 - val_loss: 0.4119\n",
      "Epoch 460/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1460 - val_loss: 0.4735\n",
      "Epoch 461/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1527 - val_loss: 0.5974\n",
      "Epoch 462/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1605 - val_loss: 0.5303\n",
      "Epoch 463/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1540 - val_loss: 0.4597\n",
      "Epoch 464/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1463 - val_loss: 0.6572\n",
      "Epoch 465/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1506 - val_loss: 0.6554\n",
      "Epoch 466/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1609 - val_loss: 0.6456\n",
      "Epoch 467/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1484 - val_loss: 0.5680\n",
      "Epoch 468/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1384 - val_loss: 0.5585\n",
      "Epoch 469/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1490 - val_loss: 0.4933\n",
      "Epoch 470/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1523 - val_loss: 0.4869\n",
      "Epoch 471/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1411 - val_loss: 0.4743\n",
      "Epoch 472/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1400 - val_loss: 0.5522\n",
      "Epoch 473/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1458 - val_loss: 0.7007\n",
      "Epoch 474/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1333 - val_loss: 0.7147\n",
      "Epoch 475/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1352 - val_loss: 0.5350\n",
      "Epoch 476/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1348 - val_loss: 0.6395\n",
      "Epoch 477/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1431 - val_loss: 0.5825\n",
      "Epoch 478/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1337 - val_loss: 0.5734\n",
      "Epoch 479/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1334 - val_loss: 0.5437\n",
      "Epoch 480/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1408 - val_loss: 0.6242\n",
      "Epoch 481/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1339 - val_loss: 0.5104\n",
      "Epoch 482/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1326 - val_loss: 0.5559\n",
      "Epoch 483/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1286 - val_loss: 0.5496\n",
      "Epoch 484/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1370 - val_loss: 0.4807\n",
      "Epoch 485/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1313 - val_loss: 0.5233\n",
      "Epoch 486/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1289 - val_loss: 0.5136\n",
      "Epoch 487/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1317 - val_loss: 0.4480\n",
      "Epoch 488/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1446 - val_loss: 0.4304\n",
      "Epoch 489/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1322 - val_loss: 0.5885\n",
      "Epoch 490/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1282 - val_loss: 0.5145\n",
      "Epoch 491/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1334 - val_loss: 0.6126\n",
      "Epoch 492/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1385 - val_loss: 0.5942\n",
      "Epoch 493/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1314 - val_loss: 0.5297\n",
      "Epoch 494/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1517 - val_loss: 0.7140\n",
      "Epoch 495/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1361 - val_loss: 0.7250\n",
      "Epoch 496/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1356 - val_loss: 0.5340\n",
      "Epoch 497/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1261 - val_loss: 0.4942\n",
      "Epoch 498/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1262 - val_loss: 0.4463\n",
      "Epoch 499/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1298 - val_loss: 0.4525\n",
      "Epoch 500/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1361 - val_loss: 0.5381\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 24, 18)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 24, 32)       3360        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 24, 18)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "seq_self_attention_2 (SeqSelfAt (None, 24, 32)       1025        bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 24, 64)       9792        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 24, 32)       0           seq_self_attention_2[0][0]       \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "seq_self_attention_1 (SeqSelfAt (None, 24, 64)       4097        bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_2 (LayerNor (None, 24, 32)       64          add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 24, 64)       0           seq_self_attention_1[0][0]       \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "seq_self_attention_3 (SeqSelfAt (None, 24, 32)       1025        layer_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 24, 64)       128         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_3 (LayerNor (None, 24, 32)       64          seq_self_attention_3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 1536)         0           layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 768)          0           layer_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           24592       flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           12304       flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 16)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 16)           0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1552)         0           dropout_1[0][0]                  \n",
      "                                                                 flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 784)          0           dropout_2[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            1553        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            785         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 1)            0           dense_2[0][0]                    \n",
      "                                                                 dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            2           add_3[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 58,791\n",
      "Trainable params: 58,791\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict time:  0.5066235065460205\n",
      "RMSE2:  9.415135784616842\n",
      "MAE2:  6.567897101124264\n",
      "R-square2:  -0.6755745920565508\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'mse score')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEGCAYAAABvtY4XAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAABNYElEQVR4nO2dd5wdVfn/3+eW7S190xshlIQETCCUhKbAl27BCEEEESyIgApSLPwUFBV7oShFECkCCtJ7r0lID0lI39TdzfZ22/n9cWb2zp1bt9ytz/v12tedO/XMDXzmmc95znOU1hpBEARh4OHp7QYIgiAI2UEEXhAEYYAiAi8IgjBAEYEXBEEYoIjAC4IgDFB8vd0AJ8OHD9eTJk3q7WYIgiD0G5YsWVKltR6RaFufEvhJkyaxePHi3m6GIAhCv0EptTXZNrFoBEEQBigi8IIgCAMUEXhBEIQBSp/y4AVBGHwEg0EqKipobW3t7ab0afLy8hg3bhx+vz/jY0TgBUHoVSoqKiguLmbSpEkopXq7OX0SrTXV1dVUVFQwefLkjI8Ti0YQhF6ltbWVYcOGibinQCnFsGHDOvyWIwIvCEKvI+Kens78RiLwblb/F5qqersVgiAIXUYE3klLLfz7K/DAF3q7JYIg9CBFRUW93YSsIAKfiL1re7sFgiAIXUYE3omOmM+QpGsJwmBEa83VV1/NjBkzmDlzJg8//DAAu3btYsGCBcyePZsZM2bw5ptvEg6HufDCC9v3/d3vftfLrY9H0iSd2AIvCEKv8P/+t5o1O+u79ZwHjSnhJ2ccnNG+jz/+OMuWLWP58uVUVVUxd+5cFixYwL/+9S9OPvlkbrjhBsLhMM3NzSxbtowdO3awatUqAGpra7u13d2BRPBOIuHeboEgCL3IW2+9xbnnnovX62XUqFEce+yxfPjhh8ydO5d77rmHG2+8kZUrV1JcXMyUKVPYtGkTl19+Oc899xwlJSW93fw4JIJ3IhG8IPQqmUbaPc2CBQt44403ePrpp7nwwgv57ne/ywUXXMDy5ct5/vnnuf3223nkkUe4++67e7upMUgE70RLBC8Ig5n58+fz8MMPEw6Hqays5I033uDwww9n69atjBo1iksuuYSvfe1rLF26lKqqKiKRCJ///Oe56aabWLp0aW83P46sRvBKqauArwEaWAlcpLXuuz2YEsELwqDms5/9LO+++y6zZs1CKcWvfvUrysvL+cc//sGvf/1r/H4/RUVF3HfffezYsYOLLrqISMToxi9+8Ytebn08SmudnRMrNRZ4CzhIa92ilHoEeEZrfW+yY+bMmaN7dcKPfZvhj7PN8o11vdcOQRhErF27lgMPPLC3m9EvSPRbKaWWaK3nJNo/2xaND8hXSvmAAmBnlq/XNZwRfESieUEQ+jdZE3it9Q7gVmAbsAuo01q/4N5PKXWpUmqxUmpxZWVltpqTGU6BDzT2XjsEQRC6gawJvFJqCHAWMBkYAxQqpc5376e1vlNrPUdrPWfEiITzxvYcMRF8qPfaIQiC0A1k06L5NLBZa12ptQ4CjwNHZfF6XceZBy858YIg9HOyKfDbgHlKqQJl6lyeCPTtIi/OCF5SJgVB6Odk04N/H3gUWIpJkfQAd2bret2CU9QlZVIQhH5OVvPgtdY/AX6SzWt0KzEevETwgiD0b2Qkq5OIWDSCIKQmVe34LVu2MGPGjB5sTWpE4J1IBC8IwgBCio05EQ9eEHqXZ6+F3Su795zlM+H/bkm6+dprr2X8+PFcdtllANx44434fD5effVVampqCAaD3HTTTZx11lkdumxrayvf/OY3Wbx4MT6fj9/+9rccf/zxrF69mosuuohAIEAkEuGxxx5jzJgxfPGLX6SiooJwOMyPfvQjFi5c2KXbBhH4WCSCF4RBx8KFC7nyyivbBf6RRx7h+eef5zvf+Q4lJSVUVVUxb948zjzzzA5NfP2Xv/wFpRQrV67k448/5qSTTmL9+vXcfvvtXHHFFSxatIhAIEA4HOaZZ55hzJgxPP300wDU1XVPqRQReCdOURcPXhB6nhSRdrY49NBD2bt3Lzt37qSyspIhQ4ZQXl7OVVddxRtvvIHH42HHjh3s2bOH8vLyjM/71ltvcfnllwNwwAEHMHHiRNavX8+RRx7JzTffTEVFBZ/73OeYNm0aM2fO5Hvf+x4/+MEPOP3005k/f3633Nvg8+B/fwh8eFfibRLBC8Kg5JxzzuHRRx/l4YcfZuHChTzwwANUVlayZMkSli1bxqhRo2ht7Z5CuOeddx5PPvkk+fn5nHrqqbzyyivsv//+LF26lJkzZ/LDH/6Qn/70p91yrcEn8LVb4envJt6mJYIXhMHIwoULeeihh3j00Uc555xzqKurY+TIkfj9fl599VW2bt3a4XPOnz+fBx54AID169ezbds2pk+fzqZNm5gyZQrf+c53OOuss1ixYgU7d+6koKCA888/n6uvvrrbasuLReNEqkkKwqDk4IMPpqGhgbFjxzJ69GgWLVrEGWecwcyZM5kzZw4HHHBAh8/5rW99i29+85vMnDkTn8/HvffeS25uLo888gj3338/fr+f8vJyrr/+ej788EOuvvpqPB4Pfr+f2267rVvuK2v14DtD1uvBaw3/r8wsJ6r3vv4F+Nc5ZvniF2H84dlriyAIgNSD7wh9rR583yLdw0w8eEEQBhCDy6JJl9suHrwgCBmwcuVKvvzlL8esy83N5f333++lFiVGBD7ZdongBaHH0Fp3KMe8t5k5cybLli3r0Wt2xk4fXBYNaX6gbOfB//ZgePLy7j+vIPRj8vLyqK6u7pSADRa01lRXV5OXl9eh4ySCj9nunPAjC1k09RWw9D4480/df25B6KeMGzeOiooKen3Kzj5OXl4e48aN69AxIvAx2x0RhHjwgtAj+P1+Jk+e3NvNGJAMLosmncD3xSn7dq2Ap65K/EYRbIFAc8+3SRCEfoEIfLLtfSWC/+fnYPHd0JTg9fWWCfDzMT3fJkEQ+gVi0cRsz2IE39nzKfsZnKADKhzodHMEQRj4DLIIvgMDnbq7HnyorZMHWqljUp9eEIQOMsgEvgMefHcLariTAm9H8JFQ97VFEIRBgQh8su3dbdGEOmmn2IM/wsHua0tnqNkCfzsRmvf1bjsEQcgYEfhk27u7k7WrEXxv++1v/hZ2LIa1T/ZuOwRByBgR+GTbuz2C76IH39sRvP2A8eb0bjsEQcgYEXgn2SxV0FmBt8tzdLfA1+/s2GhdEXhB6HeIwCfb3t0RfJc7WbtR4Ot2wG8PhNc7MP+l/YDxDK7MWkHoz4jAx2zPQhZN9UZoqe18J2u7RdONHnzjbvP5yUuZH2MLvETwgtBvEIFPtr27Ivg/HQZ3LIBQJyfsbe9kzYIH35HqffYbhNff/e0QBCErDDKB76VywbVbYyPwjnjf3ZEmGWyFja90/niItr8f1ewWhMGOCHyy7dnMoumI3dIRD373qsSduS/cAPd/1hQu6yz2A0YmIxeEfsMgE/henLLPKeqZCHw4CMsezPyY+l1w+9HwzPfjt1VtMJ+b34h9cHUkGrev31eKsAmCkJbBlRLRq3nwDg8+nVhv/wDu+kzsunQWTVu9+dz6bvw2j9d8vnADhFpg6gnme0c8+PYIXgReEPoLEsE7yWYtmo5YNOufi1+XTuCVJeKJImx7G3TeprGvLxG8IPQbRODd2+087+6IVJ0RsjOCd/vkz98AL/80+XZI/1Dw2F59gnZ7HALf2SyYdotGPHhB6C+IRROzPWxFu+HuiVSdYuucsMMdjW97F3yOyXQTpVSmqyZpP0wS3aNzcJI3p2PWjI1YNILQ75AI3r3d4zV/3RLBO87RuDe6HG6DxffA2qfM90jITL9n09EIPhKJPgAS3aNy/DN7/Y6HRUc8eIngBaG/IRG8k0jEiKHydn8E31IbXQ4H4KkrzfKNdWY/7YjqEwq8K+p3RuHB5ugxCS0axz+zx9+5h1dEInhB6G8MMoFPlwdvWTSeSPcImdNWcUbgbgGPhFzbE1g0boF3tu8XY6FkrFlO9GBye/Dt+3QkTVI6WQWhvyEWjXu7UiaK7w4rwimGzmJjbgGPhMxo0/btGVg0bk++foe1Pk0E7+1kBC8WjSD0O7Iq8EqpMqXUo0qpj5VSa5VSR2bzemlxi1OwBcIOoYyETbSrvN0jZM5Rn84I3DkrktZGrENODz5RJ6vbokki0gk9eEcE73FG8J3w4MWiEYR+Q7Yj+D8Az2mtDwBmAWuzfL3UuMXv5nJ4eFHsduU1KYfd3cnqjMofuzi6/P/KzLXS5cnHWTRJsmoSCbxzXx3pXLkB+7xi0QhCvyFrHrxSqhRYAFwIoLUOAL0771wi8XMOKtLhbu5kdXrwKQYq2Vk0WhuLqKMefLr1MWUSgl2bwFtq0QhCvyGbEfxkoBK4Ryn1kVLq70qpwixeLz09nSYZSeLBx+0XAnRUiDPy4JNZNGkEPhLs2sNLInhB6DdkU+B9wGHAbVrrQ4Em4Fr3TkqpS5VSi5VSiysrK92bu5eeTpOM6WQNxHrhMde1Imo7cs9koFNHPHhn9B8Odvzh5cw+kk5WQeg3ZFPgK4AKrfX71vdHMYIfg9b6Tq31HK31nBEjRmSxOWSYReOxIvgOCFn1xsRFvpxCGgpATlHi4+397EyazmTRJLpmomM7E8FHXB3RgiD0C7Im8Frr3cB2pdR0a9WJwJpsXS8jUgl83Q5Y8ZCZnEN5OiaCfzoM7jklfn3EFcH7kkx31x7BW5k0XfHgE1o07gi+g1G484EjFo0g9BuyPdDpcuABpVQOsAm4KMvXS4PTanClCG59O7qcjVIF4SD48xPv1y7wlpAmmr810yyaRDgj+M1vwoqHrfZlmCYZ8wYgAi8I/YWsCrzWehkwJ5vX6BCpvOTc4uhyNkoVhNsgJ0kfsy3WwVQRvEv0O+KFhwMw7WSo3QaVnchUdV5bPHhB6DcM3pGs7kjUFtlFj5mRn90xybU7D96TpJPVbleo1bQr0fR87nUdiuCDZgSrt5PP8xiLRgReEPoLg1fg3RG6LZilYyGnILa6Y2eJEWEdW9UxEaHWxB2skLkHn/DYgCkT7E3SB5DJ8Z25riAIvcogKzaWIoK3SxZ4fMZKCTR1/Xruzsx0Ar/2KcgrS7ytqx68N8eUKegMMRaNCLwg9BcGr8C7rQZbMD1ek87YvL0brucSw2QWjc2HfzN/ibBFtrWu41k+7RZNJwXe+VYhEbwg9BvEorFpF3i/FcE3duL8rqwUtxi6BzpNPTHzc9vtu2UC/Hpa5ywaTyef5xLBC0K/ZPAKvNs+iXSDReO2Tdzf3RF8srTJRMTUi29JLfDuewsHu8+D78x0f4Ig9AqDV+CTRvA+Y9F0h8C7r6E8MGpG9HtnBT7RtZy40yzDgSQWjYaaremvHZJOVkHojwwygXdEn8k6Lb1WBB9s6viIz7hzJuhkveRVyB9ivndI4N1vBynSOIPNrmPbjLi7LZpdy+EPh8DGV9NcW0ayCkJ/ZJAJvNOiSSLwtkUDsZNwZEK6CN7jNeUK7Jo0vrzMzlswLD6Cb0xRmM359hEOmvv25SfvZN2zOvX1JU1SEPolg1fgndF2JBL97hT4jto06Tx4O01SWXOhJqsu6aZwZPwDyZ6iLxHBBLND+XKTe/DpsntC0skqCP0REXgwEaodmdoePHQ8kybdYCRb0EfPMp/JShe4KRoZf+76ncn3DzoeTHaKoz8/eRZNugdNOMM0yeeug+euT30uQRB6jIEj8IHm9BG3U+D/c2l0OdyW2KJxni/QBEvuTZ1FkolFA3D27XDRs1AyOnV7bYpGxQt8zZbk+wccHrwdzftyk1s09htFMmyLxuNPff/v/RXe+0vqcwmC0GNkJPBKqYlKqU9by/lKqeJ0x/Q4t4yHn49JvY9T4HevjC6HAkacldeIXSKBf+FH8L8rYOMryc8fZ9EkieBzi2DiUelHttoUjYyKrM/qmK3ekHx/ZyerHcH78pKPZE2X+mhbNP4CsWgEoR+RVmGUUpdgJuu4w1o1DvhvFtvUOVKlDW57D+47O/Fk1gC37gdrnohaGH5b4B0WTZPVqdlWn3kb4gTeFSln6sHnlgDanM8WWDuCn34aHP712P2dD6Z2Dz4vebGxdJ3JYYfNI52sgtBvyCSEvAw4GqgH0FpvAEZms1FdItHMSo98BTa9CvW7kh9XvSFqYdhC6BQzO9pOVU3RbaOkK1WQrnPTxm5XOBB/jZN+Bkd+K3ZdTATvEPhkHny6wmr2Nf15EsELQj8iE4Fv01q3h75KKR8xM2f0MRLNrNS4x3ym85ptwbUja2dEbm9LlRsfl3qZplRBKovmS/+KLtvZL6FW4n56rx9yXI5ZIJHA56YQ+ObE69vP0Wba6s2RCF4Q+hGZCPzrSqnrgXyl1GeAfwP/y26zuhtLFJNZNDa2AHpSRfCpSgS4tiUayRrzPUUEv///wfW74Npt0Qg+kECIvbnx2TjBBBZNqiyadBF8JGSOVR6pBy8I/YhMBP4HQCWwEvg68Azww2w2qssk6zRMN4lHu8AniODbo/oUAp9sdKzduem2ZJJF8N4c8HhMXfq80qjA20LszGf3+k107iQmi8YZwSd5oKSL4HXYEnivCLwg9CNSlhdUSnmB1VrrA4AkdWz7IDqSODpOK/C2EPui52nfZp0v1VtAXCerdbwvFwLB+DZ5kgi8e4Sr3S47MvcXRNvhzYm1nvwFHffgP/onjJ0Dc1xT5oaDZq7aSNi03eMRi0YQ+hEpI3itdRhYp5Sa0EPt6R6cwtzmyIRJa9HYHrz1s8RE8JaIJpov1cbOtGlvhyWGdsTtFvRkFo070raPtyNzeyCWc5tNnMA70yRTPM8/vCt+3Us3wn1nQcWHpu3dNVetIAg9QiYFwocAq5VSHwDt5q7W+systaqrRMJRW2PnR9H1dgTvy0ss1Ck9eEt0U/nVj14EkxdA4fDY420LJc6DT/Z8dXUGe10RfE5B/DabnAJXJ6s90CmNwBeXx6+zf7u2BnOsxysRvCD0IzIR+B9lvRXdjTOC37EkumxH8Ed8Hd7+Q/xxXpdXniiCT9ch2VJjBH7vx9EiXnaUHWfRdDBNsj2Cd3SqujODvLmxD6/2CD5FFg0kFng759+bYw0Ck05WQehPpO1k1Vq/DnwMFFt/a611fRenCO1dG112jwZ1ExfBOwTejv7TDQqyo/K/HgHLrVRHO4KP62R1fD/v3yb6hwSibT0g7IeL06Jx48t1TQ7izKJJ8UCxSxcHW6J9B7a9FQ46OlmtCH7jq7D47uTnEwSh18lkJOsXgQ+Ac4AvAu8rpb6Q7YZ1CadP7Ixm2wfsJBN4SwATdbLakXC6CN45f6mNNwOLZsR0OGShvcHVLqs9zk5WN3aNeW9OtA3hELz+q+j6RBH8lOOsdrcaYb+5HJ69xqyzR/KGWsxv4/FGxf/+s+Gpq6LncVo3Ha2jLwhCVsgkTfIGYK7W+ita6wuAw+nrtk2qqpGQQuAtAUzUyWoP1w+6vHt3SmY4gcD7klk0jp/f44s+jMbNjd0vLoJPIPBXrYZrt8dG8Fvfip5TqcQCf/rvYcgkc1/2A+Sj+82nHcEHW81vkmqy75jyy2mylQRB6BEyEXiP1nqv43t1hsf1Hs4I0mlXtNSYz2QTbbjTJJ1RqV1wy50z7vakQwG4sdSxQjnOmyKC9+bAtJNg5jlw9l9j97Pb29ZgPhOVGc4phLyS2AjeXVwskcAXlxvLKtQSrWFjW0q2HRVqTd/J6vyd06WjCoLQI2TSyfqcUup54EHr+0Lg2ew1qRvQSQR+51LzmS6Cb+9kDZoIXSlHBO+yaNy57+4I3uN1vBmk8OC9figaAZ//e3y78qwHRlOV1f4UdeR9udBam7wtTr7yP/Nb+PPMQ8GO2L2ugVOhVnNsqk5W5+8gEbwg9AnSCrzW+mql1OeAY6xVd2qt/5PdZnURt0Xjzg1PGsG7PPiXbjSievLN0Qje3clavTH2u9vCsSNfiPfgPS6BT0Z+mfm08+wTWTQ2vlxHW63PS16JtsXmlFuinbq+PPPgsj139+9jR/Cp8uBjrLAUlT0FQegxMulknQw8o7X+rtb6u5iIflLWW9YR3D64U4TCgfiIPZnA2yLrjKzf/bN1Hisa3vQavHFrdPttR8aewz0LlDM9MVWpgmTT6QHklZlPO4JPNROUNzfaVvvTjsidAj/vm442WhF8u0WToC3Km9iisX/7SIK+DkEQepVMvPR/A8738rC1ru/gtg3cFo076ySRgEESIbZHsDpE65WfxX534p5VyjnAKJVFkypH3Zdj7sGO4NNZNO4I3pdA4J343R58ggegx2OOj4RjH6h25C6drILQ58hE4H3OcsHWcopwsxeImyrPbdG4Ini3x2zTLsSONEV7OdwGo2ZE17fWJT5HIoFPNpLV+SBJV8o4rywzi8abkyCCt0slJMmD9+UaaynQEP3ujtQ9vugIYOc92tF6TMaRWDSC0BfIROArlVLtZQmUUmcBVdlrUidINZNSQosmWQSfYiBQqA1GHgTHW4U07Y5MN3EWTV70DSLOokkj6k7yy6Bln1lON9CpPYJ3jGKF5LVvfPmxFo03Nz4TRnnN7xhsiWbzQFTgnbaMRPCC0CfIROC/AVyvlNqmlNqOKR/89TTH9CxpI3i3RZMmTTIGO4IPmAfDmNnmu51yaXOCJfxxEXxu9AHTkXrwbmwfHhIPdLKJieAdFSchhUWT57JocuJ9dI8v2lntFvgXfgi3HRVd99d5aW9HEITsk0kWzUZgnlKqyPremOaQnidZxx90rJM1UQTfXkWyzUS2ttC21MbuZ4tuIovG7hTNtB58IuxMGsggi6bN/AbuCD6ZwPvyLIvGkSbpfmh6vOZBkCiCf+dPmd+HIAg9RiZZNFcopUowlSR/r5RaqpQ6KftN6wBxEbzbonEJYrIIeOSBCVY6I/jcqNC6LRr7IdKRCD7TYmMQrVAJqS0aby5mgu6QI4LPQOBDrdE8eOVJYNF4rDr0bdEpEEEGNQlCHyaTEPKrWut64CRgGPBl4JastqqjpLNo3BF7ojTDq1bDMVfFrwfLn26E/KGOlMVKuPP46D7tEbzrBefYH0S3JfK1M2WCwwKx684kwu5fCLU55lJNkqbZfkye8c3t6pGRYBKLxnpQVa2Lrk9UewekHo0g9AEyEXi7J/BU4D6t9WrHur5BSoFPYNEkEvhk0a1S0GhVaigaEY3gd6+Mjoy1j/f4YgX+gidg8vzo9d1i2BGLZuoJ5nPYflA0Mvl+drQeDpho25kxlMqDB2iuto4NxneUerzRB1Xl+uj6ZDnv7n8TQRB6nExKFSxRSr0ATAauU0oVE5sX3/sky6LROrFFkyiSTZqHrqDJEvjCkWYwVE4RtNbH7hYOGDF1WjT2g6a9tot7lGsHIvjiUXD+YzD60NSDomIi+EDsvsnuMbfYfNoDqSLh+FRHZwS/+vHo+mQWTSREX8umFYTBRiYCfzEwG9iktW5WSg0DLkp9SA/jFiNbWCNhQCevPeMkWTStFDRa+edFo8xnbnE0Z9wm1GrE1RnBD9vPfDqj6kyumYz9Pm0+k00qHnOtNvPny0DgbdupYbf5TGTRKI/jTcTxoHp4UeJzSgQvCL1OJhN+RLTWS7XWtdb3aq31ikwvoJTyKqU+Uko91YV2piaZRdNeHjhF1olN0mjaEcEXjTCf3pzYafHAipYdEfz5j0OZNZVtsgi+owLfflwKh6z9WoFom2yS3aNtO9mdp+ksGoCZX4w9xo0IvCD0Oj1R9vcKYG3avbpCWoF3RPA3JhmBmiy6DbXAk5eb5ULL+/blxnemtkfwCSblaBfdNNUdO8KR3zbVIN3YlkyHInirWqV9T5FgaosGYN43UrdP5m4VhF4nqwKvlBoHnAYkqIHbjSQT+A//Zj4zsmgyEFu7M9KbG00ptLHz5NtHgzoGTdlZPF3pZHVz8s3RapBO7IdJsBXqd2bWyeocRAVGnN0RvHJF8Mn6AWaeY50jQQS/+Q3YlfHLnyAIXSQjhVFKHaOUushaHmFVmMyE3wPXkKJTVil1qVJqsVJqcWVlZYandRE30Mm63Cs3mc9UnZI2qYp9gcmIsXF77QATj4yN7GMEPkkE35E0yUyx7/XVm2Hbu7H5+snu0Z12GU6TJomKfXDM/Vp0efKx5jORwP/jDLhjfro7EAShm8hkoNNPMOUJrrNW+YF/ZnDc6cBerfWSVPtpre/UWs/RWs8ZMWJEBk1OQKpaNJChwKcR2/JDHOdzZct8/i4TTTtF3ZMogu9CFk2m2A+TTa+az4Zdjusly6Ipif1evQHuOyt2nccTjeBzi2Pv9cAz4q8R92/StxKvBGEwkEkE/1ngTMxIVrTWO4HiDI47GjhTKbUFeAg4QSmV9sHQKRJZNM5Mk1STadik6rj05sZGub6cWAvDTjN0irrzmraAFgxzXTMLDpkdWScqKZzsgeL1QU6af1I7zx9MmqjzoWl3Jk85LvF0hxCthCkIQo+RicIEtNYa0ABKqRTFyKNora/TWo/TWk8CvgS8orU+v9MtTUUigXdGy5kIfCpKRsc+ANzlhu3zOyNk5/LIA+Ds2+Czt8celw2BtztVcxOUM0hlQzlr3cRg3bfyQuEI83nSz2IFvmQcXLUGzn3YMd2h69+kviKT1guC0I1kkgf/iFLqDqBMKXUJ8FXgb9ltVgeJ8+DDsZ2g7nlUwYhVplFlUXnsd3e5YTty9zp+TvdDZfZ58efNhkVjP3zcA7EgtcDnlUHd9vj1OYWmX8HjNUXOfmKVLHbWw/flQOnY2Gu4O2nrdsR+f/lnxtNf8P3kbRIEoUtkUk3yVqXUZ4B6YDrwY631ix25iNb6NeC1zjQwIxJF8M5O0OZ98cd8/U2oWg/3nRm/zY27EzIugk9Qjjdh6WEX2YzgQy3m/FeudLQplcCXJl7vz7cE3nWs/Ru4/ftkHnz9Tut8lo//pjXtoQi8IGSNtAJvWTKvaK1fVEpNB6Yrpfxa675TRjBO4HWswI/YP/6YktHmLxPiyg0ns2iSePDJyEoWjaNto2dB6bjo91RvDMksmmSVMP15cOqtMO0zseuTefB2xJ+sw7t6IwyZlJ23GkEYpGQSQr4B5CqlxgLPYapJ3pvNRnWYRFk0tkVz1l+jQ/w7i3skrFukEnnwGQl8NiJ4h8C7ywqn6kh258Lb2BF6ouj/8EuMKDvxJong7UqV4WBsRo3WpgbOn+fC2idNdtKNpbD0/uRtFQQhIzKqJqm1bgY+B9ymtT4HODi7zeogqSya4Qmi946SNoJPMOdpJhZNVjx4x8MnVd14N8kieNueyrStySwaW+CDzbG5+YFGY6HpMNTvMn8Ab/4ms+sJgpCUjAReKXUksAh42lrXt96j3RUNdSQ661CibJKOkqlF4+2oRZPtCD6jhCdDMg/eXp+pnZRM4Ns7fTXUOzpcG/ea/gIwYm8fJ1aNIHSZTLJorsQMcvqP1nq1UmoK8GpWW9VR3BkbOhyN4DsSxSYjzqJxCbzHbdGozARKKZh1LhyysMtNjLbFZx4cOtJBgS9LvT7dSF/n9SEq1G2N5q2izZHVU+dImWyqilpHbfXRAWTZ6J8QhEFGJlk0rwOvO75vAr6TzUZ1mIQRvCXwWYng3R68bdEkiOTT4c6N7yrKKiMQakn8cFv4QOKpCZNZNHm2B5/h20Z7HrzVyfqLsTDmsNgHXq0jHbOpMjpQrK0xWoZZInhB6DKZZNHMAa4HJjn311ofkuyYHieRBx+0yvnaIzrP+EPqqe5S4Y6Ekw50skQpE/89m3hzjMAnergdeHriY5JF8Pa9Z1odMpFFs3MpDJ8e/e7Mtw+1Rn+/QKNE8ILQjWTy3v0AcDWwkr42k5ONO4Kv+gR2LTfLtnh86sLOnz+dB2/T7sVnaGdkC1tcO2LRDJkEKHOv9sMRoveeaLBYIpJ2sjaYB2xLTazAhwPRc1dvhIesAWGZvjEIgpCUTJSoUmv9ZNZb0hXcHvxrPzef3tzUqYGZ4p6025mpctgFjkwTX/z23sDutEwWlSdixP5wzSZ4/npY/mB0vd3/0GWBr4fS8ZbAOzz4cCAarTvnuJUIXhC6TCYC/xOl1N+Bl4H2erda68eTH9LDuCensEkWaWfC6FnxbwGJznvmn6LL7QLfhet2B3a55GSZMckoGAqn/QbKJsLrt5h17ZUwOyrw4VhbJ9Bo5pWtXGs8eF++OWeyOV3FgxeELpPJe/BFmDlZTwHOsP6SGLm9hDuCt+lKkbGvvgATjrTOk6RT1Y0tbl15sHQnHRV4MLbO3IvN8pl/dlg0rcmPceIsNuYuj1xsjRxu3A3FVn2fcDDzcwuC0CEyieDnaq2np9+tF0kWBWYSSU89MTrZtBN/XtSecHeapvPg+7PAAxSNhB/XGB989X/NOqcvnwr7Ibf++fgJTsomRpeLR0PNZvNwTjSJuIi+IHSZTAT+HaXUQVrrNVlvTSd5atm2xK8U7nTGRHw5hdNkvxm43wSSTeJti1umOePZprMCD9FOTvte3dF40uOse1/zX/PnZIhT4O0IPpA4QyfTB4ogCEnJRInmAcuUUpsxHrwCdF9Kk6xpaG4vWx5DV73wcBKBLxmbeP8+J/BlXT+HXahtv8+k3s8m1b2Xjo8uOy0ad6QPmXfqCoKQlEyU6JSst6KL5HrC1nQkLjKJ4FNx4BlmXlN3QS1nhUYn7QLfRzoI80rS75OOIZPgB1syf1ikEviSMSY7RofN7FYevyXwrreD/CESwQtCN5C2k1VrvTXRX080LlNyVLL0/C6mSM77FlxXYYTJSbLRsXakn40aM52hqzNZ2eQPyTzdNOWsUUOiv13hcNNZHQ7EC/xhFxiB//gZmctVELpAH1GirpHrSTbKMlFY3wGUig6jzwRb3AZzDne6WaPa56cdbgaERULxHar+AiP8D50LH0nZYEHoLH3ELO4aOSqJwHdR31PizYWwyzvuKxbNF+6GynW9c+1UAu/xROvjFAxzRPAuv905Alcm6xaETjMgBN6f1KLJIt9fHz9asz2C7+UXoxmf771rJ3u4+ax8eqdF4/FbAu96UBYMiy53pNyCIAgxDAiBTxrBZzOET1R9sa958L1BIq/+klegxOqYjong/WYUcrDZFIULNkW32SRLSRUEIS0DQomSWzTZ9GgS0Fcsmr5G4QhTpgBMBK88xo+3LZpAc3Q7GH/eZjA/LAWhiwyI/3v89EIEnwjpZE2Ms1hbXpkRfI/HRPCRoIngi8qj+xQ6Inh3P4cgCBkzICwaf1KLpoeRCD4xToGf/z2Y9SWz7LXy4ANNsXPnOiP4UKBn2igIA5ABEcH7kkXwPW3R2Ax2W+HIb8Nn74x+dwr80Mkw6RizbHeyBptjJ2NxdqxKBC8InWZARPA+woTwJhf6nkJb1x/sAn/yzeZzwjwzuUeyEcXeHCuCb44VdWdHrUTwgtBpBoQS+QixldEJtvRwBG8XzRKLxjBkYjRaT4TXZ0XwTSZb5nN/g0us+dzP+qv5zLTImSAIcQwQgQ+zlXK4ZnPshp62aOzrSSdrZnhzoLXeLOcUwCFfhLGHme+HLjKpk2v/B01VvddGQejHDAyB1yHaIl4zI1EMPS3wEsF3CG8OtNaa5Zwk9X2qN8A9p8au2/kRbP8gq00ThIHAgPDgvYQJaC9a666WF+satkUjEXxmeHzQsMssJxrQZNszVY6yC/W74M7jzPKNdVltniD0dwaGwOsQIbwEwhHaK8DP/CIc/Z2ebYhddXLkAT173f6Ks25NTgKB1wk6zVv2RZcjYXlbEoQUDAiLxkuIoPbSFnLUpPn836B8Zs82ZOrxcNGzcOTlPXvd/oqzkJg/Tc2ZZkvYnZUn2xrgka/ArhXd3zZBGAAMDIG3I/hQH6gdPvGo6HR3QmrqKqLLiSL4mH23m09n5ck9q8y0gI9d3O1NE4SBwIBQIk8kRBBfbAQv9H1s0Yb0GU/1O82nM4IPWLM+SSqlICRkYAi8FcG3BvtIyQIhM8omRJeHTk6972u3wL8vip3Kr9lKn5TBUIKQkAHRyVo18TRWflzOrMYAU0/8MYw5tLebJGTChU/Dvs0w6ej0++5aZv5WPx5dZ3v4Us5AEBIyIAS+8ZTf8+SaNzixrsUUsxL6ByVj4ue7TbjfWKjfEb++ca/5lAheEBIyICya8lIzW9DuOvFiBySzzoWRB8Wvt0e4SgQvCAkZEAJflOujONfHLhH4gcmJP4L/+2X8etuicU+dKAgCMEAsGoDy0jx21bWk31HoP1z6GtRamTZFo+K3O/PoQ23gy43fRxAGMVmL4JVS45VSryql1iilViulrsjWtQDGDy1gU2VTNi8h9DRjDoWDzjTLhSPitzsFvq2xZ9okCP2IbFo0IeB7WuuDgHnAZUqpBEZq9zBrXBmfVDZS3xrM1iWE3sQ5IYhNjMDX91xbBKGfkDWB11rv0lovtZYbgLXA2Gxd77CJZWgNS7fWZOsSQm+iFHx7cew6p/cekAheENz0SCerUmoScCjwfoJtlyqlFiulFldWVsYdmymfmjiEIQV+/vzKJzy9YheRSC9N1ydkj+HTkm9ra+i5dghCPyHrAq+UKgIeA67UWse9R2ut79Raz9FazxkxIoHPmiEFOT6uOHEai7fWcNm/lvLCmt1daLXQbygZZz7FgxeEOLIq8EopP0bcH9BaP55u/65y3hETOWxCGQD/+SjBwBhhAGFV/h91sPkUD14Q4shmFo0C7gLWaq1/m63rOMnxeXj8W0fzxTnj+GDzPnRPT9kn9CDWv+0oq99ePHhBiCObEfzRwJeBE5RSy6y/U9Md1B3MGFtKTXOQ3fUy8GnA8dk74Ozbo99H2hG8ePCC4CZrA5201m9B78ygd9DoEgD+8c5W5k4awvHTR+Lx9OpkfkJ3MetL5lN54OnvwYR55rt48IIQx4AoVeDm4DGlTBpWwO2vb+Tifyzm2VXS4TrgmLUQrtsOZePNhN0SwQtCHANS4PNzvDx/1QJuOPVAAB78YJv48QMRZb2V5RRJJ6sgJGBACjxArs/LJQumsOiICbz1SRX/eGdLbzdJyBY5hRCQMhWC4GbACrzNT844mFEluTy7ajfLttdKJD8QySmMnelJEARgEAh8js/DaTPH8P7mfZz9l7eZfN0zPL1iV283S+hOJIIXhIQMeIEHOO+ICSycM57hRTkAXPavpTS2SQ3xAYO/oPcFPtgCEZkTWOhbDAqB329kEb/8wiE8dfl8rj55OgBPr9jZy60Suo2cgt61aCJhuLkcnrqy99ogCAkYFAJvU16ax7eOm8r+o4q46em1XPPocpZsrSEUjrCzViYL6bfkFEGgAwL/8dPw4d+77/rbPzCfS++DSKT7zisIXWRQCTyAUoo7vjyHT00cwiOLK/j8be+w3w3PctQtr9AgteT7J/4CqNsGHz9jvremSZl86DwzSKqrBJphzROw8eXousq1HTuH1rBvc9fbIggJGHQCDzB5eCH3XnQ4//v2MUwYWtC+/k+vfCJZNv2RHOvf8KFzTTT9y0mwLa4ydfdQsQQa9pjl56+DRy6AlY9Gt1d/0rHzrXgE/jgbtrzVbU0UBJtBKfA2M8eV8sY1x/Pa948D4M43NnHhPR9S2dBGa1A6zPoNOUXR5Td/AzoMO5ZkduwnL8GrP4dVSYqd1m6HyvVmOdgCfz8BfrO/id4r15n1NZth7Byz/M6f0r9BONluPYh2r8r8GEHIkEEt8DaThhfyyNePpCTPx+vrKznl929wwI+e4/IHP+IPL22grkWsmz6NP/oWxs5l5rN6Q+J9nR55JAz//Dy8/kt49CKoq4jf//cz4C9zzXKLY7awfZti97PLFld8CG91oHiqsv4XjGSQ1fXSjbDhpczPLQx6ROAtDp88lA9u+DQXHT2J6qYAAP9bvpPfvbSeKx76qJdbJ6QkxyHwjVbdoaoNRpDvOBaevDy63SnS7hLDLWmme2ytiy7XbY+tf1M2IbocThEQOLeFQ9Baa117X+prA7z1O3jg8+n3EwQLEXgHeX4vPzrtIH521sF8cc44Fh0xgcuOn8pr6yq56ak1bKyUioV9kkT9JtUbjV++a5nJbtlpPaQb90T3CTRFZ4QCI/AfPwM1W+PPF4nECnztdqjZEv0+6mC44Amz7BT+1jpj7QBULIafDYfNb5jvz18HK/9tluuTDL5r3meEPSzjNoSOk7Vywf0Vj0fx5SMntX/fWdvCvW9v4e9vbebJ5Tv5ylGTKCvw86W5E/BKCeK+QaJBTg07Y6PdO4+Dhf+Exr3RdW2N0FQJU0+Aja9A7TYT7c+5GE67NfZ8rbWxAr97hXkDmHoizPwC7H+KKX42ambsNW6ZAKNnw9dfN1YQGN998gL48C5Hey2Br1hsrKKDzzbfn/4urP4PjDiwY7+JICACn5YxZfk8f9UCHl1SwT1vb+HXz5uOtRv+s4qvHj2Zs2aPoTDXy34ji3u5pYMYr9985pXGirBNwXBoroI3bo2N4Ou2Q7gNRh5kBH7DC6AjiTNhmiqhpdYsK080Cj/0fJjxueh+RSPMed/+Axyy0Kzbtcx81m4zn6GA1a6h5rwA+zaaz7+faD4Ptu7DjuybHA8NQcgQEfgMGDekgCs/vT/fOHYqbcEIL6zZzavr9nL325u5+22Tw/yNY6dyxYnTyM/x9nJrByFzvwahVigeA//9Ruy2yQvgvEfgxZ+YwU06DJOPhc2vR4V8xAHmc91z5tMWW6f101QVfXgcdDastrJuSsbGXq9olHlYvLgKtr4Tu83Ormncbbx42/OfcBRseyc2VTLQZGrsRCzP3tmPIAgZIh58B8jzeykt8HPOnPH8ddGnmD7KRO3jhuRz++sbOfDHz3H4zS9x7p3vsX1fMztqWyTdsifw5cL878HoQ2LXD58O5z8O/nwYPs2IO0QFfdt75rN8Bnj8JpoHY5GE2sxDw6apMirwRzgeIiVjYq9ZOCK6bKdqFgwzn3ZHauNek+0TCcHn74LTrayb+86KHlu73Xza0X5P0dbQ89cUsoZE8F3gbxfM4cW1e/jq0ZP48ROruf+9rUwaXsj7m6uZ/6tXASP+D14yj0A4wuRhhTJ1YDYZtl90+asvGFG37RvnthGmHhFrn4T8oVB+iImWW2uhcKSxQ24aad4MbBr3mu3+Qhh/eHR9cXnyNtj2SyhgOlrtB0bDbljzX/NQ2e/Txlry5poHzKgZsGeVsXlGHgDN1V34QdIQicCHfzNvE0d926z7xTjTr/DlJOMChH6FCHwXmDCsgIuPmQzAj04/iLMPHcthE8pYvbOeG/67ik17G6moaWkX+6P3G8bcSUNpDoQJRzTH7j+CScMKuf+9LVxzygH4vfJC1SV8uebTmwMTjojdZkftzuVICKYeDx5vNF1x7sXw2i/MsrNeTeVaCAeMGCsFB5wO656NPkBsxs2Jb1egIbbjddcy2LsGDjwd8svMuuOvM3nuZ/zRDKZ64Atw7fZo52s22LMKnr3GLE88CkbPMsvO0gu124yVZG8T+hUi8N1Ejs/DpyYOAWDG2FL++62j0BqeXrmLvQ1t7G1o5Y7XN/H2J9GI7K63ojVIjt1/JMdMG97j7R5wfOcj8OXHry8ZDb48E0WXz4iun3qC+Zx3mZn27+growLvZPcqI8aF1r/RF+8zHbJunA8SJzXWv/WYw2DnUvOw+L9fR7cfdYXJ3skpgtLxJoJf818gi6UznHn/yx+EsonR7837YNdyuP9s8/3GBJ3XHWH7B+ahdfnS6G8oZB0R+CyhlEIpOGNW1KM9ZGwZ5aV5tAbDbNjTwC+fW0eL5dG/vbGK4jwfP3piFb/43ExGFOVSku8nzy+dth1i6JTk265YbvLh80ph+mmw7umowJ/y8/j9i0aZrJvh+5sRqr482M/KcvF4gQT/Nh4vLHoUXvyxidJt7No4B5xmBB5Mxk37cR7IKzHL33zb1NNZ/pD5fsETsf68TVsj5BbFr8+E926LWka5JaZPoLkquv1/VxgLyyYSMW20efln8M4f4Yd7o3PjutHaePp5JSarqLXOdCTbKaBC1hFPoAc57ZDRfGriEI7ebzgXHj2ZD244kbmTTNR/22sbOesvb7Oioo7T/vgWh//8ZU763Rt89q9v887GKrTWRCKx0VwgFCEcyWKEN9AoLofp/2eWv3A3XPpafCcpwLkPwcIH4DSr8/OEH4HHB6EWKB0Xv7+baZ+Bg63USV+e+XzNeoDYD5RpJyc/Pq8Uxn4Ktr5tvpeOj92utcnU+cXYjhVVi0SM/9+8D5671lhCYDKNdq+MtZHslE6b5iozcOyjf0JTNbx5q3kLsd9MmqrgiW/H1uF5989wy3hzXq+ZbIdQW+bt7S7aGuGZq03bImFTL8geO7F3LSy+p+fb1ENIBN+LFOf5+fc3jqKuOcjDi7fx34920hYKs7HS/Me3bV8z2/Y1c97fzP/EZQV+Tp05mpfW7OGYacNZtaOOfU0BHrxkHn6vB49STBhWkOqSgo0/D8Ycmnib/RAA+HGNiVz3+zSsfzYzgYdoRk9OYWw2TnE5XLXadO6m4tM3wr2nmWX3NUNtsOoxs7zh+fj+hmQ8dYUZ1Xvh07HrpxwHHz8VG7G7hbh+p+kbcLNjqXlrevVm+Oh+0wF9yJfAlwOL7zb71G6PPuj2rIQ1+XDQmZm1ubPsXAZ3fQYu+8D8Vh/caX77IZPhhR+a8QWn/BzuWGAeVIddYL2V9RChgLH4/HlZvYwIfB+gtMDPpQumcumCqQBorWkNRghGItz22kZue83kZY8bks/DH24nHNE8vnRH+/Gf+d0b7ctnzhrDIeNKOXPWGIYX5dLQGmJnXQt/fW0jPzvrYMoKcnr25vo7ti1RMtpakWEW1MiDzOekY2DmOfDw+eZ78ejkloaTScfAnK+aCN2Xa942Hjof6itM9LzxNbOfXVzNyY4lxvOe983oukjEiDuYCppOZp8Hr//KiKCNe7BXss7exy6Gl38KtVZ5hycvN/bU5UujJRv+fgJMmm+W3/mT+fz+Btj4KpTPNBOwHPVtk87aEZ65xvQdXLHcDBpzsvhuI9zrnol2oKOiM3/ZA8fCVkpo875YywxMxP/M9+Gkm+O32WgNr/zMWH/n/Ru8GUhq/S74wyEme+uSl9Pv3wVE4PsgSinyc7zk4+X7J01n4ZzxTBpeCEBVYxu5Pg8PfrCNnz/zMT//7Eyu/89KAIYX5fDk8p08uXwnNz29Fq9HxVg4OV4PZQV+5k8bzrH7j2i/lpABC64xFSRnnpPZ/mXjYdFjMO5TsZ2ZHfm9T/ttdJ7XMYfCsdfA/74Dbzg6Z7e8ZbJ5hk6JDvb6mxVpz70kKji2AIOpbWOTU2TeMg48AxY7SidEgjDvW/DeX833B78U27ZDrO8rHoo9N5j7Xfu/2Jo8W96M3WfpP+CVmxztKIAjL0v+WyTigzvM568mw4wvwMk3R9NW7aJun7xsHqpgbDb793R3kDdXGRFf96zpbB51ECy5F1Y8bGy8T98IVZ+Yt4FJR5sHMJh7f/M3Zrl6A4zMoKTElrfMg2XHYtOeLL45iMD3cbwe1S7uAMOLTCrgJfOncPahYxlZnMes8aWMLs1naGEOdc1BNuxt4OpHVzBjbClTRxTy3KrdVDUGeGypKYd711ubGV6Ui1Jw9NRhjCrJY9ERE3l+9W7OPnQs2/Y1Mb28hKJc+c+jnZLR0WJimTLt0+Yzt8QUNZv/3Y4dr1RsROied/bUW02E6RZfm90roH6HEaOazYn3sStqTjgyVuDBPFBOugl+MR6Crno/n7vDlG7YvRL2robSCWZWLYD8ISYTyWlNuXGKOxifPhQwYrn8QfP2csyVyY93s+pRc70vPWC+V1k1/J0pn6210bLMqx6D8fOi25oqYeMu81sOnw7f/iA6MM2XZx4Y7/4ZltwDKPNGNWa28fBt/jrPdLDv22z6UWYtjG/nuueifStgrK+y8fH7dROqL81gNGfOHL148eLebsaApCUQ5tYX1uFRsHJHHeUleSzeWkNFTeK5aHN9HkaX5nHhUZMYP7SAknw/d725mYvnT2bm2FLJ7ukN1j0HDy6EY75rJhtZeD+8fzs8f33i/XOKjIAXlcOnLoTXb4lum3UeLP+XWb6xzgjsH2Yb0dr6dlTkwFTm/NNh0WNPvRUOv8Qsb3nL9BWMnWMeBo27zYxXz/0g+X2UTYyP+qccD7O+BP/5enTdtduMULrR2gwA++Nh0OZI3ywcCZe+ataHO9iZ+4V7zMhm+61g5jnRSp/lh5i3t9JxpsO2YaepM3T2bab2/8s/jZ7HHrAGcP2uaCnr6o3mzardLrL4ylMweX7H2upCKbVEa51gAIYI/KBGa01zIMwPHltBQY6X/UcVs3ZXA0dMHspvX1zP7vrkEdg1p0znkLFlvPlJJRU1LXzruKlMG1mMRlPbHGRkca7YP92N1sb+cPvN1RvhnlON5/7ST2K3DdvPlF4IB8zI2Ws2RQdn3TTSfNo57lqbCPqVn5kI+nTLyolE4Kcm24sLn47aE2A6Y//zDVhwtbE1AIKt5hw5hSZb5d0/m/WzF5mRuvt9OjqJypyvRjtj3Rx+qXmY2f0fTdVGgJc7bKEF18Abv4oeM/1U47tPOc5kQ911knmTSceptxp7ZmMaT/zgzxp7Z80TcNZfTH9GxWIzbiERx11vBrSteixq5Tg580/mfBWL4fTfZ+bhuxCBFzpMZUMbW6qb2NcUoLE1xDWPreCIyUNZs6ue2ubEE1p4FNiW/8I545k6spDFW2o48cCR5Pq8vLhmDzXNAX7xuZlMGFogD4BsEGw1E4ov+6f5ftpvjT//6FfNpCRffz26742lpjP4W+9G11V9YiYlP//R2ElMnrjMPCBO/13H+hEAnr/BiPyJP4naVDdakfmNdSYqXvYvePbq6DH+QmML+fJg/vdhwjxTP3/3ythzn/4788DY9h48br1VHP51ONUS/TuPi84F4Kb8EDPyePHdcOy1xhoaeZDJlgJT5tmuBGpz9JVw/PVw21FWAbpaU1HUl29+rynHwVNXGX/dSfHoaEf10Kmmz8MeXObxmjetb3RuXl4ReKHLNLWFyPN7UZia+Vurm1heUUdpvp+iXC9LttawuaqJyoYA9S1BPthi/MthhTntM2QpFS3QOKwwh5NnlLNhTwNFuT4mDC1gwrBCjpg8tP16cycNldo9ncUW0ItfgvFzTeQfCkDxqOg+gSbT8WiXeMgWrfUmK2f07GhW0t8/Y94svmd52JEw/NR6Mzn3YTM4qmqDybpxTr948s9N5/HaJ00Gz1dfMGmiLTVmcBhEU1sB/jLPlJlwc8GTMOVYs3zrdCgdax4EC642ncNVG8xD7o4FZkRvyTiTwXT678xbx9qn4OFF5vhFj0X7W8C88Sz/l5m31/nQyh8CFz1nagyBGVPwhNWxvOBqOOGHnfl1ReCFniUYjvD0il0ML8rl6P2GsWRrDc2BMAePKWHptlq++c8lhKxQf8rwQnJ8Hj7e3ZDwXOOH5hMOa/JyvPg9HsJaM6wwh6P3G05hro88v4dXP97Ltn3NHD55KJOGFbJg/xHsP2qQ1+fft9lMLHLIwo5H3D2Bnc3izCBZ/7zx3Cc4Oj93rzTpkDuWGG/7ylXRTsnmfbF21fM3GC/fKbY7lpqMnbKJJpPn1FvNW0Gpo8zz+3eYmjwFw42tM35udNv/G2qqkF74jCkd4bSMAk3moTDx6MS/sdZmYNnHT5nO8FnnwWdvc/wGEbjvTDOD2AX/hWFTO/ILtiMCL/Qp6lqC5Po87KxtYfLwQpRS1DUHqahtZkVFHaFwhHV7GmhqCxMIR2hoDbFudz2VDW3MmTSUvfWtbKluTnr+fL+X+dOGs7GykarGAOUleQTDEYrzfBw5dTgeBQ2tIbwexXlHTGBHbQszx5ayeMs+Fuw/gvqWEKNKpA+hT1G7zZSLmJGFOWm1Nlk3Q6fGe+CrHjOTu5zxh65dY/sHMGQSFI3s2nkSIAIv9Hua2kLUtQQZU2YGw7QEwlQ3tVHZ0Mb2mhamDC9k4rAC3tu0jwc/2MaHm/fRGAhx8kHl7GsKsKehlTyfl3V7Er8puDlwdAkzx5awZGsNC+eO5wufGs/W6iamlxezu66V7TUtzB5fRmm+P/3JBCGLiMALg45IRFPTHGBYUay/HApH2LC3kUcWb2f2+DJeWruX6aOK+GRvI22hCK+u28uwwlzaQhHqWgIEw6n//5gyopCSPD/1rUG+evRkNlY2sqmyCb/Xw8RhBZTl+3lhzR6+OHc8pfl+RhXnooGpI4qoqGnmwNEl+DyKhlaTnz2kUEYaCx1DBF4QMqShNUhhjg+PR6G1JhTRvPLxXt5YX8nW6maK83ws2H8EoYimurGNpdtqWbq1hsY2I9AeBeOHmtznHTUt7X0NySi0pnhsCoRNmfnyEoLhCHMmDmFsWT7VTQFeXLOHo/cbxvCiXHweBUqR6/OQ6/Owt6GN/UcV88rHezjxgFGcPKMcj4LVO+sZNySfJ5bt5OzZY9lR20JLIMz+o4piqpTWNgcozfeLHdWPEYEXhCwTCkeoqGmhNN/fHoUHwxGWb69lwtACVu+sx2tlH72+vorCXC8FOT7aQmEaW0McOLqElmCYtz+pYmhhDh9u2UdrMHY4vTMNNRllBX4CoQjNgeRTRRbkeJk2qpghBX5eW1fJuCHG9hpdmkdhro9P9jYypCCHlmAYrTWHTRiCz+uhJN9HYY6PpkCIaSPN8cu21zKmLJ/mQJhZ40ppDUbYU9/K8OJcDigvJhCK4PUodte3snFvI/uaAsydPJSZY0sJRzR5fi87a1sYWpgTN3iuLRSmqS3MUNdbTU1TgLICeSjZiMALQj+koTVIazBCnt/TLoabKptoDYXxKkUgHEFr2L6vmf1GFlHTHODfiysoyfdz2IQyNuxtJBCKsKKiljNmjWFsWT4b9jaytbqJJ5fvZEhBDvuNLGL59lrqW0Pk+73k+T1MGVFEU1uI3fWtHDS6hBUVdfi9qr3TO8frIRBOMNlJB8jxekDB+CH5bKxsojDHi9/nwefxMHl4Afk5PlZW1FLTHGTG2BL2H1XM1upmapoDbK5qYkxpPsdNH9H+VrO3oY361iDjhxTQEgxT0xRgWJFJ0fUoxSHjSqlsaKMgx8ee+lZK8/1MGVHIy2v34vMoRpflEQhFKMnzU9McZNZ4k2YaCmv8Pg+rd5jBYGOH5LN+TwNzJw2lviXIxGGF5Po8TBpeSF1LkByvh6I8H16lKCvw09gWoijXx7Z9zZTk+SnM9VHV2Ma7G6s5ZtpwRhbnojXsrm9t71/qKCLwgiDEEApH8CiFx6OIRDSNgRBa095pbOuCUsaqUkrRGjQCX5jjY0t1E5sqmzhoTAmRiGZfU4D3NlUzpDCHWePK2FTZyI7aFnL9Xhos4R1WlMOwwlze21TNxspGtIYt1U0cOmEI+5raqGow4yXW7Wkgx+th/NB86lqCVDcG2FXXSnlpHpurmjh4TAmtwTCbqprax1X4vSqmvyTP76E1GGFkcS7hiG4fiwHmDca8nZhaT+Uleeyub0VBUkvNZ43HSGe5Ocn3m+sU5nhpCoTxecwkQM52jizOpTkQpiTPx9vXntCpt5JUAp/ValJKqVOAP2Cmvvm71vqWNIcIgtAD+Bzz/3o8ipK82Gwgp9DYy3l+b7uNMnVEEVNHRGeTGj+0gFnjy9q/Ty9PPg4h1bZUaK3ZU99Geampod4aDOP3emgOhPB7PbSFIvi9isa2EMMKc6lsaGNUielk31XXSlGej9ZAmBHFubQGI3y0vYb9RhQxsiSPkPVG0hwMs7WqmYqaZsoKctBoSvL87DeyyDwoGgOMLMnl3U3VTBhawLbqZpoCIXbWtjCqJI/a5iBtoTDhCOypb2VIQQ41zQFK8v20hcKgTUd6vt9LUyDE+t0NlOb7mTW+jIgGbze7TlmL4JVSXmA98BmgAvgQOFdrvSbZMRLBC4IgdIxUEXw2p+w7HPhEa71Jax0AHgISTCwpCIIgZINsCvxYwFlircJaF4NS6lKl1GKl1OLKysosNkcQBGFw0euTbmut79Raz9FazxkxIsm0WIIgCEKHyabA7wCcU5WMs9YJgiAIPUA2Bf5DYJpSarJSKgf4EvBkmmMEQRCEbiJraZJa65BS6tvA85g0ybu11quzdT1BEAQhlqzmwWutnwGeyeY1BEEQhMT0eierIAiCkB36VKkCpVQlsDXtjokZDlR1Y3P6A3LPgwO558FBZ+95otY6YQpinxL4rqCUWpxsNNdARe55cCD3PDjIxj2LRSMIgjBAEYEXBEEYoAwkgb+ztxvQC8g9Dw7kngcH3X7PA8aDFwRBEGIZSBG8IAiC4EAEXhAEYYDS7wVeKXWKUmqdUuoTpdS1vd2e7kIpdbdSaq9SapVj3VCl1ItKqQ3W5xBrvVJK/dH6DVYopQ7rvZZ3HqXUeKXUq0qpNUqp1UqpK6z1A/a+lVJ5SqkPlFLLrXv+f9b6yUqp9617e9iq54RSKtf6/om1fVKv3kAXUEp5lVIfKaWesr4P6HtWSm1RSq1USi1TSi221mX1v+1+LfDWrFF/Af4POAg4Vyl1UO+2qtu4FzjFte5a4GWt9TTgZes7mPufZv1dCtzWQ23sbkLA97TWBwHzgMusf8+BfN9twAla61nAbOAUpdQ84JfA77TW+wE1wMXW/hcDNdb631n79VeuANY6vg+Gez5eaz3bke+e3f+2tdb99g84Enje8f064Lreblc33t8kYJXj+zpgtLU8GlhnLd+BmQ4xbr/+/Ac8gZnycVDcN1AALAWOwIxo9Fnr2/87xxTvO9Ja9ln7qd5ueyfudZwlaCcATwFqENzzFmC4a11W/9vu1xE8Gc4aNYAYpbXeZS3vBkZZywPud7Beww8F3meA37dlVSwD9gIvAhuBWq11yNrFeV/t92xtrwOG9WiDu4ffA9cAEev7MAb+PWvgBaXUEqXUpda6rP63ndVqkkL20FprpdSAzHFVShUBjwFXaq3rlYpONT8Q71trHQZmK6XKgP8AB/Rui7KLUup0YK/WeolS6rhebk5PcozWeodSaiTwolLqY+fGbPy33d8j+ME2a9QepdRoAOtzr7V+wPwOSik/Rtwf0Fo/bq0e8PcNoLWuBV7F2BNlSik7AHPeV/s9W9tLgeqebWmXORo4Uym1BXgIY9P8gYF9z2itd1ifezEP8sPJ8n/b/V3gB9usUU8CX7GWv4LxqO31F1g97/OAOsdrX79BmVD9LmCt1vq3jk0D9r6VUiOsyB2lVD6mz2EtRui/YO3mvmf7t/gC8Iq2TNr+gtb6Oq31OK31JMz/s69orRcxgO9ZKVWolCq2l4GTgFVk+7/t3u546IaOi1OB9Rjf8obebk833teDwC4giPHfLsb4ji8DG4CXgKHWvgqTTbQRWAnM6e32d/Kej8H4lCuAZdbfqQP5voFDgI+se14F/NhaPwX4APgE+DeQa63Ps75/Ym2f0tv30MX7Pw54aqDfs3Vvy62/1bZWZfu/bSlVIAiCMEDp7xaNIAiCkAQReEEQhAGKCLwgCMIARQReEARhgCICLwiCMEARgRf6NEoprZT6jeP795VSN3bhfMdY1Rs/tv4udWwbYVUr/EgpNd913GvKVC1dZv092tk2JGnXFqXU8O48pyBIqQKhr9MGfE4p9QutdVVXTqSUKgf+BZyttV5qCerzSqkdWuungROBlVrrryU5xSKt9eKutEEQehKJ4IW+TggzV+VV7g1KqUlKqVesetkvK6UmpDnXZcC9WuulANYD4xrgWqXUbOBXwFlWhJ6fSeOUUvcqpW5XSi1WSq236qzYdd7vsep/f6SUOt5a71VK3aqUWmW1+3LH6S5XSi21jjnA2v9Yx1vDR/ZoSEHIBBF4oT/wF2CRUqrUtf5PwD+01ocADwB/THOeg4ElrnWLgYO11suAHwMPa1OvuyXB8Q84xPbXjvWTMHVFTgNuV0rlYR4mWms9EzgX+Ie1/lJr/9mOdttUaa0Pw9T+/r617vvAZVrr2cB8IFG7BCEhIvBCn0drXQ/cB3zHtelIjOUCcD+m1EE2WWSJ/2yt9dWO9Y9orSNa6w3AJkw1yGOAfwJorT8GtgL7A58G7tBWWVyt9T7HeeziakswDwGAt4HfKqW+A5TpaDldQUiLCLzQX/g9ph5PYRfOsQb4lGvdpzC1QbqCu95HZ+t/tFmfYaz+Ma31LcDXgHzgbdu6EYRMEIEX+gVWpPsI0WncAN7BVCMEWAS8meY0fwEutPx2lFLDMNO//aqLzTtHKeVRSk3FFJVaZ7VlkXWd/YEJ1voXga/bZXGVUkNTnVgpNVVrvVJr/UtM9VQReCFjROCF/sRvAGcq4eXARUqpFcCXMXN8opT6hlLqG+6DtSm3ej7wN2uyhXeAu7XW/8vw+k4P/iXH+m2YKofPAt/QWrcCfwU8SqmVwMPAhVrrNuDv1v4rlFLLgfPSXPNKu0MWU1n02QzbKghSTVIQuoJS6l5MudtuzYsXhO5AInhBEIQBikTwgiAIAxSJ4AVBEAYoIvCCIAgDFBF4QRCEAYoIvCAIwgBFBF4QBGGA8v8BQ3ZaztgUjE4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "opt = optimizers.Adam(lr=0.0009)\n",
    "print('Train...')\n",
    "model.compile(optimizer = opt , loss=\"mse\")\n",
    "#model.compile(optimizer = \"adam\" , loss=\"mse\")\n",
    "history = model.fit([x_train,x_train], y_train, epochs = 500, batch_size=24, validation_split=0.1, shuffle=True)\n",
    "# history = model.fit(x_train, y_train, epochs = 500, batch_size=6, validation_split=0.1, shuffle=True)\n",
    "model.summary()\n",
    "#Save Model\n",
    "model.save('GRU_Single_Attention_model_alls.h5')  # creates a HDF5 file \n",
    "del model\n",
    "\n",
    "custom_ob = {'LayerNormalization': LayerNormalization , 'SeqSelfAttention':SeqSelfAttention}\n",
    "model = load_model('GRU_Single_Attention_model_alls.h5', custom_objects=custom_ob)\n",
    "t1 = time.time()\n",
    "y_pred2 = model.predict([x_test,x_test])\n",
    "#y_pred2 = model.predict(x_test)\n",
    "#y_pred = model.predict(x_train)\n",
    "t2 = time.time()\n",
    "print('Predict time: ',t2-t1)\n",
    "y_pred = scaler.inverse_transform(y_pred2)#Undo scaling\n",
    "rmse_lstm2 = np.sqrt(mean_squared_error(y_test, y_pred2))\n",
    "#rmse_lstm = np.sqrt(mean_squared_error(y_train, y_pred))\n",
    "#print('RMSE: ',rmse_lstm)\n",
    "print('RMSE2: ',rmse_lstm2)\n",
    "mae2 = mean_absolute_error(y_test, y_pred2)\n",
    "#mae = mean_absolute_error(y_train, y_pred)\n",
    "#print('MAE: ',mae)\n",
    "print('MAE2: ',mae2)\n",
    "r22 =  r2_score(y_test, y_pred2)\n",
    "# r2 =  r2_score(y_train, y_pred)\n",
    "# print('R-square: ',r2)\n",
    "print('R-square2: ',r22)\n",
    "\n",
    "# n = len(y_test)\n",
    "# p = 12\n",
    "# Adj_r2 = 1-(1-r2)*(n-1)/(n-p-1)\n",
    "# Adj_r22 = 1-(1-r22)*(n-1)/(n-p-1)\n",
    "# print('Adj R-square: ',Adj_r2)\n",
    "# print('Adj R-square2: ',Adj_r22)\n",
    "\n",
    "plt.plot(history.history[\"loss\"],label=\"loss\")\n",
    "plt.plot(history.history[\"val_loss\"],label=\"val_loss\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"No. Of Epochs\")\n",
    "plt.ylabel(\"mse score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7076422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>t1</th>\n",
       "      <th>t2</th>\n",
       "      <th>hum</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>weather_code</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>season</th>\n",
       "      <th>Great Marlborough Street, Soho(159)</th>\n",
       "      <th>Rathbone Street, Fitzrovia(306)</th>\n",
       "      <th>Frith Street, Soho</th>\n",
       "      <th>Green Street, Mayfair</th>\n",
       "      <th>Golden Square, Soho(129)</th>\n",
       "      <th>Craven Street, Strand(341)</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015/9/6 00:00</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015/9/6 01:00</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015/9/6 02:00</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015/9/6 03:00</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015/9/6 04:00</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>84.5</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        timestamp    t1    t2   hum  wind_speed  weather_code  is_holiday  \\\n",
       "0  2015/9/6 00:00  11.0  11.0  72.0         5.0             1           0   \n",
       "1  2015/9/6 01:00  10.0  10.0  82.0         6.0             1           0   \n",
       "2  2015/9/6 02:00  10.0  10.0  82.0         5.0             1           0   \n",
       "3  2015/9/6 03:00  10.0  10.0  79.0         6.0             1           0   \n",
       "4  2015/9/6 04:00   9.0   8.0  84.5         8.0             1           0   \n",
       "\n",
       "   is_weekend  season  Great Marlborough Street, Soho(159)  \\\n",
       "0           1       2                                    2   \n",
       "1           1       2                                    1   \n",
       "2           1       2                                    0   \n",
       "3           1       2                                    2   \n",
       "4           1       2                                    8   \n",
       "\n",
       "   Rathbone Street, Fitzrovia(306)  Frith Street, Soho  Green Street, Mayfair  \\\n",
       "0                                1                   1                      0   \n",
       "1                                1                  12                      0   \n",
       "2                                0                   6                      4   \n",
       "3                                0                   3                      0   \n",
       "4                                0                   0                      0   \n",
       "\n",
       "   Golden Square, Soho(129)  Craven Street, Strand(341)  cnt  \n",
       "0                         1                           2    2  \n",
       "1                         2                           2    7  \n",
       "2                         1                           0    2  \n",
       "3                         0                           0    1  \n",
       "4                         0                           0    1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = pd.read_csv(\"Mode2_bike_all.csv\")\n",
    "d.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2233ccbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
