{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9a39a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from tcn import TCN\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error, r2_score,mean_absolute_error\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential , load_model , Model\n",
    "from keras.layers import Dense, Dropout , LSTM , Bidirectional ,GRU ,Flatten,Add,BatchNormalization\n",
    "from keras_layer_normalization import LayerNormalization\n",
    "from keras.initializers import  glorot_normal, RandomUniform\n",
    "from keras import optimizers,Input\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55b36a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160, 13) (144, 13) (40, 13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:32: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7624c26da774bc0be8ce2c0a3d8dd70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:44: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae8ce09c4dae4c40ab10e6448e358fce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:\n",
      "(120, 24, 12) (120,)\n",
      "Test size:\n",
      "(16, 24, 12) (16,)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"station_bike _Pitfield.csv\")\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "df = df.set_index(\"timestamp\")\n",
    "#df.head()\n",
    "\n",
    "df[\"hour\"] = df.index.hour\n",
    "df[\"day_of_month\"] = df.index.day\n",
    "df[\"day_of_week\"]  = df.index.dayofweek\n",
    "df[\"month\"] = df.index.month\n",
    "\n",
    "training_data_len = math.ceil(len(df) * 0.9) # taking 90% of data to train and 10% of data to test\n",
    "testing_data_len = len(df) - training_data_len\n",
    "\n",
    "time_steps = 24\n",
    "train, test = df.iloc[0:training_data_len], df.iloc[(training_data_len-time_steps):len(df)]\n",
    "print(df.shape, train.shape, test.shape)\n",
    "train_trans = train[['t1','t2', 'hum', 'wind_speed']].to_numpy()\n",
    "test_trans = test[['t1','t2', 'hum', 'wind_speed']].to_numpy()\n",
    "\n",
    "scaler = RobustScaler() # Handles outliers\n",
    "#scaler = MinMaxScaler(feature_range=(0, 1)) # scale to (0,1)\n",
    "train.loc[:, ['t1','t2','hum', 'wind_speed']]=scaler.fit_transform(train_trans)\n",
    "test.loc[:, ['t1','t2', 'hum', 'wind_speed']]=scaler.fit_transform(test_trans)\n",
    "\n",
    "train['cnt'] = scaler.fit_transform(train[['cnt']])\n",
    "test['cnt'] = scaler.fit_transform(test[['cnt']])\n",
    "\n",
    "#Split the data into x_train and y_train data sets\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in tqdm(range(len(train) - time_steps)):\n",
    "    x_train.append(train.drop(columns='cnt').iloc[i:i + time_steps].to_numpy())\n",
    "    y_train.append(train.loc[:,'cnt'].iloc[i + time_steps])\n",
    "\n",
    "#Convert x_train and y_train to numpy arrays\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "#Create the x_test and y_test data sets\n",
    "x_test = []\n",
    "y_test = df.loc[:,'cnt'].iloc[training_data_len:len(df)]\n",
    "\n",
    "for i in tqdm(range(len(test) - time_steps)):\n",
    "    x_test.append(test.drop(columns='cnt').iloc[i:i + time_steps].to_numpy())\n",
    "    # y_test.append(test.loc[:,'cnt'].iloc[i + time_steps])\n",
    "\n",
    "#Convert x_test and y_test to numpy arrays\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# All 12 columns of the data\n",
    "print('Train size:')\n",
    "print(x_train.shape, y_train.shape)\n",
    "print('Test size:')\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8a611af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 24, 12)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 24, 24)       1800        input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "seq_self_attention_3 (SeqSelfAt (None, 24, 24)       577         bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 24, 24)       0           seq_self_attention_3[0][0]       \n",
      "                                                                 bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_3 (LayerNor (None, 24, 24)       48          add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 576)          0           layer_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 12)           6924        flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 12)           0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 588)          0           dropout_3[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            589         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 1)            2           dense_8[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 9,940\n",
      "Trainable params: 9,940\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Concatenate\n",
    "init = glorot_normal(seed=None) # 給 GRU\n",
    "init_d = RandomUniform(minval=-0.05, maxval=0.05) # 給 Dense layer\n",
    "\n",
    "def Encoder(layer):\n",
    "    shortcut = layer\n",
    "    layer = SeqSelfAttention(\n",
    "                     attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n",
    "                     kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     bias_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     attention_regularizer_weight=0.0001)(layer)\n",
    "    layer = Add()([layer,shortcut])\n",
    "    layer = LayerNormalization()(layer)\n",
    "    layer = Flatten()(layer)\n",
    "    \n",
    "    shortcut2 = layer\n",
    "    layer = Dense(12,kernel_initializer=init_d)(layer)\n",
    "    layer = Dropout(0.2)(layer)\n",
    "    layer = Concatenate()([layer,shortcut2])\n",
    "    output = Dense(1,kernel_initializer=init_d)(layer)\n",
    "    return output\n",
    "\n",
    "def Decoder(layer):\n",
    "    shortcut = layer\n",
    "    layer = SeqSelfAttention(\n",
    "                     attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n",
    "                     kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     bias_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     attention_regularizer_weight=0.0001)(layer)\n",
    "    layer = Add()([layer,shortcut])\n",
    "    layer = LayerNormalization()(layer)\n",
    "    layer = SeqSelfAttention(\n",
    "                     attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n",
    "                     kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     bias_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     attention_regularizer_weight=0.0001)(layer)\n",
    "    layer = LayerNormalization()(layer)\n",
    "    \n",
    "    layer = Flatten()(layer)\n",
    "    shortcut2 = layer\n",
    "    layer = Dense(10,kernel_initializer=init_d)(layer)\n",
    "    #layer = Dropout(0.2)(layer)\n",
    "    layer = Concatenate()([layer,shortcut2])\n",
    "    output = Dense(1,kernel_initializer=init_d)(layer)\n",
    "    return output\n",
    "\n",
    "def Bi_GRU(layer,unit):\n",
    "    output = Bidirectional(GRU(unit, dropout=0.1, recurrent_dropout=0.1, return_sequences=True,\n",
    "                            kernel_initializer=init))(layer)\n",
    "    return output\n",
    "\n",
    "#start = Input(shape = (x_train.shape[1],x_train.shape[2]))\n",
    "start = Input(shape = (x_train.shape[1:]))\n",
    "start2 = Input(shape = (x_train.shape[1:]))\n",
    "x = Bi_GRU(start,12)\n",
    "x = Encoder(x)\n",
    "\n",
    "# y = Bi_GRU(start2,8)\n",
    "# y = Decoder(y)\n",
    "\n",
    "#Merge = Add()([x,x])\n",
    "Last = Dense(1)(x)\n",
    "model = Model([start,start2] , Last)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "614dd4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 108 samples, validate on 12 samples\n",
      "Epoch 1/500\n",
      "108/108 [==============================] - 1s 14ms/step - loss: 1.7868 - val_loss: 1.2663\n",
      "Epoch 2/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.2648 - val_loss: 1.0038\n",
      "Epoch 3/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.4823 - val_loss: 0.6257\n",
      "Epoch 4/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.1912 - val_loss: 0.7033\n",
      "Epoch 5/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.6301 - val_loss: 0.8733\n",
      "Epoch 6/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.2266 - val_loss: 0.7568\n",
      "Epoch 7/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.0771 - val_loss: 0.5997\n",
      "Epoch 8/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.0308 - val_loss: 0.5188\n",
      "Epoch 9/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.9216 - val_loss: 0.4948\n",
      "Epoch 10/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.8328 - val_loss: 0.5707\n",
      "Epoch 11/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.9072 - val_loss: 0.5201\n",
      "Epoch 12/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.8079 - val_loss: 0.6356\n",
      "Epoch 13/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.7282 - val_loss: 0.4836\n",
      "Epoch 14/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.7922 - val_loss: 0.5509\n",
      "Epoch 15/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.7417 - val_loss: 0.5587\n",
      "Epoch 16/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.7696 - val_loss: 0.5060\n",
      "Epoch 17/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5886 - val_loss: 0.7865\n",
      "Epoch 18/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6282 - val_loss: 0.4888\n",
      "Epoch 19/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6712 - val_loss: 0.4777\n",
      "Epoch 20/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.7385 - val_loss: 0.4381\n",
      "Epoch 21/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5844 - val_loss: 0.4921\n",
      "Epoch 22/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5099 - val_loss: 0.4758\n",
      "Epoch 23/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4568 - val_loss: 0.4716\n",
      "Epoch 24/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5494 - val_loss: 0.4816\n",
      "Epoch 25/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5235 - val_loss: 0.5018\n",
      "Epoch 26/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4911 - val_loss: 0.5035\n",
      "Epoch 27/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5115 - val_loss: 0.4500\n",
      "Epoch 28/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4071 - val_loss: 0.6574\n",
      "Epoch 29/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4598 - val_loss: 0.5074\n",
      "Epoch 30/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4626 - val_loss: 0.4941\n",
      "Epoch 31/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4897 - val_loss: 0.4780\n",
      "Epoch 32/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5747 - val_loss: 0.4769\n",
      "Epoch 33/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4705 - val_loss: 0.4706\n",
      "Epoch 34/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4071 - val_loss: 0.4646\n",
      "Epoch 35/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3297 - val_loss: 0.4985\n",
      "Epoch 36/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3544 - val_loss: 0.5173\n",
      "Epoch 37/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3365 - val_loss: 0.5292\n",
      "Epoch 38/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3643 - val_loss: 0.4475\n",
      "Epoch 39/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4021 - val_loss: 0.5599\n",
      "Epoch 40/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4462 - val_loss: 0.6334\n",
      "Epoch 41/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2842 - val_loss: 0.5356\n",
      "Epoch 42/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2488 - val_loss: 0.5474\n",
      "Epoch 43/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2623 - val_loss: 0.5448\n",
      "Epoch 44/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3526 - val_loss: 0.5940\n",
      "Epoch 45/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3595 - val_loss: 0.4617\n",
      "Epoch 46/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3625 - val_loss: 0.4828\n",
      "Epoch 47/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3406 - val_loss: 0.4239\n",
      "Epoch 48/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3127 - val_loss: 0.5824\n",
      "Epoch 49/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2921 - val_loss: 0.6187\n",
      "Epoch 50/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2082 - val_loss: 0.5878\n",
      "Epoch 51/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3119 - val_loss: 0.5408\n",
      "Epoch 52/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2578 - val_loss: 0.6613\n",
      "Epoch 53/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3107 - val_loss: 0.5530\n",
      "Epoch 54/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3673 - val_loss: 0.5141\n",
      "Epoch 55/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2393 - val_loss: 0.5537\n",
      "Epoch 56/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2642 - val_loss: 0.5497\n",
      "Epoch 57/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2567 - val_loss: 0.4758\n",
      "Epoch 58/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2948 - val_loss: 0.5133\n",
      "Epoch 59/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2734 - val_loss: 0.5133\n",
      "Epoch 60/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3011 - val_loss: 0.5378\n",
      "Epoch 61/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2362 - val_loss: 0.6103\n",
      "Epoch 62/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3781 - val_loss: 0.6600\n",
      "Epoch 63/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3260 - val_loss: 0.4882\n",
      "Epoch 64/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2456 - val_loss: 0.4958\n",
      "Epoch 65/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2610 - val_loss: 0.5366\n",
      "Epoch 66/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2480 - val_loss: 0.5706\n",
      "Epoch 67/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3130 - val_loss: 0.4526\n",
      "Epoch 68/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3078 - val_loss: 0.5053\n",
      "Epoch 69/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2409 - val_loss: 0.5319\n",
      "Epoch 70/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2522 - val_loss: 0.5201\n",
      "Epoch 71/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2118 - val_loss: 0.5586\n",
      "Epoch 72/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1861 - val_loss: 0.5245\n",
      "Epoch 73/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2306 - val_loss: 0.4531\n",
      "Epoch 74/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2212 - val_loss: 0.5319\n",
      "Epoch 75/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2795 - val_loss: 0.5716\n",
      "Epoch 76/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2118 - val_loss: 0.5407\n",
      "Epoch 77/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2638 - val_loss: 0.5043\n",
      "Epoch 78/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2462 - val_loss: 0.5036\n",
      "Epoch 79/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2554 - val_loss: 0.5340\n",
      "Epoch 80/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2509 - val_loss: 0.4966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2422 - val_loss: 0.4579\n",
      "Epoch 82/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1993 - val_loss: 0.5620\n",
      "Epoch 83/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2204 - val_loss: 0.5519\n",
      "Epoch 84/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2060 - val_loss: 0.5296\n",
      "Epoch 85/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1941 - val_loss: 0.5230\n",
      "Epoch 86/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1633 - val_loss: 0.6177\n",
      "Epoch 87/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1896 - val_loss: 0.5593\n",
      "Epoch 88/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1881 - val_loss: 0.6896\n",
      "Epoch 89/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1645 - val_loss: 0.5975\n",
      "Epoch 90/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2032 - val_loss: 0.5578\n",
      "Epoch 91/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2222 - val_loss: 0.5958\n",
      "Epoch 92/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1633 - val_loss: 0.5562\n",
      "Epoch 93/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1883 - val_loss: 0.5375\n",
      "Epoch 94/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1616 - val_loss: 0.5886\n",
      "Epoch 95/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1763 - val_loss: 0.5939\n",
      "Epoch 96/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2952 - val_loss: 0.6261\n",
      "Epoch 97/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2110 - val_loss: 0.5746\n",
      "Epoch 98/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2061 - val_loss: 0.5569\n",
      "Epoch 99/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1883 - val_loss: 0.4724\n",
      "Epoch 100/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2281 - val_loss: 0.5967\n",
      "Epoch 101/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1742 - val_loss: 0.5522\n",
      "Epoch 102/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2376 - val_loss: 0.6013\n",
      "Epoch 103/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2072 - val_loss: 0.6920\n",
      "Epoch 104/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1632 - val_loss: 0.6918\n",
      "Epoch 105/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1203 - val_loss: 0.7481\n",
      "Epoch 106/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1736 - val_loss: 0.6665\n",
      "Epoch 107/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1537 - val_loss: 0.6542\n",
      "Epoch 108/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1289 - val_loss: 0.5469\n",
      "Epoch 109/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1557 - val_loss: 0.4637\n",
      "Epoch 110/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2318 - val_loss: 0.5756\n",
      "Epoch 111/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1578 - val_loss: 0.6539\n",
      "Epoch 112/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2390 - val_loss: 0.6580\n",
      "Epoch 113/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2059 - val_loss: 0.7925\n",
      "Epoch 114/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1868 - val_loss: 0.6187\n",
      "Epoch 115/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2172 - val_loss: 0.6218\n",
      "Epoch 116/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1856 - val_loss: 0.5955\n",
      "Epoch 117/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1685 - val_loss: 0.6078\n",
      "Epoch 118/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1879 - val_loss: 0.6904\n",
      "Epoch 119/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1285 - val_loss: 0.6925\n",
      "Epoch 120/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1434 - val_loss: 0.6082\n",
      "Epoch 121/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1999 - val_loss: 0.5629\n",
      "Epoch 122/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2069 - val_loss: 0.6689\n",
      "Epoch 123/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1710 - val_loss: 0.5875\n",
      "Epoch 124/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1668 - val_loss: 0.5399\n",
      "Epoch 125/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1592 - val_loss: 0.6123\n",
      "Epoch 126/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1534 - val_loss: 0.6217\n",
      "Epoch 127/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1266 - val_loss: 0.7034\n",
      "Epoch 128/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1560 - val_loss: 0.6881\n",
      "Epoch 129/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1763 - val_loss: 0.4939\n",
      "Epoch 130/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1330 - val_loss: 0.6092\n",
      "Epoch 131/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1591 - val_loss: 0.5634\n",
      "Epoch 132/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1412 - val_loss: 0.5244\n",
      "Epoch 133/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1364 - val_loss: 0.5767\n",
      "Epoch 134/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1449 - val_loss: 0.5304\n",
      "Epoch 135/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1718 - val_loss: 0.5829\n",
      "Epoch 136/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1608 - val_loss: 0.6270\n",
      "Epoch 137/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1562 - val_loss: 0.5930\n",
      "Epoch 138/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1502 - val_loss: 0.5706\n",
      "Epoch 139/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1432 - val_loss: 0.6005\n",
      "Epoch 140/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1309 - val_loss: 0.5380\n",
      "Epoch 141/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1428 - val_loss: 0.6687\n",
      "Epoch 142/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2032 - val_loss: 0.7577\n",
      "Epoch 143/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1532 - val_loss: 0.5502\n",
      "Epoch 144/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1186 - val_loss: 0.6857\n",
      "Epoch 145/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1675 - val_loss: 0.6050\n",
      "Epoch 146/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1610 - val_loss: 0.5166\n",
      "Epoch 147/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1305 - val_loss: 0.6298\n",
      "Epoch 148/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1480 - val_loss: 0.5915\n",
      "Epoch 149/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1593 - val_loss: 0.6236\n",
      "Epoch 150/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1498 - val_loss: 0.7153\n",
      "Epoch 151/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1897 - val_loss: 0.6842\n",
      "Epoch 152/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2160 - val_loss: 0.4792\n",
      "Epoch 153/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1430 - val_loss: 0.5270\n",
      "Epoch 154/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1060 - val_loss: 0.6173\n",
      "Epoch 155/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1021 - val_loss: 0.6009\n",
      "Epoch 156/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1178 - val_loss: 0.6516\n",
      "Epoch 157/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1179 - val_loss: 0.6198\n",
      "Epoch 158/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1171 - val_loss: 0.5954\n",
      "Epoch 159/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1127 - val_loss: 0.6529\n",
      "Epoch 160/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1837 - val_loss: 0.6348\n",
      "Epoch 161/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1315 - val_loss: 0.5689\n",
      "Epoch 162/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1892 - val_loss: 0.5327\n",
      "Epoch 163/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1276 - val_loss: 0.5060\n",
      "Epoch 164/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1252 - val_loss: 0.5486\n",
      "Epoch 165/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1274 - val_loss: 0.6083\n",
      "Epoch 166/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1615 - val_loss: 0.5153\n",
      "Epoch 167/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1552 - val_loss: 0.6721\n",
      "Epoch 168/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1555 - val_loss: 0.5225\n",
      "Epoch 169/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1150 - val_loss: 0.5080\n",
      "Epoch 170/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0898 - val_loss: 0.5382\n",
      "Epoch 171/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1173 - val_loss: 0.5879\n",
      "Epoch 172/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1258 - val_loss: 0.5050\n",
      "Epoch 173/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0932 - val_loss: 0.5059\n",
      "Epoch 174/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1486 - val_loss: 0.5716\n",
      "Epoch 175/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1290 - val_loss: 0.5896\n",
      "Epoch 176/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1223 - val_loss: 0.5813\n",
      "Epoch 177/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0754 - val_loss: 0.5200\n",
      "Epoch 178/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0795 - val_loss: 0.4995\n",
      "Epoch 179/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1055 - val_loss: 0.5835\n",
      "Epoch 180/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1678 - val_loss: 0.9883\n",
      "Epoch 181/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1208 - val_loss: 0.6190\n",
      "Epoch 182/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1035 - val_loss: 0.4864\n",
      "Epoch 183/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0836 - val_loss: 0.6542\n",
      "Epoch 184/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1045 - val_loss: 0.5715\n",
      "Epoch 185/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1081 - val_loss: 0.6166\n",
      "Epoch 186/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1086 - val_loss: 0.5951\n",
      "Epoch 187/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1426 - val_loss: 0.5277\n",
      "Epoch 188/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1573 - val_loss: 0.6026\n",
      "Epoch 189/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1719 - val_loss: 0.6668\n",
      "Epoch 190/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1240 - val_loss: 0.5574\n",
      "Epoch 191/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0913 - val_loss: 0.6880\n",
      "Epoch 192/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0951 - val_loss: 0.5511\n",
      "Epoch 193/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1098 - val_loss: 0.5840\n",
      "Epoch 194/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0940 - val_loss: 0.5804\n",
      "Epoch 195/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1262 - val_loss: 0.4763\n",
      "Epoch 196/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1774 - val_loss: 0.5893\n",
      "Epoch 197/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1588 - val_loss: 0.5335\n",
      "Epoch 198/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1087 - val_loss: 0.5246\n",
      "Epoch 199/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0872 - val_loss: 0.5980\n",
      "Epoch 200/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1385 - val_loss: 0.5735\n",
      "Epoch 201/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1234 - val_loss: 0.5981\n",
      "Epoch 202/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1920 - val_loss: 0.5807\n",
      "Epoch 203/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0941 - val_loss: 0.5809\n",
      "Epoch 204/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1029 - val_loss: 0.5104\n",
      "Epoch 205/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1142 - val_loss: 0.5216\n",
      "Epoch 206/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0866 - val_loss: 0.5533\n",
      "Epoch 207/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0774 - val_loss: 0.5468\n",
      "Epoch 208/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1512 - val_loss: 0.5594\n",
      "Epoch 209/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1104 - val_loss: 0.5120\n",
      "Epoch 210/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1208 - val_loss: 0.5695\n",
      "Epoch 211/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1172 - val_loss: 0.4667\n",
      "Epoch 212/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0997 - val_loss: 0.4676\n",
      "Epoch 213/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1080 - val_loss: 0.5714\n",
      "Epoch 214/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0891 - val_loss: 0.4989\n",
      "Epoch 215/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0871 - val_loss: 0.5041\n",
      "Epoch 216/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0725 - val_loss: 0.6168\n",
      "Epoch 217/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1003 - val_loss: 0.4737\n",
      "Epoch 218/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1152 - val_loss: 0.4820\n",
      "Epoch 219/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0794 - val_loss: 0.5414\n",
      "Epoch 220/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0878 - val_loss: 0.6227\n",
      "Epoch 221/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0957 - val_loss: 0.5137\n",
      "Epoch 222/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0961 - val_loss: 0.4891\n",
      "Epoch 223/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0932 - val_loss: 0.4998\n",
      "Epoch 224/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1000 - val_loss: 0.5247\n",
      "Epoch 225/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0820 - val_loss: 0.5582\n",
      "Epoch 226/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1342 - val_loss: 0.5513\n",
      "Epoch 227/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0883 - val_loss: 0.5224\n",
      "Epoch 228/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0991 - val_loss: 0.6220\n",
      "Epoch 229/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0867 - val_loss: 0.5778\n",
      "Epoch 230/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0958 - val_loss: 0.5665\n",
      "Epoch 231/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0999 - val_loss: 0.5129\n",
      "Epoch 232/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0750 - val_loss: 0.5141\n",
      "Epoch 233/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2342 - val_loss: 0.5594\n",
      "Epoch 234/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0928 - val_loss: 0.5407\n",
      "Epoch 235/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0900 - val_loss: 0.5215\n",
      "Epoch 236/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1516 - val_loss: 0.5963\n",
      "Epoch 237/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0747 - val_loss: 0.5529\n",
      "Epoch 238/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0960 - val_loss: 0.5205\n",
      "Epoch 239/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0919 - val_loss: 0.5659\n",
      "Epoch 240/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0905 - val_loss: 0.5753\n",
      "Epoch 241/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0883 - val_loss: 0.5480\n",
      "Epoch 242/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0715 - val_loss: 0.5836\n",
      "Epoch 243/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0881 - val_loss: 0.5743\n",
      "Epoch 244/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0672 - val_loss: 0.6039\n",
      "Epoch 245/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0783 - val_loss: 0.5154\n",
      "Epoch 246/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1121 - val_loss: 0.6025\n",
      "Epoch 247/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0562 - val_loss: 0.6315\n",
      "Epoch 248/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0738 - val_loss: 0.6225\n",
      "Epoch 249/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0610 - val_loss: 0.5933\n",
      "Epoch 250/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0784 - val_loss: 0.5696\n",
      "Epoch 251/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1075 - val_loss: 0.5066\n",
      "Epoch 252/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0890 - val_loss: 0.5240\n",
      "Epoch 253/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1106 - val_loss: 0.7356\n",
      "Epoch 254/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0924 - val_loss: 0.5790\n",
      "Epoch 255/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0942 - val_loss: 0.4958\n",
      "Epoch 256/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0772 - val_loss: 0.5447\n",
      "Epoch 257/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0597 - val_loss: 0.5561\n",
      "Epoch 258/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0985 - val_loss: 0.5909\n",
      "Epoch 259/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0794 - val_loss: 0.5483\n",
      "Epoch 260/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0944 - val_loss: 0.4993\n",
      "Epoch 261/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0550 - val_loss: 0.5537\n",
      "Epoch 262/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0784 - val_loss: 0.5307\n",
      "Epoch 263/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0895 - val_loss: 0.4970\n",
      "Epoch 264/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0716 - val_loss: 0.5905\n",
      "Epoch 265/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0558 - val_loss: 0.5192\n",
      "Epoch 266/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0890 - val_loss: 0.5132\n",
      "Epoch 267/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0795 - val_loss: 0.5899\n",
      "Epoch 268/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0709 - val_loss: 0.6514\n",
      "Epoch 269/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0883 - val_loss: 0.5285\n",
      "Epoch 270/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0440 - val_loss: 0.5455\n",
      "Epoch 271/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0654 - val_loss: 0.5983\n",
      "Epoch 272/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0617 - val_loss: 0.6168\n",
      "Epoch 273/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0864 - val_loss: 0.5999\n",
      "Epoch 274/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0984 - val_loss: 0.7026\n",
      "Epoch 275/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0898 - val_loss: 0.5834\n",
      "Epoch 276/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0878 - val_loss: 0.5636\n",
      "Epoch 277/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0718 - val_loss: 0.6239\n",
      "Epoch 278/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0792 - val_loss: 0.5910\n",
      "Epoch 279/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0891 - val_loss: 0.5873\n",
      "Epoch 280/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0889 - val_loss: 0.5568\n",
      "Epoch 281/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0650 - val_loss: 0.6814\n",
      "Epoch 282/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0779 - val_loss: 0.5269\n",
      "Epoch 283/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0777 - val_loss: 0.5041\n",
      "Epoch 284/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0708 - val_loss: 0.5458\n",
      "Epoch 285/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0843 - val_loss: 0.5659\n",
      "Epoch 286/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0822 - val_loss: 0.5025\n",
      "Epoch 287/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0703 - val_loss: 0.5448\n",
      "Epoch 288/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0743 - val_loss: 0.5544\n",
      "Epoch 289/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0695 - val_loss: 0.5007\n",
      "Epoch 290/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0758 - val_loss: 0.5284\n",
      "Epoch 291/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0613 - val_loss: 0.4931\n",
      "Epoch 292/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0851 - val_loss: 0.6368\n",
      "Epoch 293/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0754 - val_loss: 0.5964\n",
      "Epoch 294/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0545 - val_loss: 0.6999\n",
      "Epoch 295/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0479 - val_loss: 0.7443\n",
      "Epoch 296/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0738 - val_loss: 0.6105\n",
      "Epoch 297/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0664 - val_loss: 0.5553\n",
      "Epoch 298/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0680 - val_loss: 0.5913\n",
      "Epoch 299/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0864 - val_loss: 0.5727\n",
      "Epoch 300/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0617 - val_loss: 0.5614\n",
      "Epoch 301/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0598 - val_loss: 0.5593\n",
      "Epoch 302/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0620 - val_loss: 0.6949\n",
      "Epoch 303/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1481 - val_loss: 0.6818\n",
      "Epoch 304/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0657 - val_loss: 0.6007\n",
      "Epoch 305/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0862 - val_loss: 0.6501\n",
      "Epoch 306/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0438 - val_loss: 0.5497\n",
      "Epoch 307/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0532 - val_loss: 0.5736\n",
      "Epoch 308/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0498 - val_loss: 0.6582\n",
      "Epoch 309/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0671 - val_loss: 0.5970\n",
      "Epoch 310/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0674 - val_loss: 0.5694\n",
      "Epoch 311/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0695 - val_loss: 0.5850\n",
      "Epoch 312/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0833 - val_loss: 0.6454\n",
      "Epoch 313/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0790 - val_loss: 0.6963\n",
      "Epoch 314/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0690 - val_loss: 0.6389\n",
      "Epoch 315/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0732 - val_loss: 0.6156\n",
      "Epoch 316/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0629 - val_loss: 0.6226\n",
      "Epoch 317/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0576 - val_loss: 0.6726\n",
      "Epoch 318/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1011 - val_loss: 0.6023\n",
      "Epoch 319/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0614 - val_loss: 0.5816\n",
      "Epoch 320/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0643 - val_loss: 0.5055\n",
      "Epoch 321/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0682 - val_loss: 0.5081\n",
      "Epoch 322/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0483 - val_loss: 0.6338\n",
      "Epoch 323/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0750 - val_loss: 0.5894\n",
      "Epoch 324/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0672 - val_loss: 0.5889\n",
      "Epoch 325/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0652 - val_loss: 0.4868\n",
      "Epoch 326/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0535 - val_loss: 0.5792\n",
      "Epoch 327/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0543 - val_loss: 0.5591\n",
      "Epoch 328/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0495 - val_loss: 0.5543\n",
      "Epoch 329/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0593 - val_loss: 0.5648\n",
      "Epoch 330/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0554 - val_loss: 0.5412\n",
      "Epoch 331/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0681 - val_loss: 0.6573\n",
      "Epoch 332/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0741 - val_loss: 0.5414\n",
      "Epoch 333/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0631 - val_loss: 0.5328\n",
      "Epoch 334/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0755 - val_loss: 0.4732\n",
      "Epoch 335/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0773 - val_loss: 0.5961\n",
      "Epoch 336/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0615 - val_loss: 0.6573\n",
      "Epoch 337/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0617 - val_loss: 0.6382\n",
      "Epoch 338/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0682 - val_loss: 0.6185\n",
      "Epoch 339/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0631 - val_loss: 0.5895\n",
      "Epoch 340/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0660 - val_loss: 0.4935\n",
      "Epoch 341/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0852 - val_loss: 0.4666\n",
      "Epoch 342/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0612 - val_loss: 0.4902\n",
      "Epoch 343/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0782 - val_loss: 0.4907\n",
      "Epoch 344/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0860 - val_loss: 0.5082\n",
      "Epoch 345/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0739 - val_loss: 0.5158\n",
      "Epoch 346/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0661 - val_loss: 0.5854\n",
      "Epoch 347/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0421 - val_loss: 0.5457\n",
      "Epoch 348/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0565 - val_loss: 0.6063\n",
      "Epoch 349/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0519 - val_loss: 0.5695\n",
      "Epoch 350/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0587 - val_loss: 0.6122\n",
      "Epoch 351/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0621 - val_loss: 0.5307\n",
      "Epoch 352/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0590 - val_loss: 0.6619\n",
      "Epoch 353/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0712 - val_loss: 0.5717\n",
      "Epoch 354/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0532 - val_loss: 0.5241\n",
      "Epoch 355/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0496 - val_loss: 0.5648\n",
      "Epoch 356/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0552 - val_loss: 0.5825\n",
      "Epoch 357/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0566 - val_loss: 0.5908\n",
      "Epoch 358/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0431 - val_loss: 0.5360\n",
      "Epoch 359/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0622 - val_loss: 0.5222\n",
      "Epoch 360/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0442 - val_loss: 0.5522\n",
      "Epoch 361/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0647 - val_loss: 0.6578\n",
      "Epoch 362/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0461 - val_loss: 0.5137\n",
      "Epoch 363/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0483 - val_loss: 0.5302\n",
      "Epoch 364/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0844 - val_loss: 0.5680\n",
      "Epoch 365/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0483 - val_loss: 0.6462\n",
      "Epoch 366/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1052 - val_loss: 0.5809\n",
      "Epoch 367/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0711 - val_loss: 0.5589\n",
      "Epoch 368/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0914 - val_loss: 0.5621\n",
      "Epoch 369/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0644 - val_loss: 0.5147\n",
      "Epoch 370/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0601 - val_loss: 0.6402\n",
      "Epoch 371/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0569 - val_loss: 0.5842\n",
      "Epoch 372/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0434 - val_loss: 0.5585\n",
      "Epoch 373/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0762 - val_loss: 0.6558\n",
      "Epoch 374/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0472 - val_loss: 0.5961\n",
      "Epoch 375/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0657 - val_loss: 0.5243\n",
      "Epoch 376/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0505 - val_loss: 0.5413\n",
      "Epoch 377/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0561 - val_loss: 0.5019\n",
      "Epoch 378/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0589 - val_loss: 0.4843\n",
      "Epoch 379/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0524 - val_loss: 0.5857\n",
      "Epoch 380/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0535 - val_loss: 0.5421\n",
      "Epoch 381/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0588 - val_loss: 0.5443\n",
      "Epoch 382/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0519 - val_loss: 0.5604\n",
      "Epoch 383/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0526 - val_loss: 0.5528\n",
      "Epoch 384/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0608 - val_loss: 0.5593\n",
      "Epoch 385/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0346 - val_loss: 0.6020\n",
      "Epoch 386/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0487 - val_loss: 0.5359\n",
      "Epoch 387/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1339 - val_loss: 0.6242\n",
      "Epoch 388/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0769 - val_loss: 0.5401\n",
      "Epoch 389/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0759 - val_loss: 0.6287\n",
      "Epoch 390/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0532 - val_loss: 0.5135\n",
      "Epoch 391/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0487 - val_loss: 0.6138\n",
      "Epoch 392/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0502 - val_loss: 0.5679\n",
      "Epoch 393/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0438 - val_loss: 0.5858\n",
      "Epoch 394/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0361 - val_loss: 0.5997\n",
      "Epoch 395/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0455 - val_loss: 0.4785\n",
      "Epoch 396/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0725 - val_loss: 0.7141\n",
      "Epoch 397/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0511 - val_loss: 0.5743\n",
      "Epoch 398/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0557 - val_loss: 0.5999\n",
      "Epoch 399/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0787 - val_loss: 0.5916\n",
      "Epoch 400/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0727 - val_loss: 0.6015\n",
      "Epoch 401/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0584 - val_loss: 0.5938\n",
      "Epoch 402/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0651 - val_loss: 0.6847\n",
      "Epoch 403/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0789 - val_loss: 0.5500\n",
      "Epoch 404/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0860 - val_loss: 0.5701\n",
      "Epoch 405/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0631 - val_loss: 0.6633\n",
      "Epoch 406/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0602 - val_loss: 0.5398\n",
      "Epoch 407/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0503 - val_loss: 0.6311\n",
      "Epoch 408/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0471 - val_loss: 0.5517\n",
      "Epoch 409/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0487 - val_loss: 0.5455\n",
      "Epoch 410/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0450 - val_loss: 0.6243\n",
      "Epoch 411/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0444 - val_loss: 0.5904\n",
      "Epoch 412/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0518 - val_loss: 0.5346\n",
      "Epoch 413/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0563 - val_loss: 0.6145\n",
      "Epoch 414/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0639 - val_loss: 0.4743\n",
      "Epoch 415/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1381 - val_loss: 0.5556\n",
      "Epoch 416/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0585 - val_loss: 0.6383\n",
      "Epoch 417/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0446 - val_loss: 0.6100\n",
      "Epoch 418/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0814 - val_loss: 0.5767\n",
      "Epoch 419/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0723 - val_loss: 0.5771\n",
      "Epoch 420/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0685 - val_loss: 0.6474\n",
      "Epoch 421/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0488 - val_loss: 0.5475\n",
      "Epoch 422/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0550 - val_loss: 0.5125\n",
      "Epoch 423/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0813 - val_loss: 0.6435\n",
      "Epoch 424/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0681 - val_loss: 0.5697\n",
      "Epoch 425/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0585 - val_loss: 0.5312\n",
      "Epoch 426/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0643 - val_loss: 0.5272\n",
      "Epoch 427/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0465 - val_loss: 0.6520\n",
      "Epoch 428/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0660 - val_loss: 0.4857\n",
      "Epoch 429/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0654 - val_loss: 0.5931\n",
      "Epoch 430/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0503 - val_loss: 0.6170\n",
      "Epoch 431/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0408 - val_loss: 0.6288\n",
      "Epoch 432/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0452 - val_loss: 0.5995\n",
      "Epoch 433/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0527 - val_loss: 0.5873\n",
      "Epoch 434/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0525 - val_loss: 0.5986\n",
      "Epoch 435/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0469 - val_loss: 0.6619\n",
      "Epoch 436/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0416 - val_loss: 0.6425\n",
      "Epoch 437/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0867 - val_loss: 0.6044\n",
      "Epoch 438/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0461 - val_loss: 0.6143\n",
      "Epoch 439/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0456 - val_loss: 0.6757\n",
      "Epoch 440/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1154 - val_loss: 0.6035\n",
      "Epoch 441/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0489 - val_loss: 0.5770\n",
      "Epoch 442/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0367 - val_loss: 0.5717\n",
      "Epoch 443/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0390 - val_loss: 0.5624\n",
      "Epoch 444/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0505 - val_loss: 0.6428\n",
      "Epoch 445/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0488 - val_loss: 0.6611\n",
      "Epoch 446/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0548 - val_loss: 0.5353\n",
      "Epoch 447/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0440 - val_loss: 0.6011\n",
      "Epoch 448/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0790 - val_loss: 0.6490\n",
      "Epoch 449/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0473 - val_loss: 0.5953\n",
      "Epoch 450/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0521 - val_loss: 0.6316\n",
      "Epoch 451/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0937 - val_loss: 0.5419\n",
      "Epoch 452/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0466 - val_loss: 0.5957\n",
      "Epoch 453/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0673 - val_loss: 0.5671\n",
      "Epoch 454/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0496 - val_loss: 0.5339\n",
      "Epoch 455/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0489 - val_loss: 0.5291\n",
      "Epoch 456/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0476 - val_loss: 0.6658\n",
      "Epoch 457/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0501 - val_loss: 0.5716\n",
      "Epoch 458/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0716 - val_loss: 0.5080\n",
      "Epoch 459/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0601 - val_loss: 0.6440\n",
      "Epoch 460/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0483 - val_loss: 0.5098\n",
      "Epoch 461/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0609 - val_loss: 0.5515\n",
      "Epoch 462/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0422 - val_loss: 0.6403\n",
      "Epoch 463/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0372 - val_loss: 0.5154\n",
      "Epoch 464/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0337 - val_loss: 0.5725\n",
      "Epoch 465/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0423 - val_loss: 0.6178\n",
      "Epoch 466/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0452 - val_loss: 0.5001\n",
      "Epoch 467/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1086 - val_loss: 0.4803\n",
      "Epoch 468/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0639 - val_loss: 0.5552\n",
      "Epoch 469/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0489 - val_loss: 0.5635\n",
      "Epoch 470/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0488 - val_loss: 0.6174\n",
      "Epoch 471/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0402 - val_loss: 0.5660\n",
      "Epoch 472/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0298 - val_loss: 0.5554\n",
      "Epoch 473/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0367 - val_loss: 0.5823\n",
      "Epoch 474/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0483 - val_loss: 0.5467\n",
      "Epoch 475/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0442 - val_loss: 0.5946\n",
      "Epoch 476/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0472 - val_loss: 0.6018\n",
      "Epoch 477/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0447 - val_loss: 0.5003\n",
      "Epoch 478/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0482 - val_loss: 0.5930\n",
      "Epoch 479/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0600 - val_loss: 0.5347\n",
      "Epoch 480/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0617 - val_loss: 0.5520\n",
      "Epoch 481/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0513 - val_loss: 0.5820\n",
      "Epoch 482/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0510 - val_loss: 0.5490\n",
      "Epoch 483/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0558 - val_loss: 0.6097\n",
      "Epoch 484/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0642 - val_loss: 0.5307\n",
      "Epoch 485/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0423 - val_loss: 0.5566\n",
      "Epoch 486/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0424 - val_loss: 0.5075\n",
      "Epoch 487/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0549 - val_loss: 0.5966\n",
      "Epoch 488/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0522 - val_loss: 0.6394\n",
      "Epoch 489/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0397 - val_loss: 0.5471\n",
      "Epoch 490/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0723 - val_loss: 0.5321\n",
      "Epoch 491/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0726 - val_loss: 0.5920\n",
      "Epoch 492/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0523 - val_loss: 0.5237\n",
      "Epoch 493/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0922 - val_loss: 0.5375\n",
      "Epoch 494/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0521 - val_loss: 0.5462\n",
      "Epoch 495/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0491 - val_loss: 0.5466\n",
      "Epoch 496/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0510 - val_loss: 0.5614\n",
      "Epoch 497/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0397 - val_loss: 0.5719\n",
      "Epoch 498/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0771 - val_loss: 0.5633\n",
      "Epoch 499/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0438 - val_loss: 0.5212\n",
      "Epoch 500/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0448 - val_loss: 0.5084\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 24, 12)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 24, 24)       1800        input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "seq_self_attention_3 (SeqSelfAt (None, 24, 24)       577         bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 24, 24)       0           seq_self_attention_3[0][0]       \n",
      "                                                                 bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_3 (LayerNor (None, 24, 24)       48          add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 576)          0           layer_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 12)           6924        flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 12)           0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 588)          0           dropout_3[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            589         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 1)            2           dense_8[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 9,940\n",
      "Trainable params: 9,940\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Predict time:  0.30518341064453125\n",
      "RMSE:  8.424069168798404\n",
      "RMSE2:  6.738826888786874\n",
      "MAE:  6.809965016841889\n",
      "MAE2:  6.809965016841889\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'mse score')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "opt = optimizers.Adam(lr=0.001,decay = 0.0001)\n",
    "print('Train...')\n",
    "# model.compile(optimizer = opt , loss=\"mse\")\n",
    "model.compile(optimizer = \"adam\" , loss=\"mse\")\n",
    "history = model.fit([x_train,x_train], y_train, epochs = 500, batch_size=8, validation_split=0.1, shuffle=True)\n",
    "# history = model.fit(x_train, y_train, epochs = 500, batch_size=6, validation_split=0.1, shuffle=True)\n",
    "model.summary()\n",
    "#Save Model\n",
    "model.save('GRU_Single_Attention_model_Pitfield.h5')  # creates a HDF5 file \n",
    "del model\n",
    "\n",
    "custom_ob = {'LayerNormalization': LayerNormalization , 'SeqSelfAttention':SeqSelfAttention}\n",
    "model = load_model('GRU_Single_Attention_model_Pitfield.h5', custom_objects=custom_ob)\n",
    "t1 = time.time()\n",
    "# y_pred = model.predict([x_test,x_test])\n",
    "y_pred2 = model.predict(x_test)\n",
    "y_pred = model.predict(x_train)\n",
    "t2 = time.time()\n",
    "print('Predict time: ',t2-t1)\n",
    "y_pred = scaler.inverse_transform(y_pred)#Undo scaling\n",
    "rmse_lstm2 = np.sqrt(mean_squared_error(y_test, y_pred2))\n",
    "rmse_lstm = np.sqrt(mean_squared_error(y_train, y_pred))\n",
    "print('RMSE: ',rmse_lstm)\n",
    "print('RMSE2: ',rmse_lstm2)\n",
    "mae = mean_absolute_error(y_test, y_pred2)\n",
    "mae = mean_absolute_error(y_train, y_pred)\n",
    "print('MAE: ',mae)\n",
    "print('MAE2: ',mae)\n",
    "# r22 =  r2_score(y_test, y_pred2)\n",
    "# r2 =  r2_score(y_train, y_pred)\n",
    "# print('R-square: ',r2)\n",
    "# print('R-square2: ',r22)\n",
    "\n",
    "# n = len(y_test)\n",
    "# p = 12\n",
    "# Adj_r2 = 1-(1-r2)*(n-1)/(n-p-1)\n",
    "# Adj_r22 = 1-(1-r22)*(n-1)/(n-p-1)\n",
    "# print('Adj R-square: ',Adj_r2)\n",
    "# print('Adj R-square2: ',Adj_r22)\n",
    "\n",
    "plt.plot(history.history[\"loss\"],label=\"loss\")\n",
    "plt.plot(history.history[\"val_loss\"],label=\"val_loss\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"No. Of Epochs\")\n",
    "plt.ylabel(\"mse score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37a123b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 108 samples, validate on 12 samples\n",
      "Epoch 1/200\n",
      "108/108 [==============================] - 2s 18ms/step - loss: 1.9304 - val_loss: 1.5100\n",
      "Epoch 2/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 1.8735 - val_loss: 1.4417\n",
      "Epoch 3/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 1.8271 - val_loss: 1.3760\n",
      "Epoch 4/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 1.7680 - val_loss: 1.3120\n",
      "Epoch 5/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 1.7290 - val_loss: 1.2568\n",
      "Epoch 6/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 1.6747 - val_loss: 1.2072\n",
      "Epoch 7/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.6420 - val_loss: 1.1651\n",
      "Epoch 8/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.5954 - val_loss: 1.1215\n",
      "Epoch 9/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.5645 - val_loss: 1.0858\n",
      "Epoch 10/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.5296 - val_loss: 1.0504\n",
      "Epoch 11/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.4840 - val_loss: 1.0169\n",
      "Epoch 12/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.4034 - val_loss: 0.9833\n",
      "Epoch 13/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.3741 - val_loss: 0.9522\n",
      "Epoch 14/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.3804 - val_loss: 0.9259\n",
      "Epoch 15/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.3196 - val_loss: 0.8981\n",
      "Epoch 16/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.3051 - val_loss: 0.8728\n",
      "Epoch 17/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.2811 - val_loss: 0.8513\n",
      "Epoch 18/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 1.2459 - val_loss: 0.8326\n",
      "Epoch 19/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 1.1793 - val_loss: 0.8106\n",
      "Epoch 20/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.2150 - val_loss: 0.7948\n",
      "Epoch 21/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.1575 - val_loss: 0.7786\n",
      "Epoch 22/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.1370 - val_loss: 0.7627\n",
      "Epoch 23/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.1295 - val_loss: 0.7429\n",
      "Epoch 24/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.1155 - val_loss: 0.7277\n",
      "Epoch 25/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 1.1083 - val_loss: 0.7159\n",
      "Epoch 26/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.1001 - val_loss: 0.7021\n",
      "Epoch 27/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.0491 - val_loss: 0.6939\n",
      "Epoch 28/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.0466 - val_loss: 0.6817\n",
      "Epoch 29/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.0358 - val_loss: 0.6649\n",
      "Epoch 30/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.0764 - val_loss: 0.6549\n",
      "Epoch 31/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 1.0320 - val_loss: 0.6434\n",
      "Epoch 32/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.0282 - val_loss: 0.6353\n",
      "Epoch 33/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 1.0068 - val_loss: 0.6258\n",
      "Epoch 34/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 1.0280 - val_loss: 0.6144\n",
      "Epoch 35/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.9865 - val_loss: 0.6058\n",
      "Epoch 36/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.9935 - val_loss: 0.5992\n",
      "Epoch 37/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.0009 - val_loss: 0.5911\n",
      "Epoch 38/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.9749 - val_loss: 0.5892\n",
      "Epoch 39/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.9988 - val_loss: 0.5781\n",
      "Epoch 40/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.9893 - val_loss: 0.5701\n",
      "Epoch 41/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.9600 - val_loss: 0.5662\n",
      "Epoch 42/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.9117 - val_loss: 0.5595\n",
      "Epoch 43/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.9097 - val_loss: 0.5532\n",
      "Epoch 44/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.9553 - val_loss: 0.5480\n",
      "Epoch 45/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.9120 - val_loss: 0.5380\n",
      "Epoch 46/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.9110 - val_loss: 0.5316\n",
      "Epoch 47/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.8983 - val_loss: 0.5344\n",
      "Epoch 48/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.9651 - val_loss: 0.5268\n",
      "Epoch 49/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.8423 - val_loss: 0.5162\n",
      "Epoch 50/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.8928 - val_loss: 0.5121\n",
      "Epoch 51/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.8920 - val_loss: 0.5081\n",
      "Epoch 52/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.8299 - val_loss: 0.5018\n",
      "Epoch 53/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.8632 - val_loss: 0.4946\n",
      "Epoch 54/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.8575 - val_loss: 0.4927\n",
      "Epoch 55/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.8279 - val_loss: 0.4881\n",
      "Epoch 56/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.8798 - val_loss: 0.4761\n",
      "Epoch 57/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8102 - val_loss: 0.4736\n",
      "Epoch 58/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8390 - val_loss: 0.4653\n",
      "Epoch 59/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.8227 - val_loss: 0.4624\n",
      "Epoch 60/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8208 - val_loss: 0.4577\n",
      "Epoch 61/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.8354 - val_loss: 0.4593\n",
      "Epoch 62/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.8122 - val_loss: 0.4525\n",
      "Epoch 63/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8225 - val_loss: 0.4440\n",
      "Epoch 64/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7783 - val_loss: 0.4362\n",
      "Epoch 65/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7573 - val_loss: 0.4317\n",
      "Epoch 66/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7518 - val_loss: 0.4291\n",
      "Epoch 67/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7837 - val_loss: 0.4467\n",
      "Epoch 68/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7254 - val_loss: 0.4317\n",
      "Epoch 69/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7370 - val_loss: 0.4243\n",
      "Epoch 70/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7350 - val_loss: 0.4098\n",
      "Epoch 71/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7919 - val_loss: 0.4006\n",
      "Epoch 72/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7764 - val_loss: 0.4144\n",
      "Epoch 73/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7126 - val_loss: 0.4365\n",
      "Epoch 74/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.7153 - val_loss: 0.4385\n",
      "Epoch 75/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6761 - val_loss: 0.3989\n",
      "Epoch 76/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7160 - val_loss: 0.4371\n",
      "Epoch 77/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7083 - val_loss: 0.4048\n",
      "Epoch 78/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6721 - val_loss: 0.4528\n",
      "Epoch 79/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6933 - val_loss: 0.4178\n",
      "Epoch 80/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7030 - val_loss: 0.4989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6794 - val_loss: 0.4748\n",
      "Epoch 82/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6658 - val_loss: 0.4446\n",
      "Epoch 83/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6599 - val_loss: 0.4255\n",
      "Epoch 84/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6779 - val_loss: 0.3985\n",
      "Epoch 85/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6531 - val_loss: 0.3992\n",
      "Epoch 86/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6530 - val_loss: 0.3878\n",
      "Epoch 87/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6608 - val_loss: 0.4631\n",
      "Epoch 88/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6260 - val_loss: 0.4185\n",
      "Epoch 89/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6110 - val_loss: 0.5198\n",
      "Epoch 90/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6050 - val_loss: 0.4073\n",
      "Epoch 91/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5982 - val_loss: 0.4523\n",
      "Epoch 92/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6034 - val_loss: 0.4327\n",
      "Epoch 93/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5683 - val_loss: 0.4729\n",
      "Epoch 94/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5382 - val_loss: 0.4009\n",
      "Epoch 95/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5907 - val_loss: 0.4298\n",
      "Epoch 96/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5982 - val_loss: 0.4566\n",
      "Epoch 97/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5617 - val_loss: 0.4368\n",
      "Epoch 98/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5529 - val_loss: 0.4890\n",
      "Epoch 99/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5333 - val_loss: 0.4225\n",
      "Epoch 100/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5619 - val_loss: 0.4904\n",
      "Epoch 101/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5278 - val_loss: 0.4794\n",
      "Epoch 102/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5073 - val_loss: 0.4091\n",
      "Epoch 103/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5239 - val_loss: 0.4079\n",
      "Epoch 104/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5360 - val_loss: 0.4054\n",
      "Epoch 105/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5999 - val_loss: 0.4126\n",
      "Epoch 106/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5792 - val_loss: 0.4120\n",
      "Epoch 107/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5759 - val_loss: 0.4093\n",
      "Epoch 108/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5045 - val_loss: 0.3885\n",
      "Epoch 109/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5430 - val_loss: 0.4017\n",
      "Epoch 110/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5600 - val_loss: 0.4296\n",
      "Epoch 111/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5117 - val_loss: 0.3700\n",
      "Epoch 112/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5578 - val_loss: 0.4342\n",
      "Epoch 113/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5569 - val_loss: 0.4169\n",
      "Epoch 114/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5345 - val_loss: 0.3887\n",
      "Epoch 115/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5754 - val_loss: 0.4308\n",
      "Epoch 116/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4995 - val_loss: 0.4830\n",
      "Epoch 117/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5486 - val_loss: 0.3827\n",
      "Epoch 118/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5482 - val_loss: 0.4047\n",
      "Epoch 119/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5270 - val_loss: 0.4710\n",
      "Epoch 120/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4912 - val_loss: 0.4281\n",
      "Epoch 121/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5175 - val_loss: 0.4024\n",
      "Epoch 122/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5159 - val_loss: 0.3909\n",
      "Epoch 123/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4798 - val_loss: 0.4354\n",
      "Epoch 124/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4762 - val_loss: 0.4370\n",
      "Epoch 125/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5111 - val_loss: 0.4248\n",
      "Epoch 126/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4716 - val_loss: 0.4312\n",
      "Epoch 127/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4899 - val_loss: 0.4002\n",
      "Epoch 128/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5061 - val_loss: 0.4064\n",
      "Epoch 129/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4836 - val_loss: 0.3979\n",
      "Epoch 130/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5004 - val_loss: 0.4449\n",
      "Epoch 131/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5236 - val_loss: 0.3594\n",
      "Epoch 132/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4599 - val_loss: 0.3925\n",
      "Epoch 133/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4574 - val_loss: 0.3818\n",
      "Epoch 134/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5132 - val_loss: 0.3998\n",
      "Epoch 135/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4692 - val_loss: 0.3848\n",
      "Epoch 136/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4893 - val_loss: 0.3809\n",
      "Epoch 137/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4763 - val_loss: 0.3752\n",
      "Epoch 138/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4853 - val_loss: 0.3798\n",
      "Epoch 139/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4822 - val_loss: 0.3884\n",
      "Epoch 140/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5097 - val_loss: 0.4709\n",
      "Epoch 141/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5121 - val_loss: 0.3999\n",
      "Epoch 142/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4834 - val_loss: 0.3908\n",
      "Epoch 143/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4704 - val_loss: 0.3845\n",
      "Epoch 144/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4494 - val_loss: 0.4076\n",
      "Epoch 145/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4616 - val_loss: 0.4025\n",
      "Epoch 146/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5068 - val_loss: 0.4194\n",
      "Epoch 147/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5045 - val_loss: 0.3803\n",
      "Epoch 148/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4465 - val_loss: 0.3683\n",
      "Epoch 149/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4813 - val_loss: 0.3655\n",
      "Epoch 150/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4363 - val_loss: 0.3831\n",
      "Epoch 151/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4309 - val_loss: 0.3884\n",
      "Epoch 152/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5033 - val_loss: 0.4106\n",
      "Epoch 153/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4513 - val_loss: 0.3946\n",
      "Epoch 154/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4443 - val_loss: 0.3761\n",
      "Epoch 155/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5378 - val_loss: 0.3770\n",
      "Epoch 156/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4071 - val_loss: 0.3757\n",
      "Epoch 157/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4453 - val_loss: 0.3957\n",
      "Epoch 158/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4010 - val_loss: 0.4138\n",
      "Epoch 159/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4190 - val_loss: 0.3928\n",
      "Epoch 160/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4317 - val_loss: 0.4199\n",
      "Epoch 161/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4791 - val_loss: 0.4095\n",
      "Epoch 162/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4883 - val_loss: 0.4115\n",
      "Epoch 163/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4658 - val_loss: 0.3927\n",
      "Epoch 164/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4183 - val_loss: 0.3744\n",
      "Epoch 165/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4357 - val_loss: 0.3989\n",
      "Epoch 166/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5235 - val_loss: 0.3591\n",
      "Epoch 167/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4763 - val_loss: 0.3818\n",
      "Epoch 168/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4485 - val_loss: 0.3507\n",
      "Epoch 169/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4030 - val_loss: 0.3466\n",
      "Epoch 170/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4469 - val_loss: 0.3631\n",
      "Epoch 171/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4196 - val_loss: 0.3694\n",
      "Epoch 172/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4331 - val_loss: 0.3355\n",
      "Epoch 173/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4616 - val_loss: 0.3889\n",
      "Epoch 174/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4069 - val_loss: 0.4073\n",
      "Epoch 175/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4009 - val_loss: 0.3885\n",
      "Epoch 176/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4067 - val_loss: 0.3926\n",
      "Epoch 177/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4658 - val_loss: 0.3676\n",
      "Epoch 178/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4331 - val_loss: 0.3686\n",
      "Epoch 179/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3970 - val_loss: 0.3670\n",
      "Epoch 180/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4238 - val_loss: 0.3535\n",
      "Epoch 181/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3706 - val_loss: 0.3661\n",
      "Epoch 182/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4621 - val_loss: 0.3612\n",
      "Epoch 183/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4732 - val_loss: 0.3773\n",
      "Epoch 184/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4933 - val_loss: 0.3540\n",
      "Epoch 185/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4045 - val_loss: 0.3450\n",
      "Epoch 186/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4493 - val_loss: 0.3460\n",
      "Epoch 187/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4420 - val_loss: 0.3486\n",
      "Epoch 188/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4251 - val_loss: 0.3729\n",
      "Epoch 189/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4267 - val_loss: 0.3968\n",
      "Epoch 190/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4528 - val_loss: 0.4047\n",
      "Epoch 191/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4592 - val_loss: 0.3700\n",
      "Epoch 192/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3995 - val_loss: 0.3979\n",
      "Epoch 193/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4366 - val_loss: 0.3946\n",
      "Epoch 194/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3803 - val_loss: 0.3577\n",
      "Epoch 195/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4186 - val_loss: 0.3488\n",
      "Epoch 196/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3868 - val_loss: 0.3658\n",
      "Epoch 197/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3977 - val_loss: 0.3808\n",
      "Epoch 198/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3750 - val_loss: 0.3926\n",
      "Epoch 199/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4216 - val_loss: 0.3812\n",
      "Epoch 200/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4026 - val_loss: 0.3680\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_8 (Bidirection (None, 24, 12)            684       \n",
      "_________________________________________________________________\n",
      "layer_normalization_6 (Layer (None, 24, 12)            24        \n",
      "_________________________________________________________________\n",
      "bidirectional_9 (Bidirection (None, 12)                684       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 1,405\n",
      "Trainable params: 1,405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Saved\n",
      "Predict time:  0.4647552967071533\n",
      "RMSE:  7.605350859498394\n",
      "RMSE2:  6.235101631657592\n",
      "MAE:  6.503366168737413\n",
      "MAE2:  6.503366168737413\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'mse score')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEJCAYAAACZjSCSAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAABSEklEQVR4nO3dd3hUZfbA8e+ZdFJISCcJhBJ6JyBIU0EFG3ZR7G3tdV11dV1d97e79nXXgh1lVcSOooIiUqUECL0HEkJJD4GEhJT398c7CQkESCCTCeR8nifPTO7cuffkJrln3i7GGJRSSjVfDncHoJRSyr00ESilVDOniUAppZo5TQRKKdXMaSJQSqlmThOBUko1cy5LBCISJyKzRGStiKwRkftr2UdE5D8isllEVopIP1fFo5RSqnaeLjx2GfCwMWaZiAQCS0XkZ2PM2mr7jAESnF+nAW86H5VSSjUSlyUCY8wuYJfz+V4RWQfEANUTwVjgI2NHtS0UkWARiXa+t1ZhYWEmPj7eVWErpdQpaenSpdnGmPDaXnNliaCKiMQDfYFFh7wUA2yv9n26c9sRE0F8fDxJSUkNHaJSSp3SRCT1SK+5vLFYRAKAL4EHjDEFx3mM20UkSUSSsrKyGjZApZRq5lyaCETEC5sEPjbGfFXLLjuAuGrfxzq31WCMedsYk2iMSQwPr7Vko5RS6ji5steQAO8B64wxLx9ht6nA9c7eQ4OAPUdrH1BKKdXwXNlGMAS4DlglIsnObX8G2gAYYyYAPwDnAZuBIuAmF8ajlDqJlZaWkp6eTnFxsbtDadJ8fX2JjY3Fy8urzu9xZa+heYAcYx8D3O2qGJRSp4709HQCAwOJj4/HVjioQxljyMnJIT09nXbt2tX5fTqyWCl1UiguLiY0NFSTwFGICKGhofUuNWkiUEqdNDQJHNvxXKNmkwg27N7L/01bS9GBMneHopRSTUqzSQTpeUW8M3crq3cc11AGpZQiICDA3SG4RLNJBH3iggFYnpbn3kCUUqqJaTaJIDTAhzatWpC8Pd/doSilTnLGGB555BF69OhBz549+eyzzwDYtWsXw4cPp0+fPvTo0YO5c+dSXl7OjTfeWLXvK6+84uboD9cocw01FX3bBLMoJdfdYSilTtAz361h7c6Grebt1jqIv17YvU77fvXVVyQnJ7NixQqys7MZMGAAw4cP55NPPuHcc8/liSeeoLy8nKKiIpKTk9mxYwerV68GID8/v0HjbgjNpkQAtnpod0Exu/bsd3coSqmT2Lx587j66qvx8PAgMjKSESNGsGTJEgYMGMAHH3zA008/zapVqwgMDKR9+/akpKRw77338tNPPxEUFOTu8A/TzEoEIQAkp+UT3dPPzdEopY5XXT+5N7bhw4czZ84cpk2bxo033shDDz3E9ddfz4oVK5g+fToTJkxgypQpvP/+++4OtYZmVSLoGh2It4dD2wmUUidk2LBhfPbZZ5SXl5OVlcWcOXMYOHAgqampREZGctttt3HrrbeybNkysrOzqaio4LLLLuPvf/87y5Ytc3f4h2lWJQIfTw96xASxeJu2Eyiljt8ll1zC77//Tu/evRERnn/+eaKiovjwww954YUX8PLyIiAggI8++ogdO3Zw0003UVFRAcA///lPN0d/OLHT/Zw8EhMTzYksTPPSjA288dsWlj91NkG+dZ+USSnlXuvWraNr167uDuOkUNu1EpGlxpjE2vZvVlVDAEM6hlFeYVi4JcfdoSilVJPQ7BJB3zbB+Hl5MH9ztrtDUUqpJqHZJQIfTw9Oa9+KeZoIlFIKaIaJAGBoxzC2ZBXqeAKllMK1S1W+LyKZIrL6CK+3FJHvRGSFiKwRkUZbney0dqEAJG3TeYeUUsqVJYKJwOijvH43sNYY0xs4A3hJRLxdGE+VzlGBeHs6WJme3xinU0qpJs1licAYMwc4Wod9AwQ6F7kPcO7bKIsFeHs66BYdxIr0PY1xOqWUatLc2UbwGtAV2AmsAu43xlTUtqOI3C4iSSKSlJWV1SAn7xMXzOodeyivOLnGUSilTg5HW7tg27Zt9OjRoxGjOTp3JoJzgWSgNdAHeE1Eap2NyRjztjEm0RiTGB4e3iAn7xXbkqID5WzO3Ncgx1NKqZOVO6eYuAn4l7FDmzeLyFagC7C4MU7e27lQzYrt+XSOCmyMUyqlGsqPj8HuVQ17zKieMOZfR3z5scceIy4ujrvvvhuAp59+Gk9PT2bNmkVeXh6lpaX8/e9/Z+zYsfU6bXFxMXfeeSdJSUl4enry8ssvc+aZZ7JmzRpuuukmDhw4QEVFBV9++SWtW7fmyiuvJD09nfLycv7yl79w1VVXndCPDe4tEaQBIwFEJBLoDKQ01snbhfoT6OPJCm0wVkrVwVVXXcWUKVOqvp8yZQo33HADX3/9NcuWLWPWrFk8/PDD1Hfantdffx0RYdWqVXz66afccMMNFBcXM2HCBO6//36Sk5NJSkoiNjaWn376idatW7NixQpWr17N6NFH649Tdy4rEYjIp9jeQGEikg78FfACMMZMAJ4FJorIKkCAR40xjTbKy+EQ+rQJZmGKTjWh1EnnKJ/cXaVv375kZmayc+dOsrKyCAkJISoqigcffJA5c+bgcDjYsWMHGRkZREVF1fm48+bN49577wWgS5cutG3blo0bNzJ48GD+7//+j/T0dC699FISEhLo2bMnDz/8MI8++igXXHABw4YNa5CfzWWJwBhz9TFe3wmc46rz18XILhE8/d1atmTto0P4qbkotVKq4VxxxRV88cUX7N69m6uuuoqPP/6YrKwsli5dipeXF/Hx8RQXFzfIua655hpOO+00pk2bxnnnncdbb73FWWedxbJly/jhhx948sknGTlyJE899dQJn6tZjiyudE53m7Wnr9nt5kiUUieDq666ismTJ/PFF19wxRVXsGfPHiIiIvDy8mLWrFmkpqbW+5jDhg3j448/BmDjxo2kpaXRuXNnUlJSaN++Pffddx9jx45l5cqV7Ny5kxYtWnDttdfyyCOPNNjaBs1qPYJDtQ72o1dsS6avyeCuMzq6OxylVBPXvXt39u7dS0xMDNHR0YwfP54LL7yQnj17kpiYSJcuXep9zLvuuos777yTnj174unpycSJE/Hx8WHKlClMmjQJLy8voqKi+POf/8ySJUt45JFHcDgceHl58eabbzbIz9Xs1iM41OuzNvPC9A0sfHwkUS19G+y4SqmGpesR1J2uR1BPI7tGADB3U8MMVFNKqZNNs64aAugUEUiQryfL0vK4IjHO3eEopU4hq1at4rrrrquxzcfHh0WLFrkpoto1+0TgcAj92oawNFVnIlWqqTPGYKcnOzn07NmT5OTkRj3n8VT3N/uqIYD+bULYmLGPPftL3R2KUuoIfH19ycnJOa4bXXNhjCEnJwdf3/q1dzb7EgFA/7YhACxPy+OMzhFujkYpVZvY2FjS09NpqIknT1W+vr7ExsbW6z2aCLDzDnk4hGWpmgiUaqq8vLxo166du8M4JWnVEODv40nX6ECW6IplSqlmSBOB0/CEcBZvyyWzoGGGhyul1MlCE4HT5f1jKa8wfL18h7tDUUqpRqWJwKl9eAD924bwxdJ07ZWglGpWNBFUc3n/WDZl7mOlrmWslGpGNBFUc16PaBwCM9dnujsUpZRqNJoIqmnZwouescHM39xo6+MopZTbuSwRiMj7IpIpIquPss8ZIpIsImtEZLarYqmPoR1DSd6ez95iHWWslGoeXFkimAgccUFNEQkG3gAuMsZ0B65wYSx1NqRjGOUVhsVbc90dilJKNQqXJQJjzBzgaHfTa4CvjDFpzv2bRMV8vzYh+Ho5mKfVQ0qpZsKdbQSdgBAR+U1ElorI9UfaUURuF5EkEUly9Twjvl4eDIhvxeyNWdqNVCnVLLgzEXgC/YHzgXOBv4hIp9p2NMa8bYxJNMYkhoeHuzyw83tGk5JVyLK0fJefSyml3M2diSAdmG6MKTTGZANzgN5ujKfKhb1b4+/twaeL09wdilJKuZw7E8G3wFAR8RSRFsBpwDo3xlPF38eTsX1j+H7lTl2jQCl1ynNl99FPgd+BziKSLiK3iMgdInIHgDFmHfATsBJYDLxrjDliV9PGds3ANhSXVjA1WeceUkqd2ly2HoEx5uo67PMC8IKrYjgRPWJa0jkykG+Sd3Ld4Hh3h6OUUi6jI4uPYmzf1ixNzWN7bpG7Q1FKKZfRRHAUF/VuDcDUFTvdHIlSSrmOJoKjiA1pwYD4EL7VdgKl1ClME8ExjO4RzcaMfezI3+/uUJRSyiWaTyLI3QoL34TS+t3QhyWEATBvk2tHNCullLs0n0SwexX89Bhkrq3X2xIiAogM8mHOJp17SCl1amo+iSCqh33cXb+hCiLCkI5hLNicTUWFzj2klDr1NJ9EEBwP3gG2ZFBPwxLCyCsqZc3OgoaPSyml3Kz5JAKHAyK7Q0b9By8P6WjbCX5P0eohpdSpp/kkAoDIHpCxBuo5vXREoC9hAd5sztznosCUUsp9mlciiOoBJQWQn1rvt7YPDyAlq9AFQSmllHs1s0TQyz7Ws8EYoEO4PynZmgiUUqee5pUIIroCclztBO3DAsgtPEB+0YGGj0sppdyoeSUCb38I7XBcPYfah/sDsEWrh5RSp5jmlQjANhgfVyIIAGBLljYYK6VOLc0vEUT1sI3FxfUbExAX4oeXh2iDsVLqlOPKFcreF5FMETlqhbyIDBCRMhG53FWx1BDZ0z5mrKnX2zw9HLQN9SdFSwRKqVOMK0sEE4HRR9tBRDyA54AZLoyjpqjKRHA8Dcbac0gpdepxWSIwxswBco+x273Al0Cmq+I4TFBr8AuB3Svr/daEyAC2ZRdqzyGl1CnFbW0EIhIDXAK8WYd9bxeRJBFJyso6wemgRZwNxvUvEVzQqzVlFYbPk9JPLAallGpC3NlY/G/gUWNMxbF2NMa8bYxJNMYkhoeHn/iZo3pC5jqoKK/X27pGBzEwvhWTFqaSva+E1Tv2nHgsSinlZu5MBInAZBHZBlwOvCEiFzfKmSN7QNl+yNlS77def3pb0nKLOP2fvzL29fnk7CtxQYBKKdV43JYIjDHtjDHxxph44AvgLmPMN41y8soG4+NoJzi3exQD41vRK7Yl5RWGlVoqUEqd5FzZffRT4Hegs4iki8gtInKHiNzhqnPWWXgX8PCGXSvq/VYvDwdT7hjMBzcNQARWpWsiUEqd3DxddWBjzNX12PdGV8VRK09vO+/QcZQIKgX6etE+zJ+VmgiUUie55jeyuFJ0b1siqOfaBNX1ig1m1Y78hotJKaXcoPkmgqhesD8P9hx/V9CeMS3JKCgho6C4AQNTSqnG1XwTQXQf+3gc7QSVesW2BLSdQCl1cqtTIhCRtiIyyvncT0QCXRtWI4jsDuI4oXaCbq2DcAgs2JLTgIEppVTjOmYiEJHbsN0733JuigW+cWFMjcO7BYR1OqESQQtvTy7o1ZoPFmzll7UZDRicUko1nrqUCO4GhgAFAMaYTUCEK4NqNK37QnrSCTUYP3dZL3q0bsl9k5eza8/+BgxOKaUaR10SQYkxpmqWNRHxBI7/ztmUxA2EomzI23rch/Dz9uCN8f0oLa/gtV83N2BwSinVOOqSCGaLyJ8BPxE5G/gc+M61YTWS2IH2cfuSEzpMXKsWXDUgjilJ29meW9QAgSmlVOOpSyJ4FMgCVgF/AH4AnnRlUI0moit4B0L64hM+1D1nJiAivDm7/vMXKaWUOx01ETgXjllnjHnHGHOFMeZy5/NTo2rI4QEx/WD7iSeCqJa+XNAzmu9X7KSkrH6zmiqllDsdNREYY8qBDSLSppHiaXxxA+1qZSUnvgTlhX1aU1BcxpyN2Q0QmFJKNY66VA2FAGtEZKaITK38cnVgjSZ2IJgK2LH0hA81tGMYrfy9+TZ5RwMEppRSjaMuk879xeVRuFPcQDuwLHUBtB9xQofy8nBwfs9oPl+6nX0lZQT4uGxOP6WUajDHLBEYY2YD64FA59c657ZTg1+wXZ8gdX6DHG5Ut0iKSytYuT2/QY6nlFKuVpeRxVcCi4ErgCuBRSJyuasDa1Txw2yDcemJTx7XvXUQAGt3FZzwsZRSqjHUpY3gCWCAMeYGY8z1wEDqUF0kIu+LSKaI1LpKvIiMF5GVIrJKRBaISO/6hd6A4odCeQnsSDrhQ4UF+BAR6KOJQCl10qhLInAYYzKrfZ9Tx/dNBEYf5fWtwAhjTE/gWeDtOhzTNdoMBgS2NUz1UNfoINbt2sv23CKue28RWXt1XWOlVNNVlxv6TyIyXURuFJEbgWnAj8d6kzFmDpB7lNcXGGPynN8uxE5m5x6V7QTb5jbI4bq1DmJz5l4++n0bczdlM3tjVoMcVymlXKEujcWPYGce7eX8etsY86cGjuMW6pBcXKr9GZC2EEr2nvChukYHUVpu+N/CNACSt+cd4x1KKeU+dWksbgf8YIx5yBjzELaEEN9QAYjImdhE8OhR9rldRJJEJCkry0WfrhPOgYpSSDnxDlHdom2D8f7Scrw8hGTtQaSUasLqUjX0OVBR7fty57YTJiK9gHeBscaYI67uYox52xiTaIxJDA8Pb4hTH67NIDvv0OafT/hQ7cL88fVy4O3hYNyANqzftZfiUp12QinVNNUlEXhWn4ba+dz7RE/snLbiK+A6Y8zGEz3eCfPwgg5nwKafT2h9AgAPhzCofSgX9I5mWEIYZRWG1Tt0OUulVNNUl6GvWSJykTFmKoCIjAWOOZmOiHwKnAGEiUg68FfAC8AYMwF4CggF3hARgDJjTOLx/BANJuEcWPcdZK61S1megA9uHIAxkF1oewwlb88nMb5VQ0SplFINqi6J4A7gYxF5DRBgO3D9sd5kjLn6GK/fCtxalyAbTcez7eOmGSecCEQEEYgI9CUm2I+lqXncOqwBYlRKqQZWl15DW4wxg4BuQFdjzOnGmFNzKa6gaNuNdNOJtxNUd3a3SGaszWCdDjJTSjVBdek1dL+IBAGFwL9FZJmInOP60Nyk49m2G2lxw9XpPzAqgSBfT/7yzWpOlaUclFKnjro0Ft9sjCkAzsHW6V8H/MulUblTwjlgymHLrAY7ZHALbx4b04Wk1Dxu+GAJqTmFDXZspZQ6UXVJBOJ8PA/4yBizptq2U0/sAPBt2eDVQ1cmxvH0hd1YlprHJW8sYFu2JgOlVNNQl0SwVERmYBPBdBEJpOa4glOLhyd0GAmbpkN5WYMdVkS4cUg7pt4zBGMMN3ywmPQ8XeheKeV+dUkEtwCPYWcgLcKOIbjJpVG5W/dLoDALtjb8sgvtwwN494YBZO8t4eyX5/DW7C2Ulp+6eVUp1fTVpddQhTFmmTEm3/l9jjFmpcsjc6eEc8CnJaxqkAHUh+nfNoQZD41gSMcw/vnjei787zx25u93ybmUUupY6lIiaH68fKHbhXZwWalrbtAxwX68e0Mib13Xn40Ze5m8OM0l51FKqWPRRHAkPa+EA/tgg2snRT23exQ9YlqyaOsRZ+xWSimXqlMiEJGhInKT83m4c0bSU1v8UAiIcln1UHUD41uxfHs+JWU1J6Z7fdZmVqbnu/z8SqnmrS4Dyv6KnSL6cecmL+B/rgyqSXB4QM/LbTfSItd+Wh/YrhUHyipYmX5wENvW7EJemL6BCbO3uPTcSilVlxLBJcBF2JHFGGN2AoGuDKrJ6Hm5XaNg7bcuPc0A52R0i6tVD81clwHAvE3ZlGmvIqWUC9UlERwwdl4EAyAi/q4NqQmJ7gOhCbDqC5eeJsTfm86RgczdlEXRATt24Zd1GTgECorLWJGuU1grpVynLolgioi8BQSLyG3AL8A7rg2riRCBXldC6jzYk+7SU53eMZSFKbn0+Ot0nvluDUu25XH1wDY4BObomsdKKReqyziCF4EvgC+BzsBTxpj/ujqwJqPHZfbRxaWCR0d34d3rExnbJ4YP5m+jvMJwef9YesUGM2eTJgKllOvUpbHYH/jVuYj9O4CfiHi5PLKmIrQDxCS6PBH4enkwqlskL1/ZmyfP78p5PaPoHRvMWV0iWJ6Wz9kvz64qGUycv1VLCUqpBlOXqqE5gI+IxAA/YWcfnXisN4nI+yKSKSKrj/C6iMh/RGSziKwUkX71CbxR9bwCMlZB5jqXn0pEuHVYe94Y3x+HQ/jDiPY8dUE3SssrePjzFczemMXT363lxRkbXB6LUqp5qNPso845hi4F3jTGXAHUZfmuicDoo7w+Bkhwft0OvFmHY7pHj0tBPGDF5EY/tY+nBzcPbccrV/Uha28Jt32YBMDK9D1kFBQ3ejxKqVNPnRKBiAwGxgPTnNs8jvUmY8wc4Ggd8Mdip7U2xpiF2Mbo6DrE0/gCIiDhbJsIGnBG0vro2yaES/vGcKC8gjtGdABg1vpMAErLK1iZnl+16I0ufqOUqo+6JIIHsIPJvjbGrBGR9kBDrNoSg13/uFK6c1vT1Pda2Lcbtsx0WwjPjO3OW9f150/ndiYm2I+Z6zOZvTGL0f+ew0WvzefH1btZlb6HPn/7mVd+3kh5hSYEpdSxHXPxemPMbGB2te9TgPtcGdShROR2bPURbdq0acxTH5RwLrQIg+X/g07nuiWEQF8vzu0eBcDIrhFMWpjKz2szaBvagphgP177dTOhAd4UlpTx6sxNpOUW8cpVfdwSq1Lq5FGXXkOJIvKVc63ilZVfDXDuHUBcte9jndsOY4x52xiTaIxJDA8Pb4BTHwdPb+h1lZ2ErjDHPTFUc2m/WFq39OPR0V2Y8eBwHhiVwNpdBczdlM0j53bmtmHt+Hr5DlKy9rk7VKVUE1eXqqGPsQ2/lwEXVvs6UVOB6529hwYBe4wxuxrguK7T91o75cSqKe6OhD5xwcx/7CzuPKMDPp4eXNw3hphgP8IDfbh+cDy3DW+Pl4cwaWGqu0NVSjVxdUkEWcaYqcaYrcaY1MqvY71JRD4Ffgc6i0i6iNwiIneIyB3OXX4AUoDN2PEJdx3vD9FoIrtB636wbBI0sQZZLw8HH9w0gEm3DMTP24OIQF/O6xnNF0npFJYcu4F7UUoON09covMaKdUMHbONAPiriLwLzARKKjcaY7462puMMVcf43UD3F2XIJuUvtfCtIdgVzK07uvuaGroFFlzLsDrBrXl2+SdTF+zm0v7xQKwNDWP5Wl53DqsfY19p63axa/rM9ldUExsSItGi1kp5X51KRHcBPTBjgmorBa6wIUxNW09LgNPP1jynrsjOaZ+bUIIaeHF/M0H2zRe+3UTf5+2jqy9JTX2XberAIDMQ7YrpU59dSkRDDDGdHZ5JCcLv2DoOx6WfQRnPgFBTXPoA4DDIQxqH8rClByMMZSWm6qV0OZszOKy/raUYIxh/a69AGTqIDWlmp26lAgWiEg3l0dyMhl8D1SUwcI33B3JMQ3uEMqO/P1sz93PsrQ8ig7YVdBmV5urKD1vP3ud7QhaIlCq+alLIhgEJIvIBmfX0VUN1H305NWqHXS/FJI+gP357o7mqAa3DwVgwZZs5m3KxsMhnNMtkjmbsqoGnK3ZWVC1v05boVTzU5dEMBo7H9A5HGwfaIjuoye3IffDgb2Q1LTbCjpGBBAW4MOsDXYUcp+4YC7o3Zr8olK+Td5Bzr4S1u0qwCEQ0sKLjAItESjV3NRlZLF2RK9NdC/oMBIWToBBd4GXn7sjqpWIMDwhjK+W27F6949MYHhCGN6eDh6asgJfLwexIS2ID/Mn0MdTq4aUaobqUiJQRzL0QSjMhORP3B3JUT09tjsTru3Pk+d35YbT4wlu4c3PDw7ng5sGEBvSgs2Z++gaHUREkK82FivVDNWl15A6kvihENMfFvwX+t8IjmNOyuoWQb5ejO4RVWNb21B/2ob60z06iLs+Xsa53aNYlJJD0rZclmzL5aEpyfh6enD78PZckRh3hCMrpU4FWiI4ESIw5AHI2wprv3V3NMclIsiXL+48nYt6tyYyyJe8olK+WraDzIISCopLmbxk+xHfu25XgU55rdQpQBPBiepyAYQmwLxXmty0E/UVGeQDwE+rdzEgvhVjekSzdmdBrdNZL03NY8yrc2t0Q1VKnZw0EZwohwOGPgC7V560pYJKEUG+AOQVlXJau1b0jGnJ/tJyUrL2kbOvhJx9BxuSF2zOBmBV+h63xKqUajiaCBpCr3EQ0R1+/guUnryNrRGBPlXPB7ZrRc/YlgCs2rGHcW8vZPA/f+XPX6+iuLScxdvsCOX1GXvdEqtSquFoImgIHp4w+p+Qn2Ybjk9Skc4Sgbeng95xwbQP88fXy8GkhalsytxHr9iWfLIojc+WbGdZah4A63cVHO2QSqmTgCaChtJ+BHQbC3NegKwN7o7muLRq4Y2nQ+gTF4yvlweeHg66RQexPC0fXy87zXWPmCBe/nkjhQfKaRfmz9bsQopLy8koKNaGY6VOUpoIGtJ5L4J3C/j2bqgod3c09eZwCFcOiOPaQW2rtvWMsdVDo7tHEejrxXWD2rJnfykA409rQ4WBL5elM/ifM3nup4MJ8Ovl6Tz5zSpNDkqdBDQRNKSACBj9HKQvgeSP3R3NcfnHJT25qHfrqu/7tAkGqJqp9KLeMbT08yI2xI8zOkcA8NyP66kwMGH2Fr5N3sEvazP44+cr+d/CNH5cvZvyCkN+0YFG/1mUUnUjrvzEJiKjgVcBD+BdY8y/Dnm9DfAhEOzc5zFjzA9HO2ZiYqJJSkpyTcANwRh47xzIT4V7l4JP4LHf04SVllewKCWXIR1DERHAdi8FGNU1ku5/nU5JWQXXnNaGFdvzqyaw6xIVSHmFoazCENzCi5SsQn5//CxaeOsYRqXcQUSWGmMSa3vNZf+VIuIBvA6cDaQDS0RkqjFmbbXdngSmGGPedE51/QMQ76qYGoUInPsPeG8UzHkRzn7G3RGdEC8PB0MTwmpsG93j4BoMCZEBrNlZwO3D2tMqwJvfNmSRWVDMRX1ak5yWz+2TluIQqDCwKCWXM7tENPaPoJQ6Bld+PBsIbDbGpACIyGRgLFA9ERggyPm8JbDThfE0nrgB0OdamP8qtBsOHUe6OyKXGTegDdvziogP8weoUa10drdI/ja2O71igxn39u/M3piliUCpJsiViSAGqD4/QTpw2iH7PA3MEJF7AX9gVG0HEpHbgdsB2rRp0+CBusR5z8POZfDlrfCHORB8as7XU71h+VAiwvWD4wE4rV0oc+o4CrmsvIJZG7I4s3M4nh7ajKWUq7n7v+xqYKIxJhY4D5gkIofFZIx52xiTaIxJDA8Pb/Qgj4u3P1w5CcpL4fMboax5N5aO6BROSnYh23OLqrYVl5Yz3zlCubpf1mVy20dJ/N8P6xozRKWaLVcmgh1A9Y/Bsc5t1d0CTAEwxvwO+AJhnCrCOsLFb8COJJjxhLujcavhnWwCn7Uhs2rbW7NTGP/uIrZk7aux77I0O1jtg/nb+Gb5oX8ySqmG5spEsARIEJF2IuINjAOmHrJPGjASQES6YhPBqTWLWbeL7BrHi9+GlZ+7Oxq36RDuT4+YICb8toWiA2WUVximJNmaw+S0/Br7Jqfl0zOmpd1/9hY3RKtU8+KyRGCMKQPuAaYD67C9g9aIyN9E5CLnbg8Dt4nICuBT4EZzKo5AGvU0tDkdvrsPsje5Oxq3EBH+emF3du4pZsJvW5i/OZsd+fsBWJGeX7VfWXkFK3fkkxgfwohO4WzK3Edx6ck3OE+pk4lLO3U7xwT8cMi2p6o9XwsMcWUMTYKHF1zxAbx+Gnz3ANz4ve1m2swMiG/F2D6teW3WZsIDfQhu4UWH8ABWbM+v2mf97r0Ul1bQJy4Ybw8H5RWGjRl76RUb7La4lTrVubuxuPkIjIKz/wap807aUccN4dmLe3DbsPYUHSjn2tPaMiC+FWt3FVBSZj/1JzuTQr82IfRwTm+xeodObKeUK+kwz8bU9zpY8Sn8+BjEDoTwTu6OqNEF+Xrx+HldeWxMFwCmr8mgtNwwY00Ga3YWsDAlh1B/b2JD/Jz7e7Jmp655oJQraYmgMTkccNm74OkDk6+G/XnujshtRAQRO9MpwH2Tl/PWnC0kb8/n9I5hVa93b92S1Tu1RKCUK2mJoLG1jIWrJsGHF9mv674G/1Onx2x9RbX0JTbEj+LSCibdMpCIQB8Cfb2qXu/eOohJC1MpK6/QwWVKuYj+Z7lD29Ph6smQvRE+vLBZlwwAPrl1ED89MIyu0UGEBvjg7Xnwz7JHTEtKyioY/epcHvtyJQD/nbmJc16ZzW/VxiQopY6fJgJ3SRgF10yBnM0weTyU7nd3RG7TJrQFYQE+tb42uEMo7cP9KSkrZ0rSdnL2lfDJ4jQ2Ze7jxg+WMG3lrqp9s/aWsDS1eSdVpY6HJgJ3aj8CLn4TUufD22dAehOeXttNIoN8+fXhM3jjmv5UGPjPzE3s2lPM3y/uQVwrPyYvSQNg8dZcxrw6l8snLGDDbl1HWan60ETgbj0vh/FfQsle+GAMbJzh7oiapB4xQbRu6ctHC1NxCIzpEc0FvVqzYEsOS1PzuPa9RQT5ehLg7clLM07OpUKVchdNBE1Bwii4Yx5EdIXPxkPKb+6OqMkREc7pHoUxkNi2Fa38vbmgVzTlFYabJy7ByyFMvn0Qtw1vz4y1GSxPO1hFVFFhakx2p5SqSRNBU9GiFVz/LbTqYKeu3pvh7oianHO6RwJ2nQOAbtFBtA/zZ8/+Uu4dmUBEkC83D21HeKAP93yynIyCYgCenbaW4S/MqpEclFIHaSJoSvxC4IqJULIPvrzFPqoqg9uH8to1fblusF0DQUS4aWg7Bsa34qYh8QAE+Hjy/g0DyC86wDXvLOSt2Vv4YP42jIFXZx6c56m4tLxqNLNSzZ1L1yx2hSa/ZnFDWPEZfHMHRHSDqz+F4JNkMZ4mZMGWbP44ZQU79xTTPsyfC3pF859fN/Pm+H7s2V/Kv35az6B2oUy4rr+7Q1WqURxtzWJNBE3V5l/gi5vBqwVc9w1EdHF3RCedA2UV/Lw2g+6tgwgL9GHYc7+SV1QK2KkrCg+Us+jPI4/YdVWpU4kmgpNVxhqYdAmUFcOY56HXVc1y1tKGkpK1jy1ZhbTw9iA0wJvR/57LMxd154bT490dmlIud7REoG0ETVlkd7hlBoR3ga//AFOuh2KdgO14tQ8P4OxukQzpGEaXqCC6RAXyTbKugKaUJoKmLiQebvrRTmG9fhq8fSbkprg7qlPC2D4xLE/L19lNVbPn0kQgIqNFZIOIbBaRx46wz5UislZE1ojIJ66M56Tl8IAh98ON0+y8RO+Phl0r3B3VSe+KxFgiAn24439LySs84O5wlHIblyUCEfEAXgfGAN2Aq0Wk2yH7JACPA0OMMd2BB1wVzymh7WC4+SdweMK7o2DBf6G8zN1RnbTCAnx467r+ZBSUcMVbvzNjzW5OtjYzpRqCK0sEA4HNxpgUY8wBYDIw9pB9bgNeN8bkARhjdDrJYwnvDH+YAx3PhhlPwhunwYYf3R3VSatvmxDeuT6R8grD7ZOWcvHr87nv0+V0e+on5m7KqrFvRYXhtw2Z7C0urdc59pWU8fLPG+v9PqUaiysTQQywvdr36c5t1XUCOonIfBFZKCKjazuQiNwuIkkikpSVlVXbLs2LfxiM+xjGfQLiAZ+Osw3JyZ/AnnR3R3fSGdEpnJ8fHM7zl/cie98BZq3PxCHCJ4vSqvZJyyni0jcXcOMHS3jl501HOdrhZq7L4D8zN/HSjI0NHbpSDcLdjcWeQAJwBnA18I6IBB+6kzHmbWNMojEmMTw8vHEjbKpEoMv5do6iM5+AjdPhmzvhzdMhdYG7ozvpeHo4uDIxjrl/OpPlT53N5f1jmbk+kwLnp/h//7KRjRl76RwZyA+rdlFRUfcqpNU7bGP0R79vY90uXW1NNT2uTAQ7gLhq38c6t1WXDkw1xpQaY7YCG7GJQdWVpzeM+BM8nm6rjPzD4aOLYdFbUFHh7uhOOg6H4OnhYGyf1hwoq2D66t0ALN6Wy4hO4dx1Zgd2FxSztB7zFq3ZWUCHcH9a+nlx5Vu/c/cny8itR+P050nbScnS6UaU67gyESwBEkSknYh4A+OAqYfs8w22NICIhGGrirRv5PHw8ILo3nDzDGg3DH78ky0d/PacTmB3HPrEBdOmVQu+Td7Jrj37Sc/bT2J8K0Z2jcTH01FjQZyjMcawesceBrYLZeJNAxnZJYJpK3cxa33dmsN25u/nkS9W8t68rSfy4yh1VC5LBMaYMuAeYDqwDphijFkjIn8TkYucu00HckRkLTALeMQYk+OqmJoF/1AY/wVc+g74toTf/gn/6QO/PNPsl8SsDxHhkr4xzN+SzdfLbUF2QHwIAT6enNk5gqkrdrJ2p63mMcZw+0dJfLo47bDjpOftp6C4jB4xQfSOC+bFK3rj7eFgY8bBxXNmbcjkwwXbAMjcW8zXy9OZvTGLsvIKZjoTxuZMLREo13Hp4vXGmB+AHw7Z9lS15wZ4yPmlGooI9LrSfuVsgVn/gHkvQ9J7kHgzDLgVWsa6O8omb9zAOF6btZlXf9lEC28PukUHAXDPWR25aeISLn59Pv+5ug8BPl7MWJvB+t17GTcgDqk2DUjlYLXurVsCti2iQ0QA652rqKVk7ePuj5dRVm64akAcz/+0gS+W2gb/6wa1ZXueXUdhi7NqaEf+fnbv2U90Sz9aB/s1zoVQpzx3NxYrVwvtAJe/ZxuV2w2H+a/CawNg7bfujqzJi27px8guEZSUVdCvTQieHvbfpUdMS366fxhdowN5/KtV/PdX24soLbfosDWT1+wswMMhdIkKrNrWJSqQjRl7KS2v4L7JyykuLedAeQVJ2/KYuymLkV0iuDIxlk8Wp7Fgcw4BPp5k7ztA9r4Sxr42j8ve/J0RL8wieXt+o10LdWrTRNBcRPWEq/4H9y23cxhNuR6+vhO2/GoblnetdHeETdK1g+zaB4nxITW2hwb48PzlvdlbXMairbncPKQdvl4Ovlp+sD+EMYYl23LpGB6Ar5dH1fZOkYHs2lPMD6t2sXpHAX+/uCceDmHSwm1kFJQwqlskj47ugp+XBwfKKxh/mp2G/PsVO8ned4C7zuhAcAtvnp66pl69l04m//hhHW/8ttndYTQbmgiam5B4O1XF6ffBmq/s7KY//gneHgHTn4AynWqhuqEdw3j24h5VCaG6zlGB3HVmR/y8PPjDiPac2z2KaSt3UVxajjGGZ75by8KUXC7oFX3I+wIAeO3XzQT4eHJZ/xh6xbZk+pqMqnOGBvjwyLmdSYgI4OqBNhFMWpgKwHWD2/Lo6C4kb8+vkXiOV0WFqeom21RMTd7JzHUnNr60uLSc8lM0UTY0TQTNkacPnPMs3JdsG5bvXQb9b4TfX4OPL4N9zWzQ3tpvYfVXtb7kcAjXDWp7xDULHhyVwMLHRxIZ5Mv409qyZ38pE2Zv4c3ZW5i4YBu3DG3HPWd1rPGeTpG2mmhT5j7O6hKBj6cHp3cIBSA+tAVxrVoAcMPp8fz80AjatGpBC28PtmQVEh/aguiWflzaN4au0UFMXGB7E63ZuYfvV+5kw+691Nfz0zcw4vlZHChrGt2Ni0vL2V1QTNbekuM+hjGGM174jQmztzRgZKcuTQTNWVA0JJxt2xEueAUungCpv8NLne1YhKQPmkdPo7kvwdyXj+utIkLLFl4ADGzXigt6RfPGb1t4acZGzu8ZzZPnd63ReAwQE+xHgI/tpzG6RxQAg9uHATCkY9hh53A4hI4RthQxqH1o1bbL+sWwekcBa3cWcP17i7nnk+Wc++85LN6aW/XeBZuz+X7lziPGvyN/P+/P30peUSmbMg8mkUc+X8FXy44+Sn1jxl7+8cM67v54Gbv27D/ifqXl9Usw6Xn2WJl7i4977qfMvSXsLihmxlrtOl0XmgjUQX2uto3KQ+6H/FT4/gF4tbdtYC4tdnd0dVN+HFUc+duhoGHWJXjy/G54OoSYYD/+eVnPw5IA2OTRKTIAH08HIzrZkfKJ8SGc3S2SqwbEHbY/QMfwmokA4IJerRGBez5dRk7hAV68ojctvD2qursaY/jz16t47MtVVeszb80u5KyXfuOn1XYcxKu/bKy6Ua9xdofNKzzA50vT+fD31CP+nGXlFdz0wRImzt/Gj6t38fac2of/vD5rM52f/JGRL/3GopTae4ZnFBTzys8bKXPGUdlTqri0gn0lxzep4rbsQgBWpeezZ3/TqvZqijQRqJoiusCov9rqott/g7jT4Oen4L/97eC0dd/BgUJ3R1m7xe/Ay92gKPfY+1Yq2Qf7c+1X6ZE/1dZVVEtfpt4zhC/uHEyQr9cR97ttWHseH9MFf2fJwNfLg3euT6RXbHCt+3drHYRDYHCHg4kgqqUvA+JbkZJVSL82wVzWL4ZRXSP5afUuSssrWJm+h205RewrKWNhSi7FpeXc9fEyUrIK+dt3a5m9MYsvlqZzw+B4/L09WOOcCqOyN9LK9PwjjoD+cfVuduTv5/Xx/biod2s+T0qv9aY9beUu4lq1YPee4iMuAvT18h28OnNT1Xm35xZVvXa81UOpOfYYFYYjJqCGtLe4lPmbs11+HlfRRKBqJwKt+8L4z+GG7yAwCn77B3x2LbzYCX79OzS1KZu3L4LCTNvWUVd7qs2LWHDkKpTDGAOf3wTrncNk3h9te2IdKKJjRCARgb5HffuYntHcOKRdnU937Wlt+O6ORCKDah73ot6tAbhjRAdEhAt7tyavyN6UvknegbeHgxbeHsxYs5u/T1vLul0F3HlGB3buKebmiUtoG+rPH8/tTLfWQVUlgmXO6TOMgbmbsrjv0+X884d1GGNYmZ7Pr+szeGduCu3C/BnZJYIbh7RjX0kZXy6tWZWUtbeEtbsKuDIxjn5tQ0jeXvsCQBud7RpLttnzpuXUPRFscY7DWHFIV9ptOYV4eQi+Xg4WbHF9Injsy1Vc+94icvYdf7uGO7l0QJk6RbQbDrfNtCWBncthybsw5wUozIIzn4SAJjIRYLZzVtCFE2DQXXaW1mPJPyQRhHao27lyU2yvKxFoMwjSfrfb92XCDd+DR8P+a/mu+Yzu0/8MD60D7xZV28cNiKN9mH9VSWF4pzACfT359y+bSM8r4swu4Xg4hK+X76DoQDm3DG3Ho6O7sCljL3M2ZvPfq/sS4ONJ99YtmZK0nfIKw7K0PLpGB7Frz37+8cM6MgrszW15Wj5LUnOr8v+zF/fA4RD6xAXTJy6Y12Zt5szOEbQJtfFVfkIenhBOcWk5b/y2hX0lZdzzyTIu7RdblcQ2ZlYmglzupANpuUV4eQil5Yaso9xY52zM4o7/LaXoQDmpuYVMvXsoDoetikvNKSIupAUxIX4u/6Q+e2MW01bZqrZtOYWEHqFjQVOmJQJVd97+ED8ULv8Ahj4ISyfCix3hnbMgY417YzMGcjZD+zOhbD8sfKNu79tTbVqIQ0sESyfC9w/W/r7KG//uVZCx2j7vcoHdnrGqXqHXycafoDjftt1USv4Uz7R5nN4xrKotwsfTg0fO7UxK1j6y9x3gsn6xnNMtiqID5XQI9+eRczsD8N+r+zHjweH0iLEjnru3DqLoQDkpWftYsX0PiW1DGJYQTkZBCQPiQ7h+cFsWb8vlqsQ4Pr71NJ69uAfjgtdXrYXx/OW9KC2v4Jp3F5JZYNuT5mzKopW/N91bB9EnLpjyCsP787by24YsPltir3t5hWFThh01nbQtl4oKQ1puUVVcRysRvPHbZkIDvHlsTBdW7yjgu2qN4ttyCmkb2oLhCeFsytznsik60vOKeOLrVbTy97bnzS46xjuaJk0Eqv5EYORf4Zaf7VrKeanw1gj48ELbA2dPOhTsatxuqHt3w4F9dmruTmNg2UdHHxOx6gs7dXf+drumA9RsMDYG5v8Hlk2qvQE61ZkIcjZD+hL7fPDd9jFj7Yn/PNUZczDx5Kcd3Pbjn+CXpw/b/frB8Sz7y9n8+vAIzu4WyahukZzTLZJXx/WtGtjm5+1BfJh/1Xsqb7yTl2xnX0kZfdsEc37PKPy8PPj7xT155qLuzP3Tmfzrsl4M6RjGdYPa4jX7/2z7EbZL7KSbTyNrbwnPTltHRYVh7qZshnYMw+EQescFA/Dmb7Y755Kteew/UE5abhElZRUMat+KguIyNmTsJT1vP71iWuLpEDKPkAj2HyhnWWo+o7tHcfuw9nSLDuKlGRsxxmCMITWniLah/lzSLwZvTwcfzD/ypH0Hyir4all6VWN1pdzCAwx97ld+P0LV0tbsQi59Y4HtMnxtfxwCqbnHTgTFpeUUl5Yfc7/GpFVD6viIQNxA+9VnvO1+uW0uzPyb/QK7pOaAW2HEo9CilWviKMy29fwlzq6PoR0hpB1smAbrv4Mel9X+vl+eAS9fO+I6OA7259sSwd4MW6IoOwC5zj7oOVtsI3pZCUweDwNusTdmL38oLbRJxT/CNqx7+kJmAyeC3BRbDQcHE0F+KpQUwI5l9hocUg3m6eGgvbOnUYCPJ29fn3jUU3SMCMDf26NqltN+bUKID/PnrC6ReHvaz4uV4xsAZwlsi21gLy0GL196xrbkjhEdeHXmJvIKD5C1t4Rzu9vusWEBPsSG+JGet5+4Vn5sz93Pwq05lJTam++1g9qyMCWXGWsy2FdSRptQf8IDfY5YIliamseB8gpOdyaaawe15c9fr2JLViHBLbzYV1JGfGgLwgJ8uKRPDF8uS+eP53QmxPnJvbqPF6XyzHdrCfT14uxukVXbF2/NJT1vPx/M31qjkb7Sp4vTyC8q5bt7h9I5KpDoln6k5hy5I0V5hWHC7C28NXsL/dqGMPGmgUf9nTQmLRGoE+cfBqP/AXfMtVNYjHoGznsR+l4Li9+GNwbBuu8Pf1/eNtsT6UR66/zyV9tQu2Op/T4sATqcBcFtYcn7tb+nZJ+tEsreCGmLILgNBMXYRPDtXfD2mbB80sH9K2/sq7+EzT/bBYByt0DvcQdfj+wODg+7lGhDV5NVX2ioMhHsrqx+MrB55vEfe9t8KN6Dl4eD7+4dyv9d0oNnx3anrbOevzIJHKayBGbK7XV0umNEB2KC/Zi3OZsHRiVwfrVR1ZWlgifP74aPp4O5G7PZ5JyF9czOEcSG+FUNAGvTqkVVIli3q6Bq8r5K87dk4+kQBsbbDxhDOtob9YIt2VU347bOEs/NQ9tRXFrBp87qqM+WpDFno02sFRWmaubXQ+eJWpGeD8Cv6zPJrqWtItVZ/dTZOY9UfFgLUnOKWLItl7Ne+o1PFqXVGNk8eUkaL0zfgJeHg4UpOYeVQNxJE4FqWK3aw9AHYOBtcOGrtguqfwR8Nh6+uPngUpplB2DKDbYn0tT7jtwDyRjbZXXm3+yNGGwCSVtkX0uZDWXFsPBN8PSDwNbgcNhP7anzar9JVrtxUZAOLdtAUGtbzbN1ru1KuuA/EN4VxAGZ6+y5fn8dgmKh2LnKWO9x4GNnJCWqh32M6H7iJYKKQ6oN0haCXyt7bSt7Oe1ebWPzawWbZtTcf38ebJwBKz+HA0epqijYBRPPh0VvA9A+PIDxp7XlusHxtY5/qCGn2jxAmeuqnvp5e/DWdf15dVwfHhjVqcZbxvZuzbCEMEZ2iWBgu1bM3pjJut0FxLXyw9/Hk7evS6R76yA8HULnyEDCA2wiuOeTZdz7yfIax1qwOZs+ccFV3W/btGpBTLAfCzbnVNXTx4faRNA5KpDeccH8sjaD4tJynvp2DY99uZLS8gp+25jJtpwivD0dVb2lKq1MzycswJuyCsM3tUzlkZpTRJtqpaS2of6k5hTy5dJ0UrIK+fPXq7h/8nKMMZSVVzBh9hZ6xwXzlwu6UVxawabMfezM398kphjXRKBcK7o33D4LzvgzrJ0Kr/SA98fA5GtgVzJ0Gg2rpsCPj9Y+PmH5/2yX1bkvwXcP2ATy/UN2jqSM1QdvjPsybLWQw/knPfAPEJpgB8WVHPKPlrXBPno6u2IGx9lEkL0ByksgbpDd3v1iaNXB3ti3zrHnO+NRO+DOPwKi+9iSAECkMxFEdrOxFNajy2LmOtj8i30+7xW7fkT196ctsD2TgtvULBGEdoRO58KWmTWTx5Tr4ZMr4Ktb4ZMrIeU3+PYe25ZT3dbZgKmZGOsqp9q6zZk1S0A9Yloyts+hy5PDOd2jmHTLaXh6ODi3exRbsgr5YdVuOkXYT9TdWgfx+R2DWfLEKNqE2hLBpsy9bMkqJCW7sGp8wc78/azasYfTq43CFhFO7xDK7yk5/Lh6Nz6eDmKqTdM9PCGM5O35/LIug5KyCnbuKebrZTv4z8zNRAX5Mm5AHCu251cNrquoMKzcvodzu0fRO7Zl1dTglYwxbM8tquohBdC2VQvyikr5eW0G53SL5IFRCXy/chcfL0rj6+U72J67n7vO6ECvWNseszI9nwc/S+aWD5fU//o3MJcmAhEZLSIbRGSziDx2lP0uExEjIkevyFQnJw8vewO9NwnOeNzWwafMgn43wNWTYeDtsPgteH2QrQ+vVLLPjleIHWBnTi0psPX+W2fbuvlpD9v9ul1sH8Oqzenj5QsX/dfeOOe+WDOerPXg8ILul9rvW8YdXJ/Bwweu/tS2ayTeDBFd7f4L34AWYdDzShj5FDy42i4TGtXTvq8yEUR0s4+ZdaweMga+ut2OSaiogC2zbMxT77Gv5aXaaxI/1JkInIkvY5U9d6dzbQmgMpEU77HVPYk3258/dT58NNZWdc1/tea5U2bbx8q2kPrI2WITaUS3GiWCw5QWw57DP01fM7ANb47vx5mdw7mk38GkISJV9fjhgT6Ulh8sKc7emEXm3mKufXcRvb13MrZnzXaRIR3D2LO/lF/WZfDQ2Z1qVGsNSwinwsCL0zfg7emgXZg/j321kuTt+TxxfldOaxdKSVlF1WJDW3MK2VtSRu+4YC7pG8P63XvZnLmXn9dm8PTUNeQUHqDwQPlhJQKAnMIDjOgczn1nJTAsIYwnv1nNI1+sJCEigLO7RhIf6k+gryc/r81k8bZcUnOKyNpbQll5Ra2NyOl5RbWWSBqSyxqLRcQDeB04G7s28RIRmWqMWXvIfoHA/cAiV8WimoiQeJsQznjU9sRxeNpG5/NegO6X2FLCx1fCrT+DXwjM/zfs2w1XTbI3PS9/+OnPUFEGPi3tALKAKDjzz7D2GwjrXPN8bQfbm/3id213V1/7SYysDbYtofNoWPGJrXLBHHxPi1b2mGBvdOu+s1UhIx6zCQbsxH0A3cbam2K489yVJYSMtXb8BdiEtvEn+7xlLET1OjgWIHU+7HZOAZ6z2T73j4ANP8CKybYeHiDhHFvNU5hpG7Tz06D/TbbLass4W2JKOMeWXEy5bSSPH2rXsM7daq/Vyil2skFvf5tktjoTQc7xJILNtrQU2b1mG0Z1FRUw+Wo79uThDQevGXaupDE9oxnT09mGcKCoxvgIgIhAu3//tiHk5u+BJe/yQNJwYgpWMcnxDPzwlU12xXsguhendwhFBAa3D+W2Ye1rHKtvm2D8vT3YllPE8E7hXNgrmke+WMlfL+zGhb1bV82VtCwtj95xwVUD1HrHBhPSwotnvl/LN8t38u0K+8m+ck6ottVLBNWeD08Ix+EQ/jOuL58sTiPAx5Nzu0dVjXPoFduSX9YdnAcpeXs+czdlMW9TNj8/NAIPx8GquTd+28Ini9Lw9fKompuqobmy19BAYLMxJgVARCYDY4FDK1CfBZ4DHnFhLKqp8Thk+oW2p8O4T+yn1w8vhF7jYM6L9jHO2bui40hYN9U27A68zXadbDfc3oTHfXpwv+qGOKfbXvqhfQ72E37rPtDlQrj+W1vtUuZssO5wVs33R3QBDHh423aHQ8UPtV+VAiKhRagtyaz41N4ot/wKe6utcezpB9d+Yd/3++u2FFJeAuu+tZ/uz3sRln1oE2HLWNsLKrSjrcIC2Gj77hPVy17HIffDD3+Ebc42Ee8AiHVei85j7GPrvjZZzvu3Tb6t2tvusqEJtpqnKLd+PbuyN9l2kYiutmqveM/BRLtyiu2a6xdsf3awpY9O59R+rCXvwU+Pw7Vf2vW2ncKdiWBMjyi6Jr/HkJwvMGVrOTumFPKDbAeB1/rbnU+7k4gx/+J/t5xG99ZBVTfcSl4eDgZ3COWXdZmM6BTOFYlxDE0II7qlrT6KbunHnQFzKV84j5ROT/Dj6t208PagY0QAHg5hULtQ3p6TwgFn1VHlhHw1SwT2ebsw/6oeViH+3tx9prOkWrIX1v0Cnc+jV2ww8zfn0C7Mn+25toF56oqd5BeVMndTFmd0jqg6buWAuL98u5rB7UOrJjlsSK6sGooBqg3bJN25rYqI9APijDHTXBiHOlm0PR2u+thWf8x4wt6gL/z3wde7XGAfu14Iva8B32D7HKDLebWPJG7dF+KH2br3/11uRx3nbYPwLrY9of0Z9sYYkwg9r7BVP9VVVvX0uhICIg49+uFE7EyuPS6xpZqNP9nEdcN3cPdim7ACImw7x6Zf7ICswXfb0s6yj+wxovvAoLttwtr8i21HEbFVQwC/v2FLU6372O/7XmsT0LSHbMNxu+G22qq6NoNsiWnO8zD7Ofj6D3Z74s32sXqpoKLcNuAvm0StykvtNQztePD6VPYK259nk9LqL+wI9C4XgHcgrK+l11jl/r8+axPhFzfbBmynxPhWjO4exRWhKQzJ+YJME8x4z1+Jyphtr9ltM2HM8/Z3tmgCpC5gSMcwgnNW2PENh3RAOLNLBCJwRmc7Er4yCQCQtog/lU3g1r1v8OjLb/Hz2gzGDWhT9cn8gt7RHCivoF2YP96ejqq1EmJDDiaCFt6edI4M5LyetXxqT5ltqz4/Gw9rvqJvtA/3e3zJn2JWkRjlUdUVFWBK0nZWpe/h2+QdbM8tIjWniMv7x5JbeICXft5Q+3U8QW4bRyAiDuBl4MY67Hs7cDtAmzZtXBuYcq9O59huqMmfwGl/AK9q/6ydx9gbS+ItEBgJj26zN8hjOfMJO0I4dwv89LPdFn5INZJvEFz27uHvDetkbzbdxtb9Z+g29sj7V5538tXw8eX2RjrkftszKG2B7QkU2R2ie9mbWWHmwU/SLZ0lguwNtjG8MvF5+cFl78Gn42xV0tBaRkOLwPkv2aqonlfYXlH7sqDjKJj+uC0VzHkBYvrZuv9lH9p2hcBoSBhV81h522z1U2hHW6qJ6mW73VYOrCsugNt+taWMtqfbhuoNP0DFK7aLbaWKclty2p9v4596H3x9O1z3LTgchAX4MGFcN3j9NEyrDszs9gZXLb0aSotgwG3gH2qrDPteC+mL4Zu77Oy53z9o21B6XXWwqg4YN6ANiW1b0cE5vqJK6X745k6kZRylBt4reQeP4Bj8SzsAtkfVmB7RvPLzRh4YlcD/FqayZFseUUG+NVaeA5h231Ach/5NFu+xDfj+4bbab9XnjGibxTleX8KGL+nVogtDip/Cz8uDS/vF8NmS7cxcl0lU+U4mhbzLrR59uWr4cwzvFM6g9q4Zj+PKRLADqD6nbqxzW6VAoAfwm7OrWhQwVUQuMsYkVT+QMeZtnL+RxMTEJjbTmWpwwW3gjFr6FvgGwbiPD35flyQAtt7/7oX2xjPtYUj+2H7qrgsRm5AaUmVCy9oA131lq1Bi+tlEENbpYF35kPts1VHbIfb7wGhbEvDyt43Z1bUbBjdMhQX/te0ttWk37GDVy4XOhuOyAzb5JL1vb+SbptvtncbYrr5TrrNJrddVtvSUusD2xMI5KaFPgB1hPvMZu+SpKbf7xvQ/eN4u59vque2LbGIAm/i+vM2O5+h/I/S83Cax7+6HpR8crIab8yLkpyI3fM/V7QZBx0m2FOFfbYCXtz9c9Bp8eIGtVqyc4mP9NFtNtv57GHQXHg6p6vNfw8af7IeEaz7Hy8sXr0mXQBGwMsnG1vZ0Wvl7k/Tk2QCs27WXJdvyDvYYMqbqb7FyXesaFr5ppwe5/ltbUlr4Jj4Za20ptMt5xMz8GwmSTsfOidx4ejyfLk7j3PACns57lpDCPTzptRazIpSEs/9W97/5epLjXfjhmAcW8QQ2AiOxCWAJcI0xptbuFCLyG/DHQ5PAoRITE01S0lF3UeroigtsUnGnyu6elZ+QV30BX95iqzkue8duM8Z+OardXH74E8Qm2qqqhvLvXnaksm9Lu4Tphh/gmil2JPWvz9rqq+J8W9W1P89WdY19HTqcWfM4OVtg5WcHS2yVigvs9OBB0XD9VNvg/sbptn3jnL9D5/PsJH3GwKSLbVuHp699vbjAlmAufevYP8cPj9gBjKEJ4BMIpsK2e2z5FW78AeKH1P6+Hx+1bUiPb7fnLN1vY3m1t+0OfP23NXafuS6DWz5M4vL+sbw4Osomn37Xw+n3HH7solx7nHbD7YeYXSvhLWcyvvx9aDsU83IXXi+/hC7j/smobpGkpKYS/9UFlOwv5OK9j/CXqIUMzfsGhv0RRv7l2NfhCERkqTGm1p6ZLisRGGPKROQeYDrgAbxvjFkjIn8DkowxU111bqWOyt1JAGpWkYC9uYP9lF1J5PBPgOc93/CxhHawiaDHZTD8j/ar0iUTbEJYMdmOV+gwEnpcam+0tR2nsrdVdb5BcM1ndkzDW8NsFUlhpi1JxPQ7uJ+IXSVv/qv2efkB25vonGfr9nOMetqO4Ui82U69MfOZg68tnXgwEWRvhll/t111b59lSyexiQc7MFRWRw65D2Y8CS86q/NC4mHUX+kf05M/eU2hW3kv+HKWraqb9Q9bEjp0Jt7fX7ONxJXXJaqnHahYshe6jgUPT6TtEO7auxJH1wgoL6P9rLthXwZeN0yj5yJv/AZcCSuDbDdov2A4/d66XY96cFmJwFW0RKBOWdvm2SqV6u0ijWHaH2HJO3DLLxA3wHXn2bHUNtrvXAGD7jg4SZ8rZG2A1wfaasb2Z8CKz+Dh9bb6acIw2wW5tAhOu9M2NA998PBP2weK4KdHbcnCYLvm7s+zyauy6y3YT+rzXrE92cY8d3B7YbYtbXU6F6744OD2nC22RBjuHHm95F1bZXnzDDuFyZwX4JK3Dk5hAs4G/HttFdqhPdvqyC0lAqVUPVXvhtqY+o63n9pja71HNJyY/nZgYGMI6wSD77FrcrcItT2ypj1sSz4V5fCHOfb7xW/bdo02gw4/hncLO06h0p4d8N45NgmMecF2n927y5akCjPtDb1lHPS+2o54X/iG7ZZ8xuM1j3vomhfdLrFzbn001k6X0ufamkkAbAny4jpOrX4ctESglDr1TXvYtgNUlMIVE22D+orJzm60Ynug+QUf+zj5220Pq0M/lRfvsb2WDu0mO+A2OP+Qke212bvb9q7alwE3/Wgb4RvY0UoEmgiUUs3D/nxbIojubb8v2WeXXQ1pC3f9fuLHNwbWfmtv5gER9jwh7erX06daD6SGplVDSinlF1zzU79PgB1b0VCdB0TsRIUnegw30ESglGq++lzt7giaBJ2GWimlmjlNBEop1cxpIlBKqWZOE4FSSjVzmgiUUqqZ00SglFLNnCYCpZRq5jQRKKVUM3fSTTEhIllA6nG+PQzIbsBwGlJTjU3jqp+mGhc03dg0rvo53rjaGmPCa3vhpEsEJ0JEko4014a7NdXYNK76aapxQdONTeOqH1fEpVVDSinVzGkiUEqpZq65JYK33R3AUTTV2DSu+mmqcUHTjU3jqp8Gj6tZtREopZQ6XHMrESillDpEs0kEIjJaRDaIyGYRecyNccSJyCwRWSsia0Tkfuf2p0Vkh4gkO7/Oc0Ns20RklfP8Sc5trUTkZxHZ5HwMcUNcnatdl2QRKRCRB9xxzUTkfRHJFJHV1bbVeo3E+o/zb26liPRr5LheEJH1znN/LSLBzu3xIrK/2nWb0MhxHfH3JiKPO6/XBhE511VxHSW2z6rFtU1Ekp3bG/OaHeke4bq/M2PMKf8FeABbgPaAN7AC6OamWKKBfs7ngcBGoBvwNPBHN1+nbUDYIdueBx5zPn8MeK4J/C53A23dcc2A4UA/YPWxrhFwHvAjIMAgYFEjx3UO4Ol8/ly1uOKr7+eG61Xr7835f7AC8AHaOf9nPRoztkNefwl4yg3X7Ej3CJf9nTWXEsFAYLMxJsUYcwCYDIx1RyDGmF3GmGXO53uBdUCMO2Kpo7HAh87nHwIXuy8UAEYCW4wxxzuo8IQYY+YAuYdsPtI1Ggt8ZKyFQLCIRDdWXMaYGcaYMue3C4FYV5y7vnEdxVhgsjGmxBizFdiM/d9t9NhERIArgU9ddf4jOco9wmV/Z80lEcQA26t9n04TuPmKSDzQF1jk3HSPs2j3vjuqYAADzBCRpSJyu3NbpDFml/P5biDSDXFVN46a/5zuvmZw5GvUlP7ubsZ+aqzUTkSWi8hsERnmhnhq+701pes1DMgwxmyqtq3Rr9kh9wiX/Z01l0TQ5IhIAPAl8IAxpgB4E+gA9AF2YYuljW2oMaYfMAa4W0SGV3/R2HKo27qZiYg3cBHwuXNTU7hmNbj7GtVGRJ4AyoCPnZt2AW2MMX2Bh4BPRKSBVnCvkyb3e6vF1dT8wNHo16yWe0SVhv47ay6JYAcQV+37WOc2txARL+wv+GNjzFcAxpgMY0y5MaYCeAcXFomPxBizw/mYCXztjCGjspjpfMxs7LiqGQMsM8ZkQNO4Zk5HukZu/7sTkRuBC4DxzpsHzqqXHOfzpdi6+E6NFdNRfm9uv14AIuIJXAp8Vrmtsa9ZbfcIXPh31lwSwRIgQUTaOT9VjgOmuiMQZ93je8A6Y8zL1bZXr9O7BFh96HtdHJe/iARWPsc2NK7GXqcbnLvdAHzbmHEdosanNHdfs2qOdI2mAtc7e3UMAvZUK9q7nIiMBv4EXGSMKaq2PVxEPJzP2wMJQEojxnWk39tUYJyI+IhIO2dcixsrrmpGAeuNMemVGxrzmh3pHoEr/84aoxW8KXxhW9Y3YjP5E26MYyi2SLcSSHZ+nQdMAlY5t08Fohs5rvbYHhsrgDWV1wgIBWYCm4BfgFZuum7+QA7Qstq2Rr9m2ES0CyjF1sXecqRrhO3F8brzb24VkNjIcW3G1h1X/p1NcO57mfN3nAwsAy5s5LiO+HsDnnBerw3AmMb+XTq3TwTuOGTfxrxmR7pHuOzvTEcWK6VUM9dcqoaUUkodgSYCpZRq5jQRKKVUM6eJQCmlmjlNBEop1cxpIlCnBBExIvJSte//KCJPn8DxhorIYrGzd66vNuVGZZ/yRc7pBoYd8r7fnDNnVs5S+cXxxnCEuLaJSFhDHlMpT3cHoFQDKQEuFZF/GmOyT+RAIhIFfAJcbIxZ5rzxTheRHcaYadiJ71YZY249wiHGG2OSTiQGpRqTlgjUqaIMu4Tfg4e+4JxL/lfnJGczRaTNMY51NzDRHJwBMhs7QvcxEemDnQ54rPMTv19dghORiSIyQUSSRGSjiFzg3O4rIh+IXQdiuYic6dzuISIvishqZ9z3VjvcvSKyzPmeLs79R1QrhSyvHCWuVF1oIlCnkteB8SLS8pDt/wU+NMb0wk689p9jHKc7sPSQbUlAd2NMMvAU8Jkxpo8xZn8t7/+42k35hWrb47Hz6pwPTBARX2zSMcaYntgpND50br/duX+fanFXyjZ2csA3gT86t/0RuNsY0wc7c2ZtcSlVK00E6pRh7AyNHwH3HfLSYGxVD9jpDYa6OJTxziTRxxjzSLXtU4wxFcZObZwCdHHG8j8AY8x6IBU7mdko4C3jXE/AGFN93vzKSciWYpMFwHzgZRG5Dwg2B9chUOqYNBGoU82/sfPZ+J/AMdYC/Q/Z1h8718yJOHQ+l+Od36XE+ViOs53PGPMv4FbAD5hfWWWkVF1oIlCnFOcn5ynYZFBpAXbGWYDxwNxjHOZ14EZnewAiEopd6vH5EwzvChFxiEgH7CR/G5yxjHeepxPQxrn9Z+APzimREZFWRzuwiHQwxqwyxjyHnW1XE4GqM00E6lT0ElC9i+W9wE0ishK4DqhcDPwOEbnj0DcbO4XvtcA7IrIem0jeN8Z8V8fzV28j+KXa9jTstMo/Yme3LAbeABwisgo7//2NxpgS4F3n/itFZAVwzTHO+UBlwzJ2Ns0fj7G/UlV09lGlGoGITAS+N8Y06LgCpRqClgiUUqqZ0xKBUko1c1oiUEqpZk4TgVJKNXOaCJRSqpnTRKCUUs2cJgKllGrmNBEopVQz9/+45HrPFnWp0QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "init = glorot_normal(seed=None) # 給 LSTM\n",
    "init_d = RandomUniform(minval=-0.05, maxval=0.05) # 給 Dense layer\n",
    "nadam = optimizers.Nadam(lr=0.0015,clipvalue=0.5)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(GRU(6, kernel_initializer=init ,return_sequences = True,kernel_regularizer=regularizers.l2(0.01)\n",
    "                             ,recurrent_regularizer = regularizers.l2(0.01) ,input_shape=(x_train.shape[1],x_train.shape[2]))))\n",
    "model.add(LayerNormalization())\n",
    "model.add(Bidirectional(GRU(6,kernel_initializer=init,kernel_regularizer=regularizers.l2(0.01),recurrent_regularizer = regularizers.l2(0.01))))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(units=1, kernel_initializer=init_d))\n",
    "model.compile(optimizer = nadam , loss=\"mse\")\n",
    "history = model.fit(x_train, y_train, epochs=200, batch_size=24, validation_split=0.1, shuffle=True)\n",
    "#model summary\n",
    "model.summary()\n",
    "#Save Model\n",
    "model.save('GRU_model_pit.h5')  # creates a HDF5 file \n",
    "print('Model Saved')\n",
    "del model  # deletes the existing model\n",
    "\n",
    "custom_ob = {'LayerNormalization': LayerNormalization , 'SeqSelfAttention':SeqSelfAttention}\n",
    "model = load_model('GRU_model_pit.h5', custom_objects=custom_ob)\n",
    "t1 = time.time()\n",
    "# y_pred = model.predict([x_test,x_test])\n",
    "y_pred2 = model.predict(x_test)\n",
    "y_pred = model.predict(x_train)\n",
    "t2 = time.time()\n",
    "print('Predict time: ',t2-t1)\n",
    "y_pred = scaler.inverse_transform(y_pred)#Undo scaling\n",
    "rmse_lstm2 = np.sqrt(mean_squared_error(y_test, y_pred2))\n",
    "rmse_lstm = np.sqrt(mean_squared_error(y_train, y_pred))\n",
    "print('RMSE: ',rmse_lstm)\n",
    "print('RMSE2: ',rmse_lstm2)\n",
    "mae = mean_absolute_error(y_test, y_pred2)\n",
    "mae = mean_absolute_error(y_train, y_pred)\n",
    "print('MAE: ',mae)\n",
    "print('MAE2: ',mae)\n",
    "# r22 =  r2_score(y_test, y_pred2)\n",
    "# r2 =  r2_score(y_train, y_pred)\n",
    "# print('R-square: ',r2)\n",
    "# print('R-square2: ',r22)\n",
    "\n",
    "# n = len(y_test)\n",
    "# p = 12\n",
    "# Adj_r2 = 1-(1-r2)*(n-1)/(n-p-1)\n",
    "# Adj_r22 = 1-(1-r22)*(n-1)/(n-p-1)\n",
    "# print('Adj R-square: ',Adj_r2)\n",
    "# print('Adj R-square2: ',Adj_r22)\n",
    "\n",
    "plt.plot(history.history[\"loss\"],label=\"loss\")\n",
    "plt.plot(history.history[\"val_loss\"],label=\"val_loss\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"No. Of Epochs\")\n",
    "plt.ylabel(\"mse score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570ac3b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746ee329",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
