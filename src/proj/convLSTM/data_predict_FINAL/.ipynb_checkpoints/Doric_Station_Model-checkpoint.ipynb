{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bab963ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from tcn import TCN\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error, r2_score,mean_absolute_error\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential , load_model , Model\n",
    "from keras.layers import Dense, Dropout , LSTM , Bidirectional ,GRU ,Flatten,Add,BatchNormalization\n",
    "from keras_layer_normalization import LayerNormalization\n",
    "from keras.initializers import  glorot_normal, RandomUniform\n",
    "from keras import optimizers,Input\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3198e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160, 13) (144, 13) (40, 13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:32: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c74a74d036946d99d437c75ff0b8ea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:44: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7903e7f113d4b69a54963c0048cca5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:\n",
      "(120, 24, 12) (120,)\n",
      "Test size:\n",
      "(16, 24, 12) (16,)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"station_bike_Doric.csv\")\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "df = df.set_index(\"timestamp\")\n",
    "#df.head()\n",
    "\n",
    "df[\"hour\"] = df.index.hour\n",
    "df[\"day_of_month\"] = df.index.day\n",
    "df[\"day_of_week\"]  = df.index.dayofweek\n",
    "df[\"month\"] = df.index.month\n",
    "\n",
    "training_data_len = math.ceil(len(df) * 0.9) # taking 90% of data to train and 10% of data to test\n",
    "testing_data_len = len(df) - training_data_len\n",
    "\n",
    "time_steps = 24\n",
    "train, test = df.iloc[0:training_data_len], df.iloc[(training_data_len-time_steps):len(df)]\n",
    "print(df.shape, train.shape, test.shape)\n",
    "train_trans = train[['t1','t2', 'hum', 'wind_speed']].to_numpy()\n",
    "test_trans = test[['t1','t2', 'hum', 'wind_speed']].to_numpy()\n",
    "\n",
    "scaler = RobustScaler() # Handles outliers\n",
    "#scaler = MinMaxScaler(feature_range=(0, 1)) # scale to (0,1)\n",
    "train.loc[:, ['t1','t2','hum', 'wind_speed']]=scaler.fit_transform(train_trans)\n",
    "test.loc[:, ['t1','t2', 'hum', 'wind_speed']]=scaler.fit_transform(test_trans)\n",
    "\n",
    "train['cnt'] = scaler.fit_transform(train[['cnt']])\n",
    "test['cnt'] = scaler.fit_transform(test[['cnt']])\n",
    "\n",
    "#Split the data into x_train and y_train data sets\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in tqdm(range(len(train) - time_steps)):\n",
    "    x_train.append(train.drop(columns='cnt').iloc[i:i + time_steps].to_numpy())\n",
    "    y_train.append(train.loc[:,'cnt'].iloc[i + time_steps])\n",
    "\n",
    "#Convert x_train and y_train to numpy arrays\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "#Create the x_test and y_test data sets\n",
    "x_test = []\n",
    "y_test = df.loc[:,'cnt'].iloc[training_data_len:len(df)]\n",
    "\n",
    "for i in tqdm(range(len(test) - time_steps)):\n",
    "    x_test.append(test.drop(columns='cnt').iloc[i:i + time_steps].to_numpy())\n",
    "    # y_test.append(test.loc[:,'cnt'].iloc[i + time_steps])\n",
    "\n",
    "#Convert x_test and y_test to numpy arrays\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# All 12 columns of the data\n",
    "print('Train size:')\n",
    "print(x_train.shape, y_train.shape)\n",
    "print('Test size:')\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61c85bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 24, 12)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 24, 24)       1800        input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "seq_self_attention_3 (SeqSelfAt (None, 24, 24)       577         bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 24, 24)       0           seq_self_attention_3[0][0]       \n",
      "                                                                 bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_3 (LayerNor (None, 24, 24)       48          add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 576)          0           layer_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 12)           6924        flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 12)           0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 588)          0           dropout_3[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            589         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 1)            2           dense_8[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 9,940\n",
      "Trainable params: 9,940\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Concatenate\n",
    "init = glorot_normal(seed=None) # 給 GRU\n",
    "init_d = RandomUniform(minval=-0.05, maxval=0.05) # 給 Dense layer\n",
    "\n",
    "def Encoder(layer):\n",
    "    shortcut = layer\n",
    "    layer = SeqSelfAttention(\n",
    "                     attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n",
    "                     kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     bias_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     attention_regularizer_weight=0.0001)(layer)\n",
    "    layer = Add()([layer,shortcut])\n",
    "    layer = LayerNormalization()(layer)\n",
    "    layer = Flatten()(layer)\n",
    "    \n",
    "    shortcut2 = layer\n",
    "    layer = Dense(12,kernel_initializer=init_d)(layer)\n",
    "    layer = Dropout(0.2)(layer)\n",
    "    layer = Concatenate()([layer,shortcut2])\n",
    "    output = Dense(1,kernel_initializer=init_d)(layer)\n",
    "    return output\n",
    "\n",
    "def Decoder(layer):\n",
    "    shortcut = layer\n",
    "    layer = SeqSelfAttention(\n",
    "                     attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n",
    "                     kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     bias_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     attention_regularizer_weight=0.0001)(layer)\n",
    "    layer = Add()([layer,shortcut])\n",
    "    layer = LayerNormalization()(layer)\n",
    "    layer = SeqSelfAttention(\n",
    "                     attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n",
    "                     kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     bias_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     attention_regularizer_weight=0.0001)(layer)\n",
    "    layer = LayerNormalization()(layer)\n",
    "    \n",
    "    layer = Flatten()(layer)\n",
    "    shortcut2 = layer\n",
    "    layer = Dense(10,kernel_initializer=init_d)(layer)\n",
    "    #layer = Dropout(0.2)(layer)\n",
    "    layer = Concatenate()([layer,shortcut2])\n",
    "    output = Dense(1,kernel_initializer=init_d)(layer)\n",
    "    return output\n",
    "\n",
    "def Bi_GRU(layer,unit):\n",
    "    output = Bidirectional(GRU(unit, dropout=0.1, recurrent_dropout=0.1, return_sequences=True,\n",
    "                            kernel_initializer=init))(layer)\n",
    "    return output\n",
    "\n",
    "#start = Input(shape = (x_train.shape[1],x_train.shape[2]))\n",
    "start = Input(shape = (x_train.shape[1:]))\n",
    "start2 = Input(shape = (x_train.shape[1:]))\n",
    "x = Bi_GRU(start,12)\n",
    "x = Encoder(x)\n",
    "\n",
    "# y = Bi_GRU(start2,8)\n",
    "# y = Decoder(y)\n",
    "\n",
    "#Merge = Add()([x,x])\n",
    "Last = Dense(1)(x)\n",
    "model = Model([start,start2] , Last)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a4e9730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 108 samples, validate on 12 samples\n",
      "Epoch 1/500\n",
      "108/108 [==============================] - 2s 17ms/step - loss: 2.1082 - val_loss: 1.1366\n",
      "Epoch 2/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.9434 - val_loss: 1.1355\n",
      "Epoch 3/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.8796 - val_loss: 1.3978\n",
      "Epoch 4/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.8389 - val_loss: 0.8929\n",
      "Epoch 5/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.7274 - val_loss: 1.3223\n",
      "Epoch 6/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.7095 - val_loss: 0.8915\n",
      "Epoch 7/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.4130 - val_loss: 1.2995\n",
      "Epoch 8/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.4279 - val_loss: 0.7498\n",
      "Epoch 9/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.2875 - val_loss: 0.6098\n",
      "Epoch 10/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.3606 - val_loss: 1.2885\n",
      "Epoch 11/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.2015 - val_loss: 0.8485\n",
      "Epoch 12/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.1719 - val_loss: 0.6245\n",
      "Epoch 13/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.1194 - val_loss: 1.3238\n",
      "Epoch 14/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.3019 - val_loss: 1.0361\n",
      "Epoch 15/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.0654 - val_loss: 0.6515\n",
      "Epoch 16/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.8433 - val_loss: 0.8368\n",
      "Epoch 17/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.9672 - val_loss: 0.6441\n",
      "Epoch 18/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.9737 - val_loss: 0.7787\n",
      "Epoch 19/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.7895 - val_loss: 0.7103\n",
      "Epoch 20/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.8143 - val_loss: 0.5879\n",
      "Epoch 21/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7363 - val_loss: 1.0436\n",
      "Epoch 22/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.0119 - val_loss: 0.5570\n",
      "Epoch 23/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8208 - val_loss: 0.5636\n",
      "Epoch 24/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7478 - val_loss: 0.6889\n",
      "Epoch 25/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.7100 - val_loss: 0.6693\n",
      "Epoch 26/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.8398 - val_loss: 0.7898\n",
      "Epoch 27/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6688 - val_loss: 0.5691\n",
      "Epoch 28/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6661 - val_loss: 0.4893\n",
      "Epoch 29/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5318 - val_loss: 0.6526\n",
      "Epoch 30/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5629 - val_loss: 0.7415\n",
      "Epoch 31/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5114 - val_loss: 0.5975\n",
      "Epoch 32/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5874 - val_loss: 0.7205\n",
      "Epoch 33/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6228 - val_loss: 0.6607\n",
      "Epoch 34/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8845 - val_loss: 0.6878\n",
      "Epoch 35/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4701 - val_loss: 0.6343\n",
      "Epoch 36/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4396 - val_loss: 0.7622\n",
      "Epoch 37/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5524 - val_loss: 0.7895\n",
      "Epoch 38/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6179 - val_loss: 0.6609\n",
      "Epoch 39/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5928 - val_loss: 0.9383\n",
      "Epoch 40/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4593 - val_loss: 0.6746\n",
      "Epoch 41/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5144 - val_loss: 0.7003\n",
      "Epoch 42/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5392 - val_loss: 0.6264\n",
      "Epoch 43/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3967 - val_loss: 0.6427\n",
      "Epoch 44/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4467 - val_loss: 0.8119\n",
      "Epoch 45/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4676 - val_loss: 0.5354\n",
      "Epoch 46/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3430 - val_loss: 0.5733\n",
      "Epoch 47/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7853 - val_loss: 0.5823\n",
      "Epoch 48/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5972 - val_loss: 0.5318\n",
      "Epoch 49/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4274 - val_loss: 0.5262\n",
      "Epoch 50/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4057 - val_loss: 0.6037\n",
      "Epoch 51/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4317 - val_loss: 0.7551\n",
      "Epoch 52/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4681 - val_loss: 0.5364\n",
      "Epoch 53/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5978 - val_loss: 0.6232\n",
      "Epoch 54/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5643 - val_loss: 0.7676\n",
      "Epoch 55/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5490 - val_loss: 0.6338\n",
      "Epoch 56/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4186 - val_loss: 0.5907\n",
      "Epoch 57/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3001 - val_loss: 0.4992\n",
      "Epoch 58/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3571 - val_loss: 0.4956\n",
      "Epoch 59/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5303 - val_loss: 0.6568\n",
      "Epoch 60/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4216 - val_loss: 0.6806\n",
      "Epoch 61/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3812 - val_loss: 0.5081\n",
      "Epoch 62/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5418 - val_loss: 0.7745\n",
      "Epoch 63/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5129 - val_loss: 0.6464\n",
      "Epoch 64/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4898 - val_loss: 0.5648\n",
      "Epoch 65/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4258 - val_loss: 0.5105\n",
      "Epoch 66/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4103 - val_loss: 0.6490\n",
      "Epoch 67/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4039 - val_loss: 0.5039\n",
      "Epoch 68/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4137 - val_loss: 0.5567\n",
      "Epoch 69/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4263 - val_loss: 0.5001\n",
      "Epoch 70/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5050 - val_loss: 0.5949\n",
      "Epoch 71/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4193 - val_loss: 0.6140\n",
      "Epoch 72/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3912 - val_loss: 0.6663\n",
      "Epoch 73/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3234 - val_loss: 0.5681\n",
      "Epoch 74/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3950 - val_loss: 0.5694\n",
      "Epoch 75/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2973 - val_loss: 0.5625\n",
      "Epoch 76/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3846 - val_loss: 0.5670\n",
      "Epoch 77/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3295 - val_loss: 0.5021\n",
      "Epoch 78/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3505 - val_loss: 0.6264\n",
      "Epoch 79/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4007 - val_loss: 0.6374\n",
      "Epoch 80/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2916 - val_loss: 0.5661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5347 - val_loss: 0.5937\n",
      "Epoch 82/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4740 - val_loss: 0.6353\n",
      "Epoch 83/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4580 - val_loss: 0.6570\n",
      "Epoch 84/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4213 - val_loss: 0.4968\n",
      "Epoch 85/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4134 - val_loss: 0.4254\n",
      "Epoch 86/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4608 - val_loss: 0.5577\n",
      "Epoch 87/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3741 - val_loss: 0.6232\n",
      "Epoch 88/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3933 - val_loss: 0.5422\n",
      "Epoch 89/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3972 - val_loss: 0.5595\n",
      "Epoch 90/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4183 - val_loss: 0.6186\n",
      "Epoch 91/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3150 - val_loss: 0.6785\n",
      "Epoch 92/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3310 - val_loss: 0.6520\n",
      "Epoch 93/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4468 - val_loss: 0.5695\n",
      "Epoch 94/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4340 - val_loss: 0.5830\n",
      "Epoch 95/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5816 - val_loss: 0.6985\n",
      "Epoch 96/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4202 - val_loss: 0.6143\n",
      "Epoch 97/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3053 - val_loss: 0.5130\n",
      "Epoch 98/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2707 - val_loss: 0.5489\n",
      "Epoch 99/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3075 - val_loss: 0.7119\n",
      "Epoch 100/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2651 - val_loss: 0.4752\n",
      "Epoch 101/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2101 - val_loss: 0.5260\n",
      "Epoch 102/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4907 - val_loss: 0.7402\n",
      "Epoch 103/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2188 - val_loss: 0.7468\n",
      "Epoch 104/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2697 - val_loss: 0.5729\n",
      "Epoch 105/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3083 - val_loss: 0.5388\n",
      "Epoch 106/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2784 - val_loss: 0.5309\n",
      "Epoch 107/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2641 - val_loss: 0.4782\n",
      "Epoch 108/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2112 - val_loss: 0.5147\n",
      "Epoch 109/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4438 - val_loss: 0.4455\n",
      "Epoch 110/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4562 - val_loss: 0.4528\n",
      "Epoch 111/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2666 - val_loss: 0.4593\n",
      "Epoch 112/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2586 - val_loss: 0.5330\n",
      "Epoch 113/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2136 - val_loss: 0.5088\n",
      "Epoch 114/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2210 - val_loss: 0.5043\n",
      "Epoch 115/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3569 - val_loss: 0.5126\n",
      "Epoch 116/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3691 - val_loss: 0.5881\n",
      "Epoch 117/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3331 - val_loss: 0.5352\n",
      "Epoch 118/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4063 - val_loss: 0.6191\n",
      "Epoch 119/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3824 - val_loss: 0.5264\n",
      "Epoch 120/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2299 - val_loss: 0.4249\n",
      "Epoch 121/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2691 - val_loss: 0.5640\n",
      "Epoch 122/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2438 - val_loss: 0.5283\n",
      "Epoch 123/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2403 - val_loss: 0.5107\n",
      "Epoch 124/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3565 - val_loss: 0.5108\n",
      "Epoch 125/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2673 - val_loss: 0.6088\n",
      "Epoch 126/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3278 - val_loss: 0.5057\n",
      "Epoch 127/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3092 - val_loss: 0.5386\n",
      "Epoch 128/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2419 - val_loss: 0.7083\n",
      "Epoch 129/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2863 - val_loss: 0.4934\n",
      "Epoch 130/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3308 - val_loss: 0.6412\n",
      "Epoch 131/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3074 - val_loss: 0.6007\n",
      "Epoch 132/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2762 - val_loss: 0.6412\n",
      "Epoch 133/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3923 - val_loss: 0.4779\n",
      "Epoch 134/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2701 - val_loss: 0.5767\n",
      "Epoch 135/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3392 - val_loss: 0.7138\n",
      "Epoch 136/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4719 - val_loss: 0.5902\n",
      "Epoch 137/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4338 - val_loss: 0.7594\n",
      "Epoch 138/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3016 - val_loss: 0.4932\n",
      "Epoch 139/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2028 - val_loss: 0.4307\n",
      "Epoch 140/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2223 - val_loss: 0.6930\n",
      "Epoch 141/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2367 - val_loss: 0.5318\n",
      "Epoch 142/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2911 - val_loss: 0.5850\n",
      "Epoch 143/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3497 - val_loss: 0.6236\n",
      "Epoch 144/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3482 - val_loss: 0.5610\n",
      "Epoch 145/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3654 - val_loss: 0.7563\n",
      "Epoch 146/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2922 - val_loss: 0.7046\n",
      "Epoch 147/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2578 - val_loss: 0.5187\n",
      "Epoch 148/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3768 - val_loss: 0.5415\n",
      "Epoch 149/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3672 - val_loss: 0.6108\n",
      "Epoch 150/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2183 - val_loss: 0.5021\n",
      "Epoch 151/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2926 - val_loss: 0.4551\n",
      "Epoch 152/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2318 - val_loss: 0.7088\n",
      "Epoch 153/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2870 - val_loss: 0.5296\n",
      "Epoch 154/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2210 - val_loss: 0.4734\n",
      "Epoch 155/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2200 - val_loss: 0.4028\n",
      "Epoch 156/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3082 - val_loss: 0.5748\n",
      "Epoch 157/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2714 - val_loss: 0.5255\n",
      "Epoch 158/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2756 - val_loss: 0.5023\n",
      "Epoch 159/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3937 - val_loss: 0.5799\n",
      "Epoch 160/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2766 - val_loss: 0.5480\n",
      "Epoch 161/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2023 - val_loss: 0.4568\n",
      "Epoch 162/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2388 - val_loss: 0.6385\n",
      "Epoch 163/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3320 - val_loss: 0.4409\n",
      "Epoch 164/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2562 - val_loss: 0.5768\n",
      "Epoch 165/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2899 - val_loss: 0.4944\n",
      "Epoch 166/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2215 - val_loss: 0.5470\n",
      "Epoch 167/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2313 - val_loss: 0.5437\n",
      "Epoch 168/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1967 - val_loss: 0.4485\n",
      "Epoch 169/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3294 - val_loss: 0.6091\n",
      "Epoch 170/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2749 - val_loss: 0.6656\n",
      "Epoch 171/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1851 - val_loss: 0.6362\n",
      "Epoch 172/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3269 - val_loss: 0.5326\n",
      "Epoch 173/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2141 - val_loss: 0.7996\n",
      "Epoch 174/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2639 - val_loss: 0.6723\n",
      "Epoch 175/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3396 - val_loss: 0.5793\n",
      "Epoch 176/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2355 - val_loss: 0.5457\n",
      "Epoch 177/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3175 - val_loss: 0.6197\n",
      "Epoch 178/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3282 - val_loss: 0.5331\n",
      "Epoch 179/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2755 - val_loss: 0.4967\n",
      "Epoch 180/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1992 - val_loss: 0.5186\n",
      "Epoch 181/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1686 - val_loss: 0.5336\n",
      "Epoch 182/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2460 - val_loss: 0.7600\n",
      "Epoch 183/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2525 - val_loss: 0.5581\n",
      "Epoch 184/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3039 - val_loss: 0.5650\n",
      "Epoch 185/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4092 - val_loss: 0.5915\n",
      "Epoch 186/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2062 - val_loss: 0.5317\n",
      "Epoch 187/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1975 - val_loss: 0.4804\n",
      "Epoch 188/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2202 - val_loss: 0.4722\n",
      "Epoch 189/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2533 - val_loss: 0.4681\n",
      "Epoch 190/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2264 - val_loss: 0.5214\n",
      "Epoch 191/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2117 - val_loss: 0.5754\n",
      "Epoch 192/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2628 - val_loss: 0.5042\n",
      "Epoch 193/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2166 - val_loss: 0.5482\n",
      "Epoch 194/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2184 - val_loss: 0.5168\n",
      "Epoch 195/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2014 - val_loss: 0.5952\n",
      "Epoch 196/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2956 - val_loss: 0.5725\n",
      "Epoch 197/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1658 - val_loss: 0.4595\n",
      "Epoch 198/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2751 - val_loss: 0.4670\n",
      "Epoch 199/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1665 - val_loss: 0.5118\n",
      "Epoch 200/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3220 - val_loss: 0.4820\n",
      "Epoch 201/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2064 - val_loss: 0.5042\n",
      "Epoch 202/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2076 - val_loss: 0.5582\n",
      "Epoch 203/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2097 - val_loss: 0.6672\n",
      "Epoch 204/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2973 - val_loss: 0.6592\n",
      "Epoch 205/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1934 - val_loss: 0.5708\n",
      "Epoch 206/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2143 - val_loss: 0.5036\n",
      "Epoch 207/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3551 - val_loss: 0.7478\n",
      "Epoch 208/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2133 - val_loss: 0.5771\n",
      "Epoch 209/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2374 - val_loss: 0.5237\n",
      "Epoch 210/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2346 - val_loss: 0.5754\n",
      "Epoch 211/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2562 - val_loss: 1.0034\n",
      "Epoch 212/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2252 - val_loss: 0.6481\n",
      "Epoch 213/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2424 - val_loss: 0.6478\n",
      "Epoch 214/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3020 - val_loss: 0.6460\n",
      "Epoch 215/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2320 - val_loss: 0.5193\n",
      "Epoch 216/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2458 - val_loss: 0.6312\n",
      "Epoch 217/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1792 - val_loss: 0.6537\n",
      "Epoch 218/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2206 - val_loss: 0.8086\n",
      "Epoch 219/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2167 - val_loss: 0.7441\n",
      "Epoch 220/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2077 - val_loss: 0.5220\n",
      "Epoch 221/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2167 - val_loss: 0.7420\n",
      "Epoch 222/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2825 - val_loss: 0.7273\n",
      "Epoch 223/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2035 - val_loss: 0.5725\n",
      "Epoch 224/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1738 - val_loss: 0.8218\n",
      "Epoch 225/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1568 - val_loss: 0.7565\n",
      "Epoch 226/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1840 - val_loss: 0.7374\n",
      "Epoch 227/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2239 - val_loss: 0.7242\n",
      "Epoch 228/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2594 - val_loss: 0.7396\n",
      "Epoch 229/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1601 - val_loss: 0.6420\n",
      "Epoch 230/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2561 - val_loss: 0.6538\n",
      "Epoch 231/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1694 - val_loss: 0.7893\n",
      "Epoch 232/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2358 - val_loss: 0.6917\n",
      "Epoch 233/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2120 - val_loss: 0.7475\n",
      "Epoch 234/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1342 - val_loss: 0.5608\n",
      "Epoch 235/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1868 - val_loss: 0.5740\n",
      "Epoch 236/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2020 - val_loss: 0.7942\n",
      "Epoch 237/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2450 - val_loss: 0.6481\n",
      "Epoch 238/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2046 - val_loss: 0.7739\n",
      "Epoch 239/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2461 - val_loss: 0.6440\n",
      "Epoch 240/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1781 - val_loss: 0.5827\n",
      "Epoch 241/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2351 - val_loss: 0.8121\n",
      "Epoch 242/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2184 - val_loss: 0.5549\n",
      "Epoch 243/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1656 - val_loss: 0.5600\n",
      "Epoch 244/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2260 - val_loss: 0.6065\n",
      "Epoch 245/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1813 - val_loss: 0.6773\n",
      "Epoch 246/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1714 - val_loss: 0.6227\n",
      "Epoch 247/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2938 - val_loss: 0.6944\n",
      "Epoch 248/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1933 - val_loss: 0.6560\n",
      "Epoch 249/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1855 - val_loss: 0.6149\n",
      "Epoch 250/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2067 - val_loss: 0.5231\n",
      "Epoch 251/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2163 - val_loss: 0.4999\n",
      "Epoch 252/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2256 - val_loss: 0.5564\n",
      "Epoch 253/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1999 - val_loss: 0.5278\n",
      "Epoch 254/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2010 - val_loss: 0.5027\n",
      "Epoch 255/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2016 - val_loss: 0.5083\n",
      "Epoch 256/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1345 - val_loss: 0.5606\n",
      "Epoch 257/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1495 - val_loss: 0.4761\n",
      "Epoch 258/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2587 - val_loss: 0.6178\n",
      "Epoch 259/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1491 - val_loss: 0.6059\n",
      "Epoch 260/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2958 - val_loss: 0.7125\n",
      "Epoch 261/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1555 - val_loss: 0.8530\n",
      "Epoch 262/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1922 - val_loss: 0.6350\n",
      "Epoch 263/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1348 - val_loss: 0.6229\n",
      "Epoch 264/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2671 - val_loss: 0.6036\n",
      "Epoch 265/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1668 - val_loss: 0.6402\n",
      "Epoch 266/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1823 - val_loss: 0.5994\n",
      "Epoch 267/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1838 - val_loss: 0.7509\n",
      "Epoch 268/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2066 - val_loss: 0.6234\n",
      "Epoch 269/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1708 - val_loss: 0.5801\n",
      "Epoch 270/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1499 - val_loss: 0.4936\n",
      "Epoch 271/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1428 - val_loss: 0.6006\n",
      "Epoch 272/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1528 - val_loss: 0.5706\n",
      "Epoch 273/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1854 - val_loss: 0.6750\n",
      "Epoch 274/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1426 - val_loss: 0.8999\n",
      "Epoch 275/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1832 - val_loss: 0.6465\n",
      "Epoch 276/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1227 - val_loss: 0.6262\n",
      "Epoch 277/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1389 - val_loss: 0.6182\n",
      "Epoch 278/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1391 - val_loss: 0.5661\n",
      "Epoch 279/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1432 - val_loss: 0.6048\n",
      "Epoch 280/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1751 - val_loss: 0.6444\n",
      "Epoch 281/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2070 - val_loss: 0.6125\n",
      "Epoch 282/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1943 - val_loss: 0.7058\n",
      "Epoch 283/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1838 - val_loss: 0.6874\n",
      "Epoch 284/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1508 - val_loss: 0.6816\n",
      "Epoch 285/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1732 - val_loss: 0.6379\n",
      "Epoch 286/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2918 - val_loss: 0.8229\n",
      "Epoch 287/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1603 - val_loss: 0.5710\n",
      "Epoch 288/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1702 - val_loss: 0.7282\n",
      "Epoch 289/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1412 - val_loss: 0.5905\n",
      "Epoch 290/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1578 - val_loss: 0.6127\n",
      "Epoch 291/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2019 - val_loss: 0.4766\n",
      "Epoch 292/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1470 - val_loss: 0.5392\n",
      "Epoch 293/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1502 - val_loss: 0.5845\n",
      "Epoch 294/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1339 - val_loss: 0.5782\n",
      "Epoch 295/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1212 - val_loss: 0.5685\n",
      "Epoch 296/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1554 - val_loss: 0.4726\n",
      "Epoch 297/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1623 - val_loss: 0.6177\n",
      "Epoch 298/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2180 - val_loss: 0.4462\n",
      "Epoch 299/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1931 - val_loss: 0.6783\n",
      "Epoch 300/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2158 - val_loss: 0.6624\n",
      "Epoch 301/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2072 - val_loss: 0.7855\n",
      "Epoch 302/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1652 - val_loss: 0.5652\n",
      "Epoch 303/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1544 - val_loss: 0.5164\n",
      "Epoch 304/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1341 - val_loss: 0.4532\n",
      "Epoch 305/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1738 - val_loss: 0.5742\n",
      "Epoch 306/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1144 - val_loss: 0.6477\n",
      "Epoch 307/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1646 - val_loss: 0.5014\n",
      "Epoch 308/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3187 - val_loss: 0.5135\n",
      "Epoch 309/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2138 - val_loss: 0.5156\n",
      "Epoch 310/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2087 - val_loss: 0.5865\n",
      "Epoch 311/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1236 - val_loss: 0.6076\n",
      "Epoch 312/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1697 - val_loss: 0.5619\n",
      "Epoch 313/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1563 - val_loss: 0.5767\n",
      "Epoch 314/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1343 - val_loss: 0.4839\n",
      "Epoch 315/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1616 - val_loss: 0.4809\n",
      "Epoch 316/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1259 - val_loss: 0.4469\n",
      "Epoch 317/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1809 - val_loss: 0.5631\n",
      "Epoch 318/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1779 - val_loss: 0.6288\n",
      "Epoch 319/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1628 - val_loss: 0.5534\n",
      "Epoch 320/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1238 - val_loss: 0.6154\n",
      "Epoch 321/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1212 - val_loss: 0.5413\n",
      "Epoch 322/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1297 - val_loss: 0.5229\n",
      "Epoch 323/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1265 - val_loss: 0.4994\n",
      "Epoch 324/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1472 - val_loss: 0.6189\n",
      "Epoch 325/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1232 - val_loss: 0.5727\n",
      "Epoch 326/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1136 - val_loss: 0.5575\n",
      "Epoch 327/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1163 - val_loss: 0.5819\n",
      "Epoch 328/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1420 - val_loss: 0.5844\n",
      "Epoch 329/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1328 - val_loss: 0.5229\n",
      "Epoch 330/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1599 - val_loss: 0.6079\n",
      "Epoch 331/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1253 - val_loss: 0.6213\n",
      "Epoch 332/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1522 - val_loss: 0.6402\n",
      "Epoch 333/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1176 - val_loss: 0.5661\n",
      "Epoch 334/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1254 - val_loss: 0.6173\n",
      "Epoch 335/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1164 - val_loss: 0.6792\n",
      "Epoch 336/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1171 - val_loss: 0.4955\n",
      "Epoch 337/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2285 - val_loss: 0.6441\n",
      "Epoch 338/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1588 - val_loss: 0.6615\n",
      "Epoch 339/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1661 - val_loss: 0.6742\n",
      "Epoch 340/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1378 - val_loss: 0.6558\n",
      "Epoch 341/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1389 - val_loss: 0.6930\n",
      "Epoch 342/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1261 - val_loss: 0.6282\n",
      "Epoch 343/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1236 - val_loss: 0.6160\n",
      "Epoch 344/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1340 - val_loss: 0.5947\n",
      "Epoch 345/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1129 - val_loss: 0.6696\n",
      "Epoch 346/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1215 - val_loss: 0.5275\n",
      "Epoch 347/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1187 - val_loss: 0.5646\n",
      "Epoch 348/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1808 - val_loss: 0.9133\n",
      "Epoch 349/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1509 - val_loss: 0.8959\n",
      "Epoch 350/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1299 - val_loss: 0.6403\n",
      "Epoch 351/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1243 - val_loss: 0.6458\n",
      "Epoch 352/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1373 - val_loss: 0.5505\n",
      "Epoch 353/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2043 - val_loss: 0.7107\n",
      "Epoch 354/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1752 - val_loss: 0.7983\n",
      "Epoch 355/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1256 - val_loss: 0.5711\n",
      "Epoch 356/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2396 - val_loss: 0.5753\n",
      "Epoch 357/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1701 - val_loss: 0.6237\n",
      "Epoch 358/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1812 - val_loss: 0.8367\n",
      "Epoch 359/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1769 - val_loss: 0.7561\n",
      "Epoch 360/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1151 - val_loss: 0.5986\n",
      "Epoch 361/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1628 - val_loss: 0.5608\n",
      "Epoch 362/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1295 - val_loss: 0.4817\n",
      "Epoch 363/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1438 - val_loss: 0.5418\n",
      "Epoch 364/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1333 - val_loss: 0.5265\n",
      "Epoch 365/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1706 - val_loss: 0.6164\n",
      "Epoch 366/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1795 - val_loss: 0.5296\n",
      "Epoch 367/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1687 - val_loss: 0.6418\n",
      "Epoch 368/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1297 - val_loss: 0.6456\n",
      "Epoch 369/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1100 - val_loss: 0.6336\n",
      "Epoch 370/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1241 - val_loss: 0.5947\n",
      "Epoch 371/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1111 - val_loss: 0.5735\n",
      "Epoch 372/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1591 - val_loss: 0.6909\n",
      "Epoch 373/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1215 - val_loss: 0.7089\n",
      "Epoch 374/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1507 - val_loss: 0.6002\n",
      "Epoch 375/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1362 - val_loss: 0.7307\n",
      "Epoch 376/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1383 - val_loss: 0.8209\n",
      "Epoch 377/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1390 - val_loss: 0.5504\n",
      "Epoch 378/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2372 - val_loss: 0.5619\n",
      "Epoch 379/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1209 - val_loss: 0.8996\n",
      "Epoch 380/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1308 - val_loss: 0.6456\n",
      "Epoch 381/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1292 - val_loss: 0.7453\n",
      "Epoch 382/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1785 - val_loss: 0.6964\n",
      "Epoch 383/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1985 - val_loss: 0.6586\n",
      "Epoch 384/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1436 - val_loss: 0.6904\n",
      "Epoch 385/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1561 - val_loss: 0.6735\n",
      "Epoch 386/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1669 - val_loss: 0.6013\n",
      "Epoch 387/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1263 - val_loss: 0.7305\n",
      "Epoch 388/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1248 - val_loss: 0.6117\n",
      "Epoch 389/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1235 - val_loss: 0.7940\n",
      "Epoch 390/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1347 - val_loss: 0.6266\n",
      "Epoch 391/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1316 - val_loss: 0.5063\n",
      "Epoch 392/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1401 - val_loss: 0.6238\n",
      "Epoch 393/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1162 - val_loss: 0.5878\n",
      "Epoch 394/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1438 - val_loss: 0.5821\n",
      "Epoch 395/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1177 - val_loss: 0.6551\n",
      "Epoch 396/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1467 - val_loss: 0.5476\n",
      "Epoch 397/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1059 - val_loss: 0.6187\n",
      "Epoch 398/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1040 - val_loss: 0.6751\n",
      "Epoch 399/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1518 - val_loss: 0.5435\n",
      "Epoch 400/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1832 - val_loss: 0.7521\n",
      "Epoch 401/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1013 - val_loss: 0.5956\n",
      "Epoch 402/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1302 - val_loss: 0.5667\n",
      "Epoch 403/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1430 - val_loss: 0.5809\n",
      "Epoch 404/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1223 - val_loss: 0.4832\n",
      "Epoch 405/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1083 - val_loss: 0.6229\n",
      "Epoch 406/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1510 - val_loss: 0.5259\n",
      "Epoch 407/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1191 - val_loss: 0.6286\n",
      "Epoch 408/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1818 - val_loss: 0.6043\n",
      "Epoch 409/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1105 - val_loss: 0.6068\n",
      "Epoch 410/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0756 - val_loss: 0.5922\n",
      "Epoch 411/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1145 - val_loss: 0.5889\n",
      "Epoch 412/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0943 - val_loss: 0.6655\n",
      "Epoch 413/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0980 - val_loss: 0.6287\n",
      "Epoch 414/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1032 - val_loss: 0.6089\n",
      "Epoch 415/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1220 - val_loss: 0.4780\n",
      "Epoch 416/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0974 - val_loss: 0.5121\n",
      "Epoch 417/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0964 - val_loss: 0.6008\n",
      "Epoch 418/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1176 - val_loss: 0.5235\n",
      "Epoch 419/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1046 - val_loss: 0.5809\n",
      "Epoch 420/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1104 - val_loss: 0.4968\n",
      "Epoch 421/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0967 - val_loss: 0.5375\n",
      "Epoch 422/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0821 - val_loss: 0.5604\n",
      "Epoch 423/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1012 - val_loss: 0.6549\n",
      "Epoch 424/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1089 - val_loss: 0.5595\n",
      "Epoch 425/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0926 - val_loss: 0.6318\n",
      "Epoch 426/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1289 - val_loss: 0.6661\n",
      "Epoch 427/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1260 - val_loss: 0.5620\n",
      "Epoch 428/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1084 - val_loss: 0.5841\n",
      "Epoch 429/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1304 - val_loss: 0.5453\n",
      "Epoch 430/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1432 - val_loss: 0.6602\n",
      "Epoch 431/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1280 - val_loss: 0.5530\n",
      "Epoch 432/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1038 - val_loss: 0.5763\n",
      "Epoch 433/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0981 - val_loss: 0.6394\n",
      "Epoch 434/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1595 - val_loss: 0.5676\n",
      "Epoch 435/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1229 - val_loss: 0.6110\n",
      "Epoch 436/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0947 - val_loss: 0.6470\n",
      "Epoch 437/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1231 - val_loss: 0.5053\n",
      "Epoch 438/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0944 - val_loss: 0.4836\n",
      "Epoch 439/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0968 - val_loss: 0.6045\n",
      "Epoch 440/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1220 - val_loss: 0.7966\n",
      "Epoch 441/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0947 - val_loss: 0.6649\n",
      "Epoch 442/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1141 - val_loss: 0.4934\n",
      "Epoch 443/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1068 - val_loss: 0.5858\n",
      "Epoch 444/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1116 - val_loss: 0.6193\n",
      "Epoch 445/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1244 - val_loss: 0.6022\n",
      "Epoch 446/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1008 - val_loss: 0.6131\n",
      "Epoch 447/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1473 - val_loss: 0.5238\n",
      "Epoch 448/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2146 - val_loss: 0.5920\n",
      "Epoch 449/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1669 - val_loss: 0.9697\n",
      "Epoch 450/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0940 - val_loss: 0.7415\n",
      "Epoch 451/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0970 - val_loss: 0.6532\n",
      "Epoch 452/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0868 - val_loss: 0.5792\n",
      "Epoch 453/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1546 - val_loss: 0.5689\n",
      "Epoch 454/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1636 - val_loss: 0.6748\n",
      "Epoch 455/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1382 - val_loss: 0.6670\n",
      "Epoch 456/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1352 - val_loss: 0.5232\n",
      "Epoch 457/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1876 - val_loss: 0.7670\n",
      "Epoch 458/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1828 - val_loss: 0.5281\n",
      "Epoch 459/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0848 - val_loss: 0.5686\n",
      "Epoch 460/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0939 - val_loss: 0.5619\n",
      "Epoch 461/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0950 - val_loss: 0.5060\n",
      "Epoch 462/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0934 - val_loss: 0.5836\n",
      "Epoch 463/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0879 - val_loss: 0.6957\n",
      "Epoch 464/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1914 - val_loss: 0.5992\n",
      "Epoch 465/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1089 - val_loss: 0.5487\n",
      "Epoch 466/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1763 - val_loss: 1.0589\n",
      "Epoch 467/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2025 - val_loss: 0.5165\n",
      "Epoch 468/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1385 - val_loss: 0.7100\n",
      "Epoch 469/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1137 - val_loss: 0.6766\n",
      "Epoch 470/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1066 - val_loss: 0.6758\n",
      "Epoch 471/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0715 - val_loss: 0.6634\n",
      "Epoch 472/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0946 - val_loss: 0.5889\n",
      "Epoch 473/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1258 - val_loss: 0.5657\n",
      "Epoch 474/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0960 - val_loss: 0.5956\n",
      "Epoch 475/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1133 - val_loss: 0.5763\n",
      "Epoch 476/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0844 - val_loss: 0.6625\n",
      "Epoch 477/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0807 - val_loss: 0.5600\n",
      "Epoch 478/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0986 - val_loss: 0.6606\n",
      "Epoch 479/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0803 - val_loss: 0.6810\n",
      "Epoch 480/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0914 - val_loss: 0.6961\n",
      "Epoch 481/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0881 - val_loss: 0.7204\n",
      "Epoch 482/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0857 - val_loss: 0.7176\n",
      "Epoch 483/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0905 - val_loss: 0.6921\n",
      "Epoch 484/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0965 - val_loss: 0.6824\n",
      "Epoch 485/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0786 - val_loss: 0.7383\n",
      "Epoch 486/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1182 - val_loss: 0.5953\n",
      "Epoch 487/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0834 - val_loss: 0.6803\n",
      "Epoch 488/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0754 - val_loss: 0.6188\n",
      "Epoch 489/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1171 - val_loss: 0.7030\n",
      "Epoch 490/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0814 - val_loss: 0.6728\n",
      "Epoch 491/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1042 - val_loss: 0.6922\n",
      "Epoch 492/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0973 - val_loss: 0.6005\n",
      "Epoch 493/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1013 - val_loss: 0.6037\n",
      "Epoch 494/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1698 - val_loss: 0.5348\n",
      "Epoch 495/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1101 - val_loss: 0.6015\n",
      "Epoch 496/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1027 - val_loss: 0.5242\n",
      "Epoch 497/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0909 - val_loss: 0.5821\n",
      "Epoch 498/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0760 - val_loss: 0.6081\n",
      "Epoch 499/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0833 - val_loss: 0.5463\n",
      "Epoch 500/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1947 - val_loss: 0.5700\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 24, 12)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 24, 24)       1800        input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "seq_self_attention_3 (SeqSelfAt (None, 24, 24)       577         bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 24, 24)       0           seq_self_attention_3[0][0]       \n",
      "                                                                 bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_3 (LayerNor (None, 24, 24)       48          add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 576)          0           layer_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 12)           6924        flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 12)           0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 588)          0           dropout_3[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            589         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 1)            2           dense_8[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 9,940\n",
      "Trainable params: 9,940\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Predict time:  0.30213451385498047\n",
      "RMSE:  6.505646323782301\n",
      "RMSE2:  4.298012888589038\n",
      "MAE:  4.124935684601466\n",
      "MAE2:  4.124935684601466\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'mse score')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "opt = optimizers.Adam(lr=0.001,decay = 0.0001)\n",
    "print('Train...')\n",
    "# model.compile(optimizer = opt , loss=\"mse\")\n",
    "model.compile(optimizer = \"adam\" , loss=\"mse\")\n",
    "history = model.fit([x_train,x_train], y_train, epochs = 500, batch_size=8, validation_split=0.1, shuffle=True)\n",
    "# history = model.fit(x_train, y_train, epochs = 500, batch_size=6, validation_split=0.1, shuffle=True)\n",
    "model.summary()\n",
    "#Save Model\n",
    "model.save('GRU_Single_Attention_model_Doric.h5')  # creates a HDF5 file \n",
    "del model\n",
    "\n",
    "custom_ob = {'LayerNormalization': LayerNormalization , 'SeqSelfAttention':SeqSelfAttention}\n",
    "model = load_model('GRU_Single_Attention_model_Doric.h5', custom_objects=custom_ob)\n",
    "t1 = time.time()\n",
    "# y_pred = model.predict([x_test,x_test])\n",
    "y_pred2 = model.predict(x_test)\n",
    "y_pred = model.predict(x_train)\n",
    "t2 = time.time()\n",
    "print('Predict time: ',t2-t1)\n",
    "y_pred = scaler.inverse_transform(y_pred)#Undo scaling\n",
    "rmse_lstm2 = np.sqrt(mean_squared_error(y_test, y_pred2))\n",
    "rmse_lstm = np.sqrt(mean_squared_error(y_train, y_pred))\n",
    "print('RMSE: ',rmse_lstm)\n",
    "print('RMSE2: ',rmse_lstm2)\n",
    "mae = mean_absolute_error(y_test, y_pred2)\n",
    "mae = mean_absolute_error(y_train, y_pred)\n",
    "print('MAE: ',mae)\n",
    "print('MAE2: ',mae)\n",
    "# r22 =  r2_score(y_test, y_pred2)\n",
    "# r2 =  r2_score(y_train, y_pred)\n",
    "# print('R-square: ',r2)\n",
    "# print('R-square2: ',r22)\n",
    "\n",
    "# n = len(y_test)\n",
    "# p = 12\n",
    "# Adj_r2 = 1-(1-r2)*(n-1)/(n-p-1)\n",
    "# Adj_r22 = 1-(1-r22)*(n-1)/(n-p-1)\n",
    "# print('Adj R-square: ',Adj_r2)\n",
    "# print('Adj R-square2: ',Adj_r22)\n",
    "\n",
    "plt.plot(history.history[\"loss\"],label=\"loss\")\n",
    "plt.plot(history.history[\"val_loss\"],label=\"val_loss\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"No. Of Epochs\")\n",
    "plt.ylabel(\"mse score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f37aaf3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 108 samples, validate on 12 samples\n",
      "Epoch 1/200\n",
      "108/108 [==============================] - 2s 19ms/step - loss: 2.8949 - val_loss: 1.9718\n",
      "Epoch 2/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 2.8131 - val_loss: 1.8969\n",
      "Epoch 3/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 2.7432 - val_loss: 1.8125\n",
      "Epoch 4/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 2.6520 - val_loss: 1.7386\n",
      "Epoch 5/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 2.5753 - val_loss: 1.6666\n",
      "Epoch 6/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 2.4934 - val_loss: 1.6081\n",
      "Epoch 7/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 2.4518 - val_loss: 1.5611\n",
      "Epoch 8/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 2.3499 - val_loss: 1.5146\n",
      "Epoch 9/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 2.2949 - val_loss: 1.4751\n",
      "Epoch 10/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 2.2205 - val_loss: 1.4430\n",
      "Epoch 11/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 2.1862 - val_loss: 1.4158\n",
      "Epoch 12/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 2.1562 - val_loss: 1.3907\n",
      "Epoch 13/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 2.1121 - val_loss: 1.3704\n",
      "Epoch 14/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 2.0470 - val_loss: 1.3459\n",
      "Epoch 15/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 2.0161 - val_loss: 1.3349\n",
      "Epoch 16/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 2.0038 - val_loss: 1.3204\n",
      "Epoch 17/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.9504 - val_loss: 1.2975\n",
      "Epoch 18/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.8956 - val_loss: 1.2888\n",
      "Epoch 19/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.9738 - val_loss: 1.2629\n",
      "Epoch 20/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.8984 - val_loss: 1.2468\n",
      "Epoch 21/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.8809 - val_loss: 1.2390\n",
      "Epoch 22/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.8575 - val_loss: 1.2107\n",
      "Epoch 23/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.7789 - val_loss: 1.1947\n",
      "Epoch 24/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.7979 - val_loss: 1.1886\n",
      "Epoch 25/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.6759 - val_loss: 1.1814\n",
      "Epoch 26/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 1.6857 - val_loss: 1.1755\n",
      "Epoch 27/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 1.7590 - val_loss: 1.1342\n",
      "Epoch 28/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.6489 - val_loss: 1.1280\n",
      "Epoch 29/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.7368 - val_loss: 1.0944\n",
      "Epoch 30/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.6475 - val_loss: 1.0856\n",
      "Epoch 31/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.6238 - val_loss: 1.0619\n",
      "Epoch 32/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.6454 - val_loss: 1.0515\n",
      "Epoch 33/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.6355 - val_loss: 1.0204\n",
      "Epoch 34/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.6352 - val_loss: 0.9944\n",
      "Epoch 35/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.5851 - val_loss: 0.9818\n",
      "Epoch 36/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.5800 - val_loss: 0.9796\n",
      "Epoch 37/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.4958 - val_loss: 0.9639\n",
      "Epoch 38/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.6026 - val_loss: 0.9361\n",
      "Epoch 39/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.5554 - val_loss: 0.9430\n",
      "Epoch 40/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.5413 - val_loss: 0.9326\n",
      "Epoch 41/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.5313 - val_loss: 0.9135\n",
      "Epoch 42/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.4942 - val_loss: 0.9043\n",
      "Epoch 43/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.4462 - val_loss: 0.9116\n",
      "Epoch 44/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.3989 - val_loss: 0.8999\n",
      "Epoch 45/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.3917 - val_loss: 0.8768\n",
      "Epoch 46/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.4026 - val_loss: 0.8796\n",
      "Epoch 47/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.4061 - val_loss: 0.8616\n",
      "Epoch 48/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.4333 - val_loss: 0.8588\n",
      "Epoch 49/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.3126 - val_loss: 0.8271\n",
      "Epoch 50/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.4064 - val_loss: 0.8456\n",
      "Epoch 51/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.4302 - val_loss: 0.8207\n",
      "Epoch 52/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.3524 - val_loss: 0.8317\n",
      "Epoch 53/200\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 1.3124 - val_loss: 0.8074\n",
      "Epoch 54/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.3069 - val_loss: 0.7963\n",
      "Epoch 55/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.3907 - val_loss: 0.7935\n",
      "Epoch 56/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.2477 - val_loss: 0.8057\n",
      "Epoch 57/200\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 1.2430 - val_loss: 0.8068\n",
      "Epoch 58/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.3551 - val_loss: 0.7742\n",
      "Epoch 59/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.2522 - val_loss: 0.7814\n",
      "Epoch 60/200\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 1.2414 - val_loss: 0.7545\n",
      "Epoch 61/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.2351 - val_loss: 0.7827\n",
      "Epoch 62/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.2223 - val_loss: 0.7765\n",
      "Epoch 63/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.1365 - val_loss: 0.7916\n",
      "Epoch 64/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.2090 - val_loss: 0.7489\n",
      "Epoch 65/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.1608 - val_loss: 0.7249\n",
      "Epoch 66/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.1762 - val_loss: 0.7359\n",
      "Epoch 67/200\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 1.1724 - val_loss: 0.7225\n",
      "Epoch 68/200\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 1.1694 - val_loss: 0.7286\n",
      "Epoch 69/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.2142 - val_loss: 0.7348\n",
      "Epoch 70/200\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 1.0697 - val_loss: 0.7118\n",
      "Epoch 71/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.0834 - val_loss: 0.7712\n",
      "Epoch 72/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.0126 - val_loss: 0.7271\n",
      "Epoch 73/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.0194 - val_loss: 0.7388\n",
      "Epoch 74/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.9733 - val_loss: 0.7953\n",
      "Epoch 75/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.0099 - val_loss: 0.7230\n",
      "Epoch 76/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.0301 - val_loss: 0.7363\n",
      "Epoch 77/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.9563 - val_loss: 0.7143\n",
      "Epoch 78/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.0208 - val_loss: 0.7017\n",
      "Epoch 79/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.9338 - val_loss: 0.6825\n",
      "Epoch 80/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8860 - val_loss: 0.7307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.9387 - val_loss: 0.7000\n",
      "Epoch 82/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.9421 - val_loss: 0.6980\n",
      "Epoch 83/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8373 - val_loss: 0.7630\n",
      "Epoch 84/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.9451 - val_loss: 0.6844\n",
      "Epoch 85/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8953 - val_loss: 0.7135\n",
      "Epoch 86/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.9072 - val_loss: 0.6714\n",
      "Epoch 87/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8375 - val_loss: 0.7267\n",
      "Epoch 88/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7849 - val_loss: 0.7341\n",
      "Epoch 89/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8787 - val_loss: 0.6712\n",
      "Epoch 90/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8371 - val_loss: 0.7555\n",
      "Epoch 91/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.9118 - val_loss: 0.7877\n",
      "Epoch 92/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8626 - val_loss: 0.7901\n",
      "Epoch 93/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8049 - val_loss: 0.7625\n",
      "Epoch 94/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8815 - val_loss: 0.7249\n",
      "Epoch 95/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8530 - val_loss: 0.6721\n",
      "Epoch 96/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7998 - val_loss: 0.6604\n",
      "Epoch 97/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8492 - val_loss: 0.6683\n",
      "Epoch 98/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7932 - val_loss: 0.7466\n",
      "Epoch 99/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7498 - val_loss: 0.6748\n",
      "Epoch 100/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8153 - val_loss: 0.7073\n",
      "Epoch 101/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8177 - val_loss: 0.6945\n",
      "Epoch 102/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7864 - val_loss: 0.7327\n",
      "Epoch 103/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7456 - val_loss: 0.6861\n",
      "Epoch 104/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.7439 - val_loss: 0.6606\n",
      "Epoch 105/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.7596 - val_loss: 0.7111\n",
      "Epoch 106/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7703 - val_loss: 0.6653\n",
      "Epoch 107/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7817 - val_loss: 0.7185\n",
      "Epoch 108/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7765 - val_loss: 0.7479\n",
      "Epoch 109/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.7990 - val_loss: 0.6592\n",
      "Epoch 110/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8497 - val_loss: 0.6650\n",
      "Epoch 111/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7799 - val_loss: 0.7547\n",
      "Epoch 112/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8260 - val_loss: 0.6906\n",
      "Epoch 113/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7059 - val_loss: 0.6890\n",
      "Epoch 114/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7573 - val_loss: 0.7299\n",
      "Epoch 115/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7854 - val_loss: 0.6897\n",
      "Epoch 116/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.7216 - val_loss: 0.6213\n",
      "Epoch 117/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.7365 - val_loss: 0.6159\n",
      "Epoch 118/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7926 - val_loss: 0.6358\n",
      "Epoch 119/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6704 - val_loss: 0.7147\n",
      "Epoch 120/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.7640 - val_loss: 0.6470\n",
      "Epoch 121/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.7198 - val_loss: 0.6643\n",
      "Epoch 122/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7331 - val_loss: 0.7595\n",
      "Epoch 123/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7708 - val_loss: 0.7195\n",
      "Epoch 124/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6516 - val_loss: 0.6570\n",
      "Epoch 125/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.7168 - val_loss: 0.6491\n",
      "Epoch 126/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7080 - val_loss: 0.6312\n",
      "Epoch 127/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.7389 - val_loss: 0.6807\n",
      "Epoch 128/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7824 - val_loss: 0.6763\n",
      "Epoch 129/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6547 - val_loss: 0.6950\n",
      "Epoch 130/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7114 - val_loss: 0.6331\n",
      "Epoch 131/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7501 - val_loss: 0.5902\n",
      "Epoch 132/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7575 - val_loss: 0.6809\n",
      "Epoch 133/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.7175 - val_loss: 0.7090\n",
      "Epoch 134/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6869 - val_loss: 0.6790\n",
      "Epoch 135/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7070 - val_loss: 0.6009\n",
      "Epoch 136/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6443 - val_loss: 0.6038\n",
      "Epoch 137/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6884 - val_loss: 0.5948\n",
      "Epoch 138/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6975 - val_loss: 0.5953\n",
      "Epoch 139/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7134 - val_loss: 0.6417\n",
      "Epoch 140/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6562 - val_loss: 0.5899\n",
      "Epoch 141/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6631 - val_loss: 0.6075\n",
      "Epoch 142/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6774 - val_loss: 0.6409\n",
      "Epoch 143/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6459 - val_loss: 0.5974\n",
      "Epoch 144/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6511 - val_loss: 0.6263\n",
      "Epoch 145/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6422 - val_loss: 0.6321\n",
      "Epoch 146/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6687 - val_loss: 0.5701\n",
      "Epoch 147/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6747 - val_loss: 0.5887\n",
      "Epoch 148/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6029 - val_loss: 0.5651\n",
      "Epoch 149/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.7218 - val_loss: 0.5725\n",
      "Epoch 150/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6568 - val_loss: 0.5520\n",
      "Epoch 151/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6961 - val_loss: 0.5941\n",
      "Epoch 152/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6373 - val_loss: 0.5985\n",
      "Epoch 153/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6340 - val_loss: 0.5771\n",
      "Epoch 154/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6842 - val_loss: 0.6639\n",
      "Epoch 155/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6794 - val_loss: 0.6042\n",
      "Epoch 156/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5910 - val_loss: 0.5899\n",
      "Epoch 157/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6612 - val_loss: 0.5742\n",
      "Epoch 158/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6454 - val_loss: 0.5739\n",
      "Epoch 159/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6203 - val_loss: 0.5719\n",
      "Epoch 160/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6685 - val_loss: 0.5418\n",
      "Epoch 161/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6364 - val_loss: 0.5811\n",
      "Epoch 162/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5991 - val_loss: 0.5525\n",
      "Epoch 163/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7194 - val_loss: 0.6623\n",
      "Epoch 164/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6431 - val_loss: 0.5401\n",
      "Epoch 165/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5874 - val_loss: 0.5981\n",
      "Epoch 166/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5719 - val_loss: 0.5776\n",
      "Epoch 167/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6308 - val_loss: 0.6279\n",
      "Epoch 168/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5672 - val_loss: 0.5271\n",
      "Epoch 169/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5965 - val_loss: 0.5580\n",
      "Epoch 170/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6208 - val_loss: 0.6289\n",
      "Epoch 171/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5856 - val_loss: 0.5696\n",
      "Epoch 172/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6265 - val_loss: 0.6204\n",
      "Epoch 173/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5948 - val_loss: 0.5920\n",
      "Epoch 174/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5129 - val_loss: 0.5573\n",
      "Epoch 175/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6238 - val_loss: 0.6745\n",
      "Epoch 176/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6312 - val_loss: 0.5296\n",
      "Epoch 177/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6341 - val_loss: 0.5527\n",
      "Epoch 178/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6265 - val_loss: 0.5334\n",
      "Epoch 179/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5390 - val_loss: 0.6350\n",
      "Epoch 180/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5742 - val_loss: 0.5672\n",
      "Epoch 181/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6438 - val_loss: 0.6064\n",
      "Epoch 182/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6034 - val_loss: 0.5814\n",
      "Epoch 183/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6327 - val_loss: 0.5671\n",
      "Epoch 184/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5733 - val_loss: 0.6237\n",
      "Epoch 185/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5880 - val_loss: 0.6568\n",
      "Epoch 186/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7174 - val_loss: 0.5798\n",
      "Epoch 187/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6472 - val_loss: 0.5576\n",
      "Epoch 188/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5693 - val_loss: 0.5964\n",
      "Epoch 189/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5466 - val_loss: 0.5217\n",
      "Epoch 190/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6417 - val_loss: 0.5305\n",
      "Epoch 191/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5836 - val_loss: 0.5787\n",
      "Epoch 192/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5862 - val_loss: 0.5253\n",
      "Epoch 193/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5731 - val_loss: 0.5626\n",
      "Epoch 194/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6018 - val_loss: 0.5504\n",
      "Epoch 195/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5185 - val_loss: 0.5298\n",
      "Epoch 196/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5885 - val_loss: 0.5253\n",
      "Epoch 197/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5922 - val_loss: 0.5942\n",
      "Epoch 198/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5369 - val_loss: 0.5660\n",
      "Epoch 199/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5240 - val_loss: 0.5341\n",
      "Epoch 200/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6354 - val_loss: 0.5486\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_6 (Bidirection (None, 24, 12)            684       \n",
      "_________________________________________________________________\n",
      "layer_normalization_5 (Layer (None, 24, 12)            24        \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 12)                684       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 1,405\n",
      "Trainable params: 1,405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Saved\n",
      "Predict time:  0.566483736038208\n",
      "RMSE:  5.437687835633065\n",
      "RMSE2:  4.476408021014097\n",
      "MAE:  4.125387244025867\n",
      "MAE2:  4.125387244025867\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'mse score')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "init = glorot_normal(seed=None) # 給 LSTM\n",
    "init_d = RandomUniform(minval=-0.05, maxval=0.05) # 給 Dense layer\n",
    "nadam = optimizers.Nadam(lr=0.0015,clipvalue=0.5)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(GRU(6, kernel_initializer=init ,return_sequences = True,kernel_regularizer=regularizers.l2(0.01)\n",
    "                             ,recurrent_regularizer = regularizers.l2(0.01) ,input_shape=(x_train.shape[1],x_train.shape[2]))))\n",
    "model.add(LayerNormalization())\n",
    "model.add(Bidirectional(GRU(6,kernel_initializer=init,kernel_regularizer=regularizers.l2(0.01),recurrent_regularizer = regularizers.l2(0.01))))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(units=1, kernel_initializer=init_d))\n",
    "model.compile(optimizer = nadam , loss=\"mse\")\n",
    "history = model.fit(x_train, y_train, epochs=200, batch_size=24, validation_split=0.1, shuffle=True)\n",
    "#model summary\n",
    "model.summary()\n",
    "#Save Model\n",
    "model.save('GRU_model_doric.h5')  # creates a HDF5 file \n",
    "print('Model Saved')\n",
    "del model  # deletes the existing model\n",
    "\n",
    "custom_ob = {'LayerNormalization': LayerNormalization , 'SeqSelfAttention':SeqSelfAttention}\n",
    "model = load_model('GRU_model_doric.h5', custom_objects=custom_ob)\n",
    "t1 = time.time()\n",
    "# y_pred = model.predict([x_test,x_test])\n",
    "y_pred2 = model.predict(x_test)\n",
    "y_pred = model.predict(x_train)\n",
    "t2 = time.time()\n",
    "print('Predict time: ',t2-t1)\n",
    "y_pred = scaler.inverse_transform(y_pred)#Undo scaling\n",
    "rmse_lstm2 = np.sqrt(mean_squared_error(y_test, y_pred2))\n",
    "rmse_lstm = np.sqrt(mean_squared_error(y_train, y_pred))\n",
    "print('RMSE: ',rmse_lstm)\n",
    "print('RMSE2: ',rmse_lstm2)\n",
    "mae = mean_absolute_error(y_test, y_pred2)\n",
    "mae = mean_absolute_error(y_train, y_pred)\n",
    "print('MAE: ',mae)\n",
    "print('MAE2: ',mae)\n",
    "# r22 =  r2_score(y_test, y_pred2)\n",
    "# r2 =  r2_score(y_train, y_pred)\n",
    "# print('R-square: ',r2)\n",
    "# print('R-square2: ',r22)\n",
    "\n",
    "# n = len(y_test)\n",
    "# p = 12\n",
    "# Adj_r2 = 1-(1-r2)*(n-1)/(n-p-1)\n",
    "# Adj_r22 = 1-(1-r22)*(n-1)/(n-p-1)\n",
    "# print('Adj R-square: ',Adj_r2)\n",
    "# print('Adj R-square2: ',Adj_r22)\n",
    "\n",
    "plt.plot(history.history[\"loss\"],label=\"loss\")\n",
    "plt.plot(history.history[\"val_loss\"],label=\"val_loss\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"No. Of Epochs\")\n",
    "plt.ylabel(\"mse score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664a192e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
