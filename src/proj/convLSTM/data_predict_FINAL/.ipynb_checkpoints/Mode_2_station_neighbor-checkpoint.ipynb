{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "910fbff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from tcn import TCN\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error, r2_score,mean_absolute_error\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential , load_model , Model\n",
    "from keras.layers import Dense, Dropout , LSTM , Bidirectional ,GRU ,Flatten,Add,BatchNormalization\n",
    "from keras_layer_normalization import LayerNormalization\n",
    "from keras.initializers import  glorot_normal, RandomUniform\n",
    "from keras import optimizers,Input\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bb87f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(735, 16) (662, 16) (97, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:32: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "418ef5321a604b19990f8beccc436e76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/638 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:44: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "633c4dfe04c44bc0927b9a9a63ab948d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:\n",
      "(638, 24, 15) (638,)\n",
      "Test size:\n",
      "(73, 24, 15) (73,)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"Mode2_bike_neighbor.csv\")\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "df = df.set_index(\"timestamp\")\n",
    "#df.head()\n",
    "\n",
    "df[\"hour\"] = df.index.hour\n",
    "df[\"day_of_month\"] = df.index.day\n",
    "df[\"day_of_week\"]  = df.index.dayofweek\n",
    "df[\"month\"] = df.index.month\n",
    "\n",
    "training_data_len = math.ceil(len(df) * 0.9) # taking 90% of data to train and 10% of data to test\n",
    "testing_data_len = len(df) - training_data_len\n",
    "\n",
    "time_steps = 24\n",
    "train, test = df.iloc[0:training_data_len], df.iloc[(training_data_len-time_steps):len(df)]\n",
    "print(df.shape, train.shape, test.shape)\n",
    "train_trans = train[['t1','t2', 'hum', 'wind_speed']].to_numpy()\n",
    "test_trans = test[['t1','t2', 'hum', 'wind_speed']].to_numpy()\n",
    "\n",
    "scaler = RobustScaler() # Handles outliers\n",
    "#scaler = MinMaxScaler(feature_range=(0, 1)) # scale to (0,1)\n",
    "train.loc[:, ['t1','t2','hum', 'wind_speed']]=scaler.fit_transform(train_trans)\n",
    "test.loc[:, ['t1','t2', 'hum', 'wind_speed']]=scaler.fit_transform(test_trans)\n",
    "\n",
    "train['cnt'] = scaler.fit_transform(train[['cnt']])\n",
    "test['cnt'] = scaler.fit_transform(test[['cnt']])\n",
    "\n",
    "#Split the data into x_train and y_train data sets\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in tqdm(range(len(train) - time_steps)):\n",
    "    x_train.append(train.drop(columns='cnt').iloc[i:i + time_steps].to_numpy())\n",
    "    y_train.append(train.loc[:,'cnt'].iloc[i + time_steps])\n",
    "\n",
    "#Convert x_train and y_train to numpy arrays\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "#Create the x_test and y_test data sets\n",
    "x_test = []\n",
    "y_test = df.loc[:,'cnt'].iloc[training_data_len:len(df)]\n",
    "\n",
    "for i in tqdm(range(len(test) - time_steps)):\n",
    "    x_test.append(test.drop(columns='cnt').iloc[i:i + time_steps].to_numpy())\n",
    "    # y_test.append(test.loc[:,'cnt'].iloc[i + time_steps])\n",
    "\n",
    "#Convert x_test and y_test to numpy arrays\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# All 12 columns of the data\n",
    "print('Train size:')\n",
    "print(x_train.shape, y_train.shape)\n",
    "print('Test size:')\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc9a8190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 24, 15)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 24, 32)       3072        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 24, 15)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "seq_self_attention_2 (SeqSelfAt (None, 24, 32)       1025        bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 24, 64)       9216        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 24, 32)       0           seq_self_attention_2[0][0]       \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "seq_self_attention_1 (SeqSelfAt (None, 24, 64)       4097        bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_2 (LayerNor (None, 24, 32)       64          add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 24, 64)       0           seq_self_attention_1[0][0]       \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "seq_self_attention_3 (SeqSelfAt (None, 24, 32)       1025        layer_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 24, 64)       128         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_3 (LayerNor (None, 24, 32)       64          seq_self_attention_3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 1536)         0           layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 768)          0           layer_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           24592       flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           12304       flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 16)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 16)           0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1552)         0           dropout_1[0][0]                  \n",
      "                                                                 flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 784)          0           dropout_2[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            1553        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            785         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 1)            0           dense_2[0][0]                    \n",
      "                                                                 dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            2           add_3[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 57,927\n",
      "Trainable params: 57,927\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Concatenate\n",
    "init = glorot_normal(seed=None) # 給 GRU\n",
    "init_d = RandomUniform(minval=-0.05, maxval=0.05) # 給 Dense layer\n",
    "\n",
    "def Encoder(layer):\n",
    "    shortcut = layer\n",
    "    layer = SeqSelfAttention(\n",
    "                     attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n",
    "                     kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     bias_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     attention_regularizer_weight=0.0001)(layer)\n",
    "    layer = Add()([layer,shortcut])\n",
    "    layer = LayerNormalization()(layer)\n",
    "    layer = Flatten()(layer)\n",
    "    \n",
    "    shortcut2 = layer\n",
    "    layer = Dense(16,kernel_initializer=init_d)(layer)\n",
    "    layer = Dropout(0.5)(layer)\n",
    "    layer = Concatenate()([layer,shortcut2])\n",
    "    output = Dense(1,kernel_initializer=init_d)(layer)\n",
    "    return output\n",
    "\n",
    "def Decoder(layer):\n",
    "    shortcut = layer\n",
    "    layer = SeqSelfAttention(\n",
    "                     attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n",
    "                     kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     bias_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     attention_regularizer_weight=0.0001)(layer)\n",
    "    layer = Add()([layer,shortcut])\n",
    "    layer = LayerNormalization()(layer)\n",
    "    layer = SeqSelfAttention(\n",
    "                     attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n",
    "                     kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     bias_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     attention_regularizer_weight=0.0001)(layer)\n",
    "    layer = LayerNormalization()(layer)\n",
    "    \n",
    "    layer = Flatten()(layer)\n",
    "    shortcut2 = layer\n",
    "    layer = Dense(16,kernel_initializer=init_d)(layer)\n",
    "    layer = Dropout(0.25)(layer)\n",
    "    layer = Concatenate()([layer,shortcut2])\n",
    "    output = Dense(1,kernel_initializer=init_d)(layer)\n",
    "    return output\n",
    "\n",
    "def Bi_GRU(layer,unit):\n",
    "    output = Bidirectional(GRU(unit, dropout=0.25, recurrent_dropout=0.25, return_sequences=True,\n",
    "                            kernel_initializer=init))(layer)\n",
    "    return output\n",
    "\n",
    "#start = Input(shape = (x_train.shape[1],x_train.shape[2]))\n",
    "start = Input(shape = (x_train.shape[1:]))\n",
    "start2 = Input(shape = (x_train.shape[1:]))\n",
    "x = Bi_GRU(start,32)\n",
    "x = Encoder(x)\n",
    "\n",
    "y = Bi_GRU(start2,16)\n",
    "y = Decoder(y)\n",
    "\n",
    "Merge = Add()([x,y])\n",
    "Last = Dense(1)(Merge)\n",
    "model = Model([start,start2] , Last)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a9b2bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 574 samples, validate on 64 samples\n",
      "Epoch 1/500\n",
      "574/574 [==============================] - 4s 7ms/step - loss: 2.4231 - val_loss: 3.7538\n",
      "Epoch 2/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.8804 - val_loss: 1.7426\n",
      "Epoch 3/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.8182 - val_loss: 2.2610\n",
      "Epoch 4/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.7706 - val_loss: 3.6289\n",
      "Epoch 5/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.7096 - val_loss: 4.7827\n",
      "Epoch 6/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.6731 - val_loss: 4.8760\n",
      "Epoch 7/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.6037 - val_loss: 2.7860\n",
      "Epoch 8/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.5483 - val_loss: 3.7958\n",
      "Epoch 9/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.5443 - val_loss: 3.1932\n",
      "Epoch 10/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.4942 - val_loss: 2.8217\n",
      "Epoch 11/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.4589 - val_loss: 3.2387\n",
      "Epoch 12/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.4135 - val_loss: 4.3788\n",
      "Epoch 13/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.3937 - val_loss: 2.6109\n",
      "Epoch 14/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.3879 - val_loss: 3.0139\n",
      "Epoch 15/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.3822 - val_loss: 2.1461\n",
      "Epoch 16/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.3100 - val_loss: 3.5379\n",
      "Epoch 17/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.2976 - val_loss: 2.7433\n",
      "Epoch 18/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.2738 - val_loss: 4.5232\n",
      "Epoch 19/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.2569 - val_loss: 4.4650\n",
      "Epoch 20/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.2446 - val_loss: 3.8393\n",
      "Epoch 21/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.2020 - val_loss: 2.9704\n",
      "Epoch 22/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.1830 - val_loss: 2.7025\n",
      "Epoch 23/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.1561 - val_loss: 3.3774\n",
      "Epoch 24/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.1670 - val_loss: 3.9159\n",
      "Epoch 25/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.1361 - val_loss: 4.4299\n",
      "Epoch 26/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.1448 - val_loss: 6.7745\n",
      "Epoch 27/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.1223 - val_loss: 3.5205\n",
      "Epoch 28/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.0727 - val_loss: 4.2568\n",
      "Epoch 29/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.0866 - val_loss: 5.4170\n",
      "Epoch 30/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.0811 - val_loss: 3.5076\n",
      "Epoch 31/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.0569 - val_loss: 5.8756\n",
      "Epoch 32/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.0432 - val_loss: 6.2494\n",
      "Epoch 33/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.0372 - val_loss: 7.4852\n",
      "Epoch 34/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.9952 - val_loss: 7.6076\n",
      "Epoch 35/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.9988 - val_loss: 5.6151\n",
      "Epoch 36/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.0092 - val_loss: 5.8563\n",
      "Epoch 37/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.9785 - val_loss: 5.8532\n",
      "Epoch 38/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.9281 - val_loss: 5.1747\n",
      "Epoch 39/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.9318 - val_loss: 5.3268\n",
      "Epoch 40/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.9151 - val_loss: 4.6238\n",
      "Epoch 41/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.9229 - val_loss: 4.9037\n",
      "Epoch 42/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.9131 - val_loss: 4.6449\n",
      "Epoch 43/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.9014 - val_loss: 4.5476\n",
      "Epoch 44/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.9036 - val_loss: 5.9229\n",
      "Epoch 45/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.9015 - val_loss: 4.6416\n",
      "Epoch 46/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.8980 - val_loss: 4.2674\n",
      "Epoch 47/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.8630 - val_loss: 3.8916\n",
      "Epoch 48/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.8607 - val_loss: 4.3090\n",
      "Epoch 49/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.8566 - val_loss: 5.7672\n",
      "Epoch 50/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.8638 - val_loss: 4.6054\n",
      "Epoch 51/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.8640 - val_loss: 4.3505\n",
      "Epoch 52/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.8587 - val_loss: 4.5341\n",
      "Epoch 53/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.8266 - val_loss: 5.3864\n",
      "Epoch 54/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.8368 - val_loss: 5.0499\n",
      "Epoch 55/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.8453 - val_loss: 3.2834\n",
      "Epoch 56/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7982 - val_loss: 3.6077\n",
      "Epoch 57/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7956 - val_loss: 3.5211\n",
      "Epoch 58/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7992 - val_loss: 4.0452\n",
      "Epoch 59/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.8168 - val_loss: 3.9856\n",
      "Epoch 60/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7866 - val_loss: 4.0223\n",
      "Epoch 61/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7985 - val_loss: 5.1584\n",
      "Epoch 62/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7716 - val_loss: 5.3301\n",
      "Epoch 63/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7919 - val_loss: 5.2861\n",
      "Epoch 64/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7738 - val_loss: 5.3651\n",
      "Epoch 65/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7628 - val_loss: 5.2101\n",
      "Epoch 66/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7713 - val_loss: 5.0481\n",
      "Epoch 67/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7348 - val_loss: 5.8913\n",
      "Epoch 68/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7158 - val_loss: 6.0276\n",
      "Epoch 69/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7404 - val_loss: 5.0733\n",
      "Epoch 70/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7512 - val_loss: 4.9250\n",
      "Epoch 71/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7288 - val_loss: 5.6194\n",
      "Epoch 72/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6960 - val_loss: 7.0617\n",
      "Epoch 73/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6957 - val_loss: 6.6177\n",
      "Epoch 74/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7207 - val_loss: 4.9258\n",
      "Epoch 75/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7099 - val_loss: 5.5074\n",
      "Epoch 76/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7227 - val_loss: 5.9375\n",
      "Epoch 77/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6839 - val_loss: 5.7707\n",
      "Epoch 78/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6954 - val_loss: 4.5033\n",
      "Epoch 79/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6850 - val_loss: 4.0839\n",
      "Epoch 80/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6628 - val_loss: 4.1430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6859 - val_loss: 3.7135\n",
      "Epoch 82/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7179 - val_loss: 3.4819\n",
      "Epoch 83/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7042 - val_loss: 3.7922\n",
      "Epoch 84/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6679 - val_loss: 4.4097\n",
      "Epoch 85/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6481 - val_loss: 4.3159\n",
      "Epoch 86/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6580 - val_loss: 3.9129\n",
      "Epoch 87/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6703 - val_loss: 4.6251\n",
      "Epoch 88/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6689 - val_loss: 5.0610\n",
      "Epoch 89/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6289 - val_loss: 5.9089\n",
      "Epoch 90/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6242 - val_loss: 5.2131\n",
      "Epoch 91/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6442 - val_loss: 5.6200\n",
      "Epoch 92/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6407 - val_loss: 5.5963\n",
      "Epoch 93/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6240 - val_loss: 6.6023\n",
      "Epoch 94/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6471 - val_loss: 6.8504\n",
      "Epoch 95/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6304 - val_loss: 6.1310\n",
      "Epoch 96/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6327 - val_loss: 4.4842\n",
      "Epoch 97/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6384 - val_loss: 5.2942\n",
      "Epoch 98/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5995 - val_loss: 5.6497\n",
      "Epoch 99/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5997 - val_loss: 5.7520\n",
      "Epoch 100/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6253 - val_loss: 4.5499\n",
      "Epoch 101/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5831 - val_loss: 6.2724\n",
      "Epoch 102/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5962 - val_loss: 6.5891\n",
      "Epoch 103/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5938 - val_loss: 6.8949\n",
      "Epoch 104/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6308 - val_loss: 4.9791\n",
      "Epoch 105/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6013 - val_loss: 5.7400\n",
      "Epoch 106/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6035 - val_loss: 6.3064\n",
      "Epoch 107/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6044 - val_loss: 7.2940\n",
      "Epoch 108/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5720 - val_loss: 6.2399\n",
      "Epoch 109/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5860 - val_loss: 5.9015\n",
      "Epoch 110/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6154 - val_loss: 5.1250\n",
      "Epoch 111/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5585 - val_loss: 4.9808\n",
      "Epoch 112/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5607 - val_loss: 5.4220\n",
      "Epoch 113/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5521 - val_loss: 4.1766\n",
      "Epoch 114/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5692 - val_loss: 4.7329\n",
      "Epoch 115/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5518 - val_loss: 4.9564\n",
      "Epoch 116/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5780 - val_loss: 4.6631\n",
      "Epoch 117/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5673 - val_loss: 4.4240\n",
      "Epoch 118/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5602 - val_loss: 4.1153\n",
      "Epoch 119/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5423 - val_loss: 4.9002\n",
      "Epoch 120/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5243 - val_loss: 4.8978\n",
      "Epoch 121/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5512 - val_loss: 5.0557\n",
      "Epoch 122/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5262 - val_loss: 5.4861\n",
      "Epoch 123/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5628 - val_loss: 6.2809\n",
      "Epoch 124/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5243 - val_loss: 7.3374\n",
      "Epoch 125/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5249 - val_loss: 5.9724\n",
      "Epoch 126/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5216 - val_loss: 5.6804\n",
      "Epoch 127/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5355 - val_loss: 4.2783\n",
      "Epoch 128/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4965 - val_loss: 5.8987\n",
      "Epoch 129/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5001 - val_loss: 6.4991\n",
      "Epoch 130/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5001 - val_loss: 7.1917\n",
      "Epoch 131/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5210 - val_loss: 7.3927\n",
      "Epoch 132/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5348 - val_loss: 6.7204\n",
      "Epoch 133/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5045 - val_loss: 5.4491\n",
      "Epoch 134/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4982 - val_loss: 7.1543\n",
      "Epoch 135/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5129 - val_loss: 7.1386\n",
      "Epoch 136/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5118 - val_loss: 5.6703\n",
      "Epoch 137/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4861 - val_loss: 6.6255\n",
      "Epoch 138/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4685 - val_loss: 6.9968\n",
      "Epoch 139/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4716 - val_loss: 7.4503\n",
      "Epoch 140/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4835 - val_loss: 4.5595\n",
      "Epoch 141/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5072 - val_loss: 4.0305\n",
      "Epoch 142/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4784 - val_loss: 4.4050\n",
      "Epoch 143/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4789 - val_loss: 5.8202\n",
      "Epoch 144/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4791 - val_loss: 5.5452\n",
      "Epoch 145/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4618 - val_loss: 5.4934\n",
      "Epoch 146/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4751 - val_loss: 6.7560\n",
      "Epoch 147/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4626 - val_loss: 5.9799\n",
      "Epoch 148/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4650 - val_loss: 6.4107\n",
      "Epoch 149/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4417 - val_loss: 5.4125\n",
      "Epoch 150/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4329 - val_loss: 5.6127\n",
      "Epoch 151/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4328 - val_loss: 6.3268\n",
      "Epoch 152/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4509 - val_loss: 5.1123\n",
      "Epoch 153/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4605 - val_loss: 5.1897\n",
      "Epoch 154/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4546 - val_loss: 4.5230\n",
      "Epoch 155/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4503 - val_loss: 4.9734\n",
      "Epoch 156/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4386 - val_loss: 4.6033\n",
      "Epoch 157/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4159 - val_loss: 3.9735\n",
      "Epoch 158/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4495 - val_loss: 4.0631\n",
      "Epoch 159/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4232 - val_loss: 4.0478\n",
      "Epoch 160/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4341 - val_loss: 3.4269\n",
      "Epoch 161/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4242 - val_loss: 2.9523\n",
      "Epoch 162/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4000 - val_loss: 3.1423\n",
      "Epoch 163/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4294 - val_loss: 4.1327\n",
      "Epoch 164/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4277 - val_loss: 3.8898\n",
      "Epoch 165/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4141 - val_loss: 3.6286\n",
      "Epoch 166/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4190 - val_loss: 3.2784\n",
      "Epoch 167/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4018 - val_loss: 3.4553\n",
      "Epoch 168/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3982 - val_loss: 3.7244\n",
      "Epoch 169/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3950 - val_loss: 4.1338\n",
      "Epoch 170/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4040 - val_loss: 4.3119\n",
      "Epoch 171/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3990 - val_loss: 3.7515\n",
      "Epoch 172/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4072 - val_loss: 4.1287\n",
      "Epoch 173/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3864 - val_loss: 4.5397\n",
      "Epoch 174/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3811 - val_loss: 4.3355\n",
      "Epoch 175/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4053 - val_loss: 3.7234\n",
      "Epoch 176/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4262 - val_loss: 3.2849\n",
      "Epoch 177/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3763 - val_loss: 3.5193\n",
      "Epoch 178/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3796 - val_loss: 3.9317\n",
      "Epoch 179/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3831 - val_loss: 4.1099\n",
      "Epoch 180/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3801 - val_loss: 3.9759\n",
      "Epoch 181/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3943 - val_loss: 4.1402\n",
      "Epoch 182/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3845 - val_loss: 3.8691\n",
      "Epoch 183/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3899 - val_loss: 3.4532\n",
      "Epoch 184/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3676 - val_loss: 4.1296\n",
      "Epoch 185/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3893 - val_loss: 4.1339\n",
      "Epoch 186/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3694 - val_loss: 4.2474\n",
      "Epoch 187/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3681 - val_loss: 3.7284\n",
      "Epoch 188/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3871 - val_loss: 2.8378\n",
      "Epoch 189/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3857 - val_loss: 3.2223\n",
      "Epoch 190/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3639 - val_loss: 4.3226\n",
      "Epoch 191/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3576 - val_loss: 4.1177\n",
      "Epoch 192/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3558 - val_loss: 3.9847\n",
      "Epoch 193/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3531 - val_loss: 4.9876\n",
      "Epoch 194/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3639 - val_loss: 4.0423\n",
      "Epoch 195/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3502 - val_loss: 3.8100\n",
      "Epoch 196/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3619 - val_loss: 3.8290\n",
      "Epoch 197/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3737 - val_loss: 3.3324\n",
      "Epoch 198/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3875 - val_loss: 3.3259\n",
      "Epoch 199/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3472 - val_loss: 3.7167\n",
      "Epoch 200/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3553 - val_loss: 3.7776\n",
      "Epoch 201/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3538 - val_loss: 3.6949\n",
      "Epoch 202/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3318 - val_loss: 4.0631\n",
      "Epoch 203/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3537 - val_loss: 3.9855\n",
      "Epoch 204/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3444 - val_loss: 4.7363\n",
      "Epoch 205/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3514 - val_loss: 4.6066\n",
      "Epoch 206/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3346 - val_loss: 4.6592\n",
      "Epoch 207/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3605 - val_loss: 4.1466\n",
      "Epoch 208/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3356 - val_loss: 4.2254\n",
      "Epoch 209/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3213 - val_loss: 3.3981\n",
      "Epoch 210/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3467 - val_loss: 3.7242\n",
      "Epoch 211/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3330 - val_loss: 3.6406\n",
      "Epoch 212/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3245 - val_loss: 3.3071\n",
      "Epoch 213/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3501 - val_loss: 3.3640\n",
      "Epoch 214/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3252 - val_loss: 3.2960\n",
      "Epoch 215/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3158 - val_loss: 3.6485\n",
      "Epoch 216/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3429 - val_loss: 3.5938\n",
      "Epoch 217/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3145 - val_loss: 3.7634\n",
      "Epoch 218/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3498 - val_loss: 3.8712\n",
      "Epoch 219/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3207 - val_loss: 3.4703\n",
      "Epoch 220/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3401 - val_loss: 3.6147\n",
      "Epoch 221/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3380 - val_loss: 2.7886\n",
      "Epoch 222/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3176 - val_loss: 3.2642\n",
      "Epoch 223/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3154 - val_loss: 2.2499\n",
      "Epoch 224/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3371 - val_loss: 2.9351\n",
      "Epoch 225/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3117 - val_loss: 3.0722\n",
      "Epoch 226/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3159 - val_loss: 3.2210\n",
      "Epoch 227/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2866 - val_loss: 3.1675\n",
      "Epoch 228/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3193 - val_loss: 4.0083\n",
      "Epoch 229/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3093 - val_loss: 2.7792\n",
      "Epoch 230/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3162 - val_loss: 2.7816\n",
      "Epoch 231/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3051 - val_loss: 2.7927\n",
      "Epoch 232/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3162 - val_loss: 2.8389\n",
      "Epoch 233/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3081 - val_loss: 2.2047\n",
      "Epoch 234/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3344 - val_loss: 2.5791\n",
      "Epoch 235/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2751 - val_loss: 2.5612\n",
      "Epoch 236/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2738 - val_loss: 2.3721\n",
      "Epoch 237/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3086 - val_loss: 2.7888\n",
      "Epoch 238/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3247 - val_loss: 3.1621\n",
      "Epoch 239/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3212 - val_loss: 2.7450\n",
      "Epoch 240/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2918 - val_loss: 2.7166\n",
      "Epoch 241/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2839 - val_loss: 3.7980\n",
      "Epoch 242/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3241 - val_loss: 2.8336\n",
      "Epoch 243/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3011 - val_loss: 3.6800\n",
      "Epoch 244/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3031 - val_loss: 3.1564\n",
      "Epoch 245/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2858 - val_loss: 2.7314\n",
      "Epoch 246/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2869 - val_loss: 2.4353\n",
      "Epoch 247/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2775 - val_loss: 2.2918\n",
      "Epoch 248/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2855 - val_loss: 2.7144\n",
      "Epoch 249/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2760 - val_loss: 2.3040\n",
      "Epoch 250/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2766 - val_loss: 2.2767\n",
      "Epoch 251/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2756 - val_loss: 1.7991\n",
      "Epoch 252/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2739 - val_loss: 2.6770\n",
      "Epoch 253/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2946 - val_loss: 2.6121\n",
      "Epoch 254/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2682 - val_loss: 2.2506\n",
      "Epoch 255/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2657 - val_loss: 2.4728\n",
      "Epoch 256/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2846 - val_loss: 2.1346\n",
      "Epoch 257/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2820 - val_loss: 2.4308\n",
      "Epoch 258/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2914 - val_loss: 2.1937\n",
      "Epoch 259/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2932 - val_loss: 1.6605\n",
      "Epoch 260/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2863 - val_loss: 1.8680\n",
      "Epoch 261/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2924 - val_loss: 1.9370\n",
      "Epoch 262/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2948 - val_loss: 1.8866\n",
      "Epoch 263/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2743 - val_loss: 1.7903\n",
      "Epoch 264/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2856 - val_loss: 1.8643\n",
      "Epoch 265/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2612 - val_loss: 2.1647\n",
      "Epoch 266/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2720 - val_loss: 1.9127\n",
      "Epoch 267/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2739 - val_loss: 2.0093\n",
      "Epoch 268/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2805 - val_loss: 2.1331\n",
      "Epoch 269/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2856 - val_loss: 1.7188\n",
      "Epoch 270/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2528 - val_loss: 2.0754\n",
      "Epoch 271/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2606 - val_loss: 2.1207\n",
      "Epoch 272/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2781 - val_loss: 1.6922\n",
      "Epoch 273/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2676 - val_loss: 1.9436\n",
      "Epoch 274/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2595 - val_loss: 2.1028\n",
      "Epoch 275/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2520 - val_loss: 1.5074\n",
      "Epoch 276/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2799 - val_loss: 1.7503\n",
      "Epoch 277/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2486 - val_loss: 1.7959\n",
      "Epoch 278/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2558 - val_loss: 1.7830\n",
      "Epoch 279/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2654 - val_loss: 1.3226\n",
      "Epoch 280/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2485 - val_loss: 1.4746\n",
      "Epoch 281/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2562 - val_loss: 1.2179\n",
      "Epoch 282/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2533 - val_loss: 1.0804\n",
      "Epoch 283/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2444 - val_loss: 1.2106\n",
      "Epoch 284/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2728 - val_loss: 1.3424\n",
      "Epoch 285/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2482 - val_loss: 1.3635\n",
      "Epoch 286/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2258 - val_loss: 1.3018\n",
      "Epoch 287/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2555 - val_loss: 1.1518\n",
      "Epoch 288/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2360 - val_loss: 1.0952\n",
      "Epoch 289/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2330 - val_loss: 1.1850\n",
      "Epoch 290/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2454 - val_loss: 1.5022\n",
      "Epoch 291/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2285 - val_loss: 1.1927\n",
      "Epoch 292/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2526 - val_loss: 1.5561\n",
      "Epoch 293/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2478 - val_loss: 1.5211\n",
      "Epoch 294/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2344 - val_loss: 1.3449\n",
      "Epoch 295/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2261 - val_loss: 1.8358\n",
      "Epoch 296/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2324 - val_loss: 1.7042\n",
      "Epoch 297/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2327 - val_loss: 1.8042\n",
      "Epoch 298/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2545 - val_loss: 1.2979\n",
      "Epoch 299/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2633 - val_loss: 1.1434\n",
      "Epoch 300/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2274 - val_loss: 1.8265\n",
      "Epoch 301/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2434 - val_loss: 1.5242\n",
      "Epoch 302/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2401 - val_loss: 1.3763\n",
      "Epoch 303/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2289 - val_loss: 1.6574\n",
      "Epoch 304/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2296 - val_loss: 1.7093\n",
      "Epoch 305/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2393 - val_loss: 1.5744\n",
      "Epoch 306/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2476 - val_loss: 1.1622\n",
      "Epoch 307/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2273 - val_loss: 1.1687\n",
      "Epoch 308/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2200 - val_loss: 1.2694\n",
      "Epoch 309/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2469 - val_loss: 1.3848\n",
      "Epoch 310/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2084 - val_loss: 1.4287\n",
      "Epoch 311/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2347 - val_loss: 1.3761\n",
      "Epoch 312/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2325 - val_loss: 1.4592\n",
      "Epoch 313/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2221 - val_loss: 1.6443\n",
      "Epoch 314/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2266 - val_loss: 1.3905\n",
      "Epoch 315/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2361 - val_loss: 1.6679\n",
      "Epoch 316/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2227 - val_loss: 1.6085\n",
      "Epoch 317/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2306 - val_loss: 1.5410\n",
      "Epoch 318/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2086 - val_loss: 1.4583\n",
      "Epoch 319/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2221 - val_loss: 1.1660\n",
      "Epoch 320/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2306 - val_loss: 1.0505\n",
      "Epoch 321/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2267 - val_loss: 1.0396\n",
      "Epoch 322/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2252 - val_loss: 1.1829\n",
      "Epoch 323/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2126 - val_loss: 1.4417\n",
      "Epoch 324/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2187 - val_loss: 1.5699\n",
      "Epoch 325/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2212 - val_loss: 1.1927\n",
      "Epoch 326/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2129 - val_loss: 1.2038\n",
      "Epoch 327/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2232 - val_loss: 1.2137\n",
      "Epoch 328/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2357 - val_loss: 1.1728\n",
      "Epoch 329/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2243 - val_loss: 0.9912\n",
      "Epoch 330/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2189 - val_loss: 1.2928\n",
      "Epoch 331/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2162 - val_loss: 1.2354\n",
      "Epoch 332/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2194 - val_loss: 1.0416\n",
      "Epoch 333/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2119 - val_loss: 1.3157\n",
      "Epoch 334/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2097 - val_loss: 1.1709\n",
      "Epoch 335/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2132 - val_loss: 1.0276\n",
      "Epoch 336/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2027 - val_loss: 1.0576\n",
      "Epoch 337/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2237 - val_loss: 1.2808\n",
      "Epoch 338/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1978 - val_loss: 0.9884\n",
      "Epoch 339/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2011 - val_loss: 0.9842\n",
      "Epoch 340/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2143 - val_loss: 1.0226\n",
      "Epoch 341/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2243 - val_loss: 1.1790\n",
      "Epoch 342/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2192 - val_loss: 1.0917\n",
      "Epoch 343/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2043 - val_loss: 1.1048\n",
      "Epoch 344/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2226 - val_loss: 1.3268\n",
      "Epoch 345/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2026 - val_loss: 1.0748\n",
      "Epoch 346/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2176 - val_loss: 1.2604\n",
      "Epoch 347/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2124 - val_loss: 1.5533\n",
      "Epoch 348/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2481 - val_loss: 1.3511\n",
      "Epoch 349/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2508 - val_loss: 1.3598\n",
      "Epoch 350/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2187 - val_loss: 1.4029\n",
      "Epoch 351/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2429 - val_loss: 0.8437\n",
      "Epoch 352/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2408 - val_loss: 1.1152\n",
      "Epoch 353/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2067 - val_loss: 1.2357\n",
      "Epoch 354/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2130 - val_loss: 1.5768\n",
      "Epoch 355/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2254 - val_loss: 0.9969\n",
      "Epoch 356/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2105 - val_loss: 0.9274\n",
      "Epoch 357/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2049 - val_loss: 0.9668\n",
      "Epoch 358/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2004 - val_loss: 1.2120\n",
      "Epoch 359/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1973 - val_loss: 1.1489\n",
      "Epoch 360/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2034 - val_loss: 1.3143\n",
      "Epoch 361/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2007 - val_loss: 1.0732\n",
      "Epoch 362/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2000 - val_loss: 1.2370\n",
      "Epoch 363/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1968 - val_loss: 0.9380\n",
      "Epoch 364/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2002 - val_loss: 0.8897\n",
      "Epoch 365/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2049 - val_loss: 1.0492\n",
      "Epoch 366/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1975 - val_loss: 0.8193\n",
      "Epoch 367/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1879 - val_loss: 1.0064\n",
      "Epoch 368/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1949 - val_loss: 1.0454\n",
      "Epoch 369/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2027 - val_loss: 0.7285\n",
      "Epoch 370/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2041 - val_loss: 1.1176\n",
      "Epoch 371/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1967 - val_loss: 1.1979\n",
      "Epoch 372/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1984 - val_loss: 1.0581\n",
      "Epoch 373/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2046 - val_loss: 1.1939\n",
      "Epoch 374/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2078 - val_loss: 1.2279\n",
      "Epoch 375/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1932 - val_loss: 0.9793\n",
      "Epoch 376/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1909 - val_loss: 0.8424\n",
      "Epoch 377/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1953 - val_loss: 0.9059\n",
      "Epoch 378/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1789 - val_loss: 0.9946\n",
      "Epoch 379/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2067 - val_loss: 0.9761\n",
      "Epoch 380/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1786 - val_loss: 0.8907\n",
      "Epoch 381/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2046 - val_loss: 0.9501\n",
      "Epoch 382/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1924 - val_loss: 1.1363\n",
      "Epoch 383/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2002 - val_loss: 1.0203\n",
      "Epoch 384/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1979 - val_loss: 0.7852\n",
      "Epoch 385/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1917 - val_loss: 1.1286\n",
      "Epoch 386/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1861 - val_loss: 1.4040\n",
      "Epoch 387/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2040 - val_loss: 0.9858\n",
      "Epoch 388/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1886 - val_loss: 0.9061\n",
      "Epoch 389/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1947 - val_loss: 1.0324\n",
      "Epoch 390/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1834 - val_loss: 0.8395\n",
      "Epoch 391/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1830 - val_loss: 0.8453\n",
      "Epoch 392/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1947 - val_loss: 0.9903\n",
      "Epoch 393/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1837 - val_loss: 0.9296\n",
      "Epoch 394/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2091 - val_loss: 0.7442\n",
      "Epoch 395/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2055 - val_loss: 0.7841\n",
      "Epoch 396/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1972 - val_loss: 0.8248\n",
      "Epoch 397/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1980 - val_loss: 1.0445\n",
      "Epoch 398/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2043 - val_loss: 0.8210\n",
      "Epoch 399/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1963 - val_loss: 0.7003\n",
      "Epoch 400/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1860 - val_loss: 0.9153\n",
      "Epoch 401/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1767 - val_loss: 0.8869\n",
      "Epoch 402/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1835 - val_loss: 1.0862\n",
      "Epoch 403/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2034 - val_loss: 0.8245\n",
      "Epoch 404/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1722 - val_loss: 0.8518\n",
      "Epoch 405/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1981 - val_loss: 0.7463\n",
      "Epoch 406/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1942 - val_loss: 0.8159\n",
      "Epoch 407/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1628 - val_loss: 0.7233\n",
      "Epoch 408/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1700 - val_loss: 0.7904\n",
      "Epoch 409/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1908 - val_loss: 0.8988\n",
      "Epoch 410/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1788 - val_loss: 1.0521\n",
      "Epoch 411/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1735 - val_loss: 1.1632\n",
      "Epoch 412/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1712 - val_loss: 1.3400\n",
      "Epoch 413/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1930 - val_loss: 1.2052\n",
      "Epoch 414/500\n",
      "574/574 [==============================] - ETA: 0s - loss: 0.176 - 2s 3ms/step - loss: 0.1742 - val_loss: 0.9464\n",
      "Epoch 415/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1727 - val_loss: 0.9673\n",
      "Epoch 416/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1756 - val_loss: 1.0075\n",
      "Epoch 417/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1631 - val_loss: 0.9275\n",
      "Epoch 418/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1821 - val_loss: 1.1526\n",
      "Epoch 419/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1906 - val_loss: 0.7850\n",
      "Epoch 420/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1890 - val_loss: 0.7078\n",
      "Epoch 421/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1868 - val_loss: 0.8705\n",
      "Epoch 422/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1770 - val_loss: 1.0921\n",
      "Epoch 423/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1748 - val_loss: 1.0684\n",
      "Epoch 424/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1747 - val_loss: 0.7894\n",
      "Epoch 425/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1710 - val_loss: 0.7522\n",
      "Epoch 426/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1788 - val_loss: 0.6385\n",
      "Epoch 427/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1771 - val_loss: 0.8312\n",
      "Epoch 428/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1832 - val_loss: 0.7160\n",
      "Epoch 429/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1656 - val_loss: 0.9496\n",
      "Epoch 430/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1813 - val_loss: 0.7841\n",
      "Epoch 431/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1727 - val_loss: 0.7638\n",
      "Epoch 432/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1598 - val_loss: 0.8930\n",
      "Epoch 433/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1615 - val_loss: 0.9351\n",
      "Epoch 434/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1731 - val_loss: 0.9181\n",
      "Epoch 435/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1678 - val_loss: 0.7734\n",
      "Epoch 436/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1667 - val_loss: 0.7947\n",
      "Epoch 437/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1726 - val_loss: 0.7664\n",
      "Epoch 438/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1589 - val_loss: 0.7310\n",
      "Epoch 439/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1620 - val_loss: 0.5910\n",
      "Epoch 440/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1711 - val_loss: 1.1373\n",
      "Epoch 441/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1727 - val_loss: 0.9533\n",
      "Epoch 442/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1723 - val_loss: 0.8222\n",
      "Epoch 443/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1590 - val_loss: 0.8125\n",
      "Epoch 444/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1574 - val_loss: 0.6236\n",
      "Epoch 445/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1685 - val_loss: 0.7474\n",
      "Epoch 446/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1630 - val_loss: 0.8606\n",
      "Epoch 447/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1615 - val_loss: 0.7165\n",
      "Epoch 448/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1553 - val_loss: 0.7208\n",
      "Epoch 449/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1687 - val_loss: 0.6944\n",
      "Epoch 450/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1525 - val_loss: 0.9747\n",
      "Epoch 451/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1785 - val_loss: 0.7585\n",
      "Epoch 452/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1755 - val_loss: 0.9290\n",
      "Epoch 453/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1771 - val_loss: 0.6072\n",
      "Epoch 454/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1735 - val_loss: 0.6871\n",
      "Epoch 455/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1729 - val_loss: 0.8649\n",
      "Epoch 456/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1597 - val_loss: 0.8854\n",
      "Epoch 457/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1636 - val_loss: 0.8957\n",
      "Epoch 458/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1567 - val_loss: 0.7380\n",
      "Epoch 459/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1610 - val_loss: 0.9357\n",
      "Epoch 460/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1613 - val_loss: 0.9977\n",
      "Epoch 461/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1631 - val_loss: 1.1211\n",
      "Epoch 462/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1579 - val_loss: 0.9642\n",
      "Epoch 463/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1585 - val_loss: 1.1915\n",
      "Epoch 464/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1576 - val_loss: 1.2438\n",
      "Epoch 465/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1525 - val_loss: 0.8255\n",
      "Epoch 466/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1638 - val_loss: 0.9475\n",
      "Epoch 467/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1509 - val_loss: 1.1150\n",
      "Epoch 468/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1606 - val_loss: 1.1712\n",
      "Epoch 469/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1513 - val_loss: 1.0349\n",
      "Epoch 470/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1552 - val_loss: 0.7202\n",
      "Epoch 471/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1462 - val_loss: 0.6907\n",
      "Epoch 472/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1646 - val_loss: 0.9106\n",
      "Epoch 473/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1543 - val_loss: 0.8376\n",
      "Epoch 474/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1624 - val_loss: 1.1691\n",
      "Epoch 475/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1722 - val_loss: 0.9518\n",
      "Epoch 476/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1491 - val_loss: 0.7814\n",
      "Epoch 477/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1576 - val_loss: 0.8344\n",
      "Epoch 478/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1597 - val_loss: 0.7101\n",
      "Epoch 479/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1372 - val_loss: 0.8521\n",
      "Epoch 480/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1543 - val_loss: 0.8584\n",
      "Epoch 481/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1471 - val_loss: 1.0774\n",
      "Epoch 482/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1539 - val_loss: 0.8468\n",
      "Epoch 483/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1786 - val_loss: 0.7752\n",
      "Epoch 484/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1714 - val_loss: 0.7936\n",
      "Epoch 485/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1696 - val_loss: 1.2557\n",
      "Epoch 486/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1564 - val_loss: 0.9519\n",
      "Epoch 487/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1656 - val_loss: 0.9211\n",
      "Epoch 488/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1638 - val_loss: 0.8840\n",
      "Epoch 489/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1395 - val_loss: 0.8216\n",
      "Epoch 490/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1559 - val_loss: 0.9244\n",
      "Epoch 491/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1472 - val_loss: 1.1288\n",
      "Epoch 492/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1505 - val_loss: 0.9420\n",
      "Epoch 493/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1763 - val_loss: 0.8361\n",
      "Epoch 494/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1599 - val_loss: 0.8847\n",
      "Epoch 495/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1503 - val_loss: 0.7113\n",
      "Epoch 496/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1462 - val_loss: 0.9984\n",
      "Epoch 497/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1493 - val_loss: 0.9644\n",
      "Epoch 498/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1586 - val_loss: 0.8563\n",
      "Epoch 499/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1537 - val_loss: 0.6940\n",
      "Epoch 500/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1325 - val_loss: 0.8504\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 24, 15)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 24, 32)       3072        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 24, 15)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "seq_self_attention_2 (SeqSelfAt (None, 24, 32)       1025        bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 24, 64)       9216        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 24, 32)       0           seq_self_attention_2[0][0]       \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "seq_self_attention_1 (SeqSelfAt (None, 24, 64)       4097        bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_2 (LayerNor (None, 24, 32)       64          add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 24, 64)       0           seq_self_attention_1[0][0]       \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "seq_self_attention_3 (SeqSelfAt (None, 24, 32)       1025        layer_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 24, 64)       128         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_3 (LayerNor (None, 24, 32)       64          seq_self_attention_3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 1536)         0           layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 768)          0           layer_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           24592       flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           12304       flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 16)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 16)           0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1552)         0           dropout_1[0][0]                  \n",
      "                                                                 flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 784)          0           dropout_2[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            1553        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            785         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 1)            0           dense_2[0][0]                    \n",
      "                                                                 dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            2           add_3[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 57,927\n",
      "Trainable params: 57,927\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict time:  0.5203068256378174\n",
      "RMSE2:  9.130894557496346\n",
      "MAE2:  6.392448279942858\n",
      "R-square2:  -0.575931171845804\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'mse score')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "opt = optimizers.Adam(lr=0.0009)\n",
    "print('Train...')\n",
    "model.compile(optimizer = opt , loss=\"mse\")\n",
    "#model.compile(optimizer = \"adam\" , loss=\"mse\")\n",
    "history = model.fit([x_train,x_train], y_train, epochs = 500, batch_size=24, validation_split=0.1, shuffle=True)\n",
    "# history = model.fit(x_train, y_train, epochs = 500, batch_size=6, validation_split=0.1, shuffle=True)\n",
    "model.summary()\n",
    "#Save Model\n",
    "model.save('GRU_Single_Attention_model_neighbor.h5')  # creates a HDF5 file \n",
    "del model\n",
    "\n",
    "custom_ob = {'LayerNormalization': LayerNormalization , 'SeqSelfAttention':SeqSelfAttention}\n",
    "model = load_model('GRU_Single_Attention_model_neighbor.h5', custom_objects=custom_ob)\n",
    "t1 = time.time()\n",
    "y_pred2 = model.predict([x_test,x_test])\n",
    "#y_pred2 = model.predict(x_test)\n",
    "#y_pred = model.predict(x_train)\n",
    "t2 = time.time()\n",
    "print('Predict time: ',t2-t1)\n",
    "y_pred = scaler.inverse_transform(y_pred2)#Undo scaling\n",
    "rmse_lstm2 = np.sqrt(mean_squared_error(y_test, y_pred2))\n",
    "#rmse_lstm = np.sqrt(mean_squared_error(y_train, y_pred))\n",
    "#print('RMSE: ',rmse_lstm)\n",
    "print('RMSE2: ',rmse_lstm2)\n",
    "mae2 = mean_absolute_error(y_test, y_pred2)\n",
    "#mae = mean_absolute_error(y_train, y_pred)\n",
    "#print('MAE: ',mae)\n",
    "print('MAE2: ',mae2)\n",
    "r22 =  r2_score(y_test, y_pred2)\n",
    "# r2 =  r2_score(y_train, y_pred)\n",
    "# print('R-square: ',r2)\n",
    "print('R-square2: ',r22)\n",
    "\n",
    "# n = len(y_test)\n",
    "# p = 12\n",
    "# Adj_r2 = 1-(1-r2)*(n-1)/(n-p-1)\n",
    "# Adj_r22 = 1-(1-r22)*(n-1)/(n-p-1)\n",
    "# print('Adj R-square: ',Adj_r2)\n",
    "# print('Adj R-square2: ',Adj_r22)\n",
    "\n",
    "plt.plot(history.history[\"loss\"],label=\"loss\")\n",
    "plt.plot(history.history[\"val_loss\"],label=\"val_loss\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"No. Of Epochs\")\n",
    "plt.ylabel(\"mse score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e327ddf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2990d8ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d3fa12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
