{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "256ccbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tcn import TCN # main library\n",
    "import math\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "#from keras.initializers import  glorot_normal, RandomUniform\n",
    "#from keras import optimizers,regularizers\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.models import load_model \n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2327e957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160, 13) (144, 13) (40, 13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:34: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be19042a4e134c689ffb7ea10c4caecd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:46: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d12fbf0c4bdd4d03b2a58d137ceea84e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:\n",
      "(120, 24, 12) (120,)\n",
      "Test size:\n",
      "(16, 24, 12) (16,)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"station_bike_Doric.csv\")\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "df = df.set_index(\"timestamp\")\n",
    "#df.head()\n",
    "\n",
    "df[\"hour\"] = df.index.hour\n",
    "df[\"day_of_month\"] = df.index.day\n",
    "df[\"day_of_week\"]  = df.index.dayofweek\n",
    "df[\"month\"] = df.index.month\n",
    "\n",
    "training_data_len = math.ceil(len(df) * 0.9) # taking 90% of data to train and 10% of data to test\n",
    "testing_data_len = len(df) - training_data_len\n",
    "\n",
    "time_steps = 24\n",
    "train, test = df.iloc[0:training_data_len], df.iloc[(training_data_len-time_steps):len(df)]\n",
    "print(df.shape, train.shape, test.shape)\n",
    "\n",
    "train_trans = train[['t1','t2', 'hum', 'wind_speed']].to_numpy()\n",
    "test_trans = test[['t1','t2', 'hum', 'wind_speed']].to_numpy()\n",
    "\n",
    "scaler = RobustScaler() # Handles outliers\n",
    "train.loc[:, ['t1','t2','hum', 'wind_speed']]=scaler.fit_transform(train_trans)\n",
    "test.loc[:, ['t1','t2', 'hum', 'wind_speed']]=scaler.fit_transform(test_trans)\n",
    "\n",
    "train['cnt'] = scaler.fit_transform(train[['cnt']])\n",
    "test['cnt'] = scaler.fit_transform(test[['cnt']])\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "#Split the data into x_train and y_train data sets\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in tqdm(range(len(train) - time_steps)):\n",
    "    x_train.append(train.drop(columns='cnt').iloc[i:i + time_steps].to_numpy())\n",
    "    y_train.append(train.loc[:,'cnt'].iloc[i + time_steps])\n",
    "\n",
    "#Convert x_train and y_train to numpy arrays\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "#Create the x_test and y_test data sets\n",
    "x_test = []\n",
    "y_test = df.loc[:,'cnt'].iloc[training_data_len:len(df)]\n",
    "\n",
    "for i in tqdm(range(len(test) - time_steps)):\n",
    "    x_test.append(test.drop(columns='cnt').iloc[i:i + time_steps].to_numpy())\n",
    "    # y_test.append(test.loc[:,'cnt'].iloc[i + time_steps])\n",
    "\n",
    "#Convert x_test and y_test to numpy arrays\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# All 12 columns of the data\n",
    "print('Train size:')\n",
    "print(x_train.shape, y_train.shape)\n",
    "print('Test size:')\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afbbda7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "tcn_2 (TCN)                  (None, 24, 20)            9860      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 24, 20)            0         \n",
      "_________________________________________________________________\n",
      "tcn_3 (TCN)                  (None, 20)                10080     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 19,961\n",
      "Trainable params: 19,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train...\n",
      "Train on 108 samples, validate on 12 samples\n",
      "Epoch 1/120\n",
      "108/108 [==============================] - 6s 51ms/sample - loss: 1.7486 - val_loss: 0.9207\n",
      "Epoch 2/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 1.7166 - val_loss: 0.9062\n",
      "Epoch 3/120\n",
      "108/108 [==============================] - ETA: 0s - loss: 1.859 - 0s 1ms/sample - loss: 1.8541 - val_loss: 0.8989\n",
      "Epoch 4/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 1.7417 - val_loss: 0.8782\n",
      "Epoch 5/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 1.7524 - val_loss: 0.8629\n",
      "Epoch 6/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 1.7185 - val_loss: 0.8378\n",
      "Epoch 7/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 1.6459 - val_loss: 0.7891\n",
      "Epoch 8/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 1.7175 - val_loss: 0.7895\n",
      "Epoch 9/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 1.8562 - val_loss: 0.7634\n",
      "Epoch 10/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 1.6241 - val_loss: 0.7066\n",
      "Epoch 11/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 1.7782 - val_loss: 0.6782\n",
      "Epoch 12/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 1.6026 - val_loss: 0.6277\n",
      "Epoch 13/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 1.4038 - val_loss: 0.5564\n",
      "Epoch 14/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 1.3192 - val_loss: 0.4818\n",
      "Epoch 15/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 1.1522 - val_loss: 0.4492\n",
      "Epoch 16/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 1.1780 - val_loss: 0.4734\n",
      "Epoch 17/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 1.1809 - val_loss: 0.5351\n",
      "Epoch 18/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 1.0963 - val_loss: 0.4622\n",
      "Epoch 19/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.8873 - val_loss: 0.4651\n",
      "Epoch 20/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.8467 - val_loss: 0.4791\n",
      "Epoch 21/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.8229 - val_loss: 0.4772\n",
      "Epoch 22/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.6994 - val_loss: 0.4722\n",
      "Epoch 23/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.7900 - val_loss: 0.4860\n",
      "Epoch 24/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.7975 - val_loss: 0.4788\n",
      "Epoch 25/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.7237 - val_loss: 0.3158\n",
      "Epoch 26/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.6570 - val_loss: 0.3781\n",
      "Epoch 27/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.6360 - val_loss: 0.3902\n",
      "Epoch 28/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.4977 - val_loss: 0.4083\n",
      "Epoch 29/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.6484 - val_loss: 0.5769\n",
      "Epoch 30/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.5813 - val_loss: 0.4148\n",
      "Epoch 31/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.5229 - val_loss: 0.3762\n",
      "Epoch 32/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.4898 - val_loss: 0.4226\n",
      "Epoch 33/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.4940 - val_loss: 0.3844\n",
      "Epoch 34/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.5287 - val_loss: 0.4172\n",
      "Epoch 35/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.3741 - val_loss: 0.4384\n",
      "Epoch 36/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.5102 - val_loss: 0.3585\n",
      "Epoch 37/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.5427 - val_loss: 0.3778\n",
      "Epoch 38/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.5662 - val_loss: 0.3665\n",
      "Epoch 39/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.3552 - val_loss: 0.3327\n",
      "Epoch 40/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.4135 - val_loss: 0.3838\n",
      "Epoch 41/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.4594 - val_loss: 0.3903\n",
      "Epoch 42/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.4528 - val_loss: 0.4496\n",
      "Epoch 43/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.4158 - val_loss: 0.4199\n",
      "Epoch 44/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.4341 - val_loss: 0.5160\n",
      "Epoch 45/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.5363 - val_loss: 0.4331\n",
      "Epoch 46/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.4888 - val_loss: 0.4831\n",
      "Epoch 47/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.5892 - val_loss: 0.3830\n",
      "Epoch 48/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.3753 - val_loss: 0.3730\n",
      "Epoch 49/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.4836 - val_loss: 0.3963\n",
      "Epoch 50/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.5934 - val_loss: 0.3018\n",
      "Epoch 51/120\n",
      "108/108 [==============================] - 0s 2ms/sample - loss: 0.4205 - val_loss: 0.2624\n",
      "Epoch 52/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.4001 - val_loss: 0.3163\n",
      "Epoch 53/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.4068 - val_loss: 0.3346\n",
      "Epoch 54/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.4377 - val_loss: 0.3966\n",
      "Epoch 55/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.3802 - val_loss: 0.3362\n",
      "Epoch 56/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.3738 - val_loss: 0.3010\n",
      "Epoch 57/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.4496 - val_loss: 0.2851\n",
      "Epoch 58/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.3816 - val_loss: 0.3084\n",
      "Epoch 59/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.4180 - val_loss: 0.3405\n",
      "Epoch 60/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.3267 - val_loss: 0.3303\n",
      "Epoch 61/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.4605 - val_loss: 0.3225\n",
      "Epoch 62/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.4862 - val_loss: 0.4022\n",
      "Epoch 63/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.3834 - val_loss: 0.3609\n",
      "Epoch 64/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.4624 - val_loss: 0.4083\n",
      "Epoch 65/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.3720 - val_loss: 0.3215\n",
      "Epoch 66/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.2989 - val_loss: 0.3902\n",
      "Epoch 67/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.3610 - val_loss: 0.4807\n",
      "Epoch 68/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.3627 - val_loss: 0.2869\n",
      "Epoch 69/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.2216 - val_loss: 0.2852\n",
      "Epoch 70/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.3512 - val_loss: 0.2828\n",
      "Epoch 71/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.4168 - val_loss: 0.2697\n",
      "Epoch 72/120\n",
      "108/108 [==============================] - 0s 2ms/sample - loss: 0.4648 - val_loss: 0.3029\n",
      "Epoch 73/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.4225 - val_loss: 0.3828\n",
      "Epoch 74/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.3240 - val_loss: 0.3811\n",
      "Epoch 75/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.3294 - val_loss: 0.4189\n",
      "Epoch 76/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.4363 - val_loss: 0.3735\n",
      "Epoch 77/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.3822 - val_loss: 0.3764\n",
      "Epoch 78/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.4494 - val_loss: 0.3232\n",
      "Epoch 79/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.2951 - val_loss: 0.3869\n",
      "Epoch 80/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.3743 - val_loss: 0.4136\n",
      "Epoch 81/120\n",
      "108/108 [==============================] - 0s 2ms/sample - loss: 0.2496 - val_loss: 0.4164\n",
      "Epoch 82/120\n",
      "108/108 [==============================] - 0s 2ms/sample - loss: 0.3051 - val_loss: 0.4790\n",
      "Epoch 83/120\n",
      "108/108 [==============================] - 0s 2ms/sample - loss: 0.2867 - val_loss: 0.4097\n",
      "Epoch 84/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.2170 - val_loss: 0.3552\n",
      "Epoch 85/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.4041 - val_loss: 0.4015\n",
      "Epoch 86/120\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.3838 - val_loss: 0.3764\n",
      "Epoch 87/120\n",
      "108/108 [==============================] - 0s 2ms/sample - loss: 0.2900 - val_loss: 0.4172\n",
      "Epoch 88/120\n",
      "108/108 [==============================] - 0s 2ms/sample - loss: 0.3325 - val_loss: 0.5036\n",
      "Epoch 89/120\n",
      "108/108 [==============================] - 0s 2ms/sample - loss: 0.2803 - val_loss: 0.4137\n",
      "Epoch 90/120\n",
      "108/108 [==============================] - 0s 2ms/sample - loss: 0.2781 - val_loss: 0.4017\n",
      "Epoch 91/120\n",
      "108/108 [==============================] - 0s 2ms/sample - loss: 0.3077 - val_loss: 0.3586\n",
      "Epoch 92/120\n",
      "108/108 [==============================] - 0s 2ms/sample - loss: 0.3563 - val_loss: 0.3661\n",
      "Epoch 93/120\n",
      "108/108 [==============================] - 0s 2ms/sample - loss: 0.2463 - val_loss: 0.3521\n",
      "Epoch 94/120\n",
      "108/108 [==============================] - 0s 2ms/sample - loss: 0.3472 - val_loss: 0.3099\n",
      "Epoch 95/120\n",
      "108/108 [==============================] - 0s 2ms/sample - loss: 0.4015 - val_loss: 0.3061\n",
      "Epoch 96/120\n",
      "108/108 [==============================] - 0s 2ms/sample - loss: 0.3452 - val_loss: 0.3366\n",
      "Epoch 97/120\n",
      "108/108 [==============================] - 0s 2ms/sample - loss: 0.3488 - val_loss: 0.4600\n",
      "Epoch 98/120\n",
      "108/108 [==============================] - 0s 2ms/sample - loss: 0.3070 - val_loss: 0.4240\n",
      "Epoch 99/120\n",
      "108/108 [==============================] - 0s 2ms/sample - loss: 0.2571 - val_loss: 0.3341\n",
      "Epoch 100/120\n",
      "108/108 [==============================] - 0s 2ms/sample - loss: 0.2854 - val_loss: 0.3325\n",
      "Epoch 101/120\n",
      "108/108 [==============================] - 0s 2ms/sample - loss: 0.3314 - val_loss: 0.3727\n",
      "Epoch 102/120\n",
      "108/108 [==============================] - 0s 2ms/sample - loss: 0.2766 - val_loss: 0.4430\n",
      "Epoch 103/120\n",
      "108/108 [==============================] - 0s 2ms/sample - loss: 0.4851 - val_loss: 0.3149\n",
      "Epoch 104/120\n",
      "108/108 [==============================] - 0s 2ms/sample - loss: 0.2716 - val_loss: 0.3250\n",
      "Epoch 105/120\n",
      "108/108 [==============================] - 0s 2ms/sample - loss: 0.3245 - val_loss: 0.3286\n",
      "Epoch 106/120\n",
      "108/108 [==============================] - 0s 2ms/sample - loss: 0.3083 - val_loss: 0.4087\n",
      "Epoch 107/120\n",
      "108/108 [==============================] - 0s 2ms/sample - loss: 0.3139 - val_loss: 0.3720\n",
      "Epoch 108/120\n",
      "108/108 [==============================] - 0s 2ms/sample - loss: 0.2708 - val_loss: 0.3721\n",
      "Epoch 109/120\n",
      "108/108 [==============================] - 0s 2ms/sample - loss: 0.2937 - val_loss: 0.3786\n",
      "Epoch 110/120\n",
      "108/108 [==============================] - 0s 2ms/sample - loss: 0.3066 - val_loss: 0.3832\n",
      "Epoch 111/120\n",
      "108/108 [==============================] - 0s 2ms/sample - loss: 0.3392 - val_loss: 0.3367\n",
      "Epoch 112/120\n",
      "108/108 [==============================] - 0s 2ms/sample - loss: 0.3307 - val_loss: 0.2835\n",
      "Epoch 113/120\n",
      "108/108 [==============================] - 0s 2ms/sample - loss: 0.3206 - val_loss: 0.2823\n",
      "Epoch 114/120\n",
      "108/108 [==============================] - 0s 2ms/sample - loss: 0.3108 - val_loss: 0.3265\n",
      "Epoch 115/120\n",
      "108/108 [==============================] - 0s 2ms/sample - loss: 0.3943 - val_loss: 0.2559\n",
      "Epoch 116/120\n",
      "108/108 [==============================] - 0s 2ms/sample - loss: 0.4262 - val_loss: 0.2468\n",
      "Epoch 117/120\n",
      "108/108 [==============================] - 0s 2ms/sample - loss: 0.3375 - val_loss: 0.2609\n",
      "Epoch 118/120\n",
      "108/108 [==============================] - 0s 2ms/sample - loss: 0.3194 - val_loss: 0.2654\n",
      "Epoch 119/120\n",
      "108/108 [==============================] - 0s 2ms/sample - loss: 0.3618 - val_loss: 0.2792\n",
      "Epoch 120/120\n",
      "108/108 [==============================] - 0s 2ms/sample - loss: 0.2682 - val_loss: 0.2868\n",
      "Load Success\n",
      "Predict time:  0.7854838371276855\n",
      "RMSE:  5.578237063561625\n",
      "MAE:  4.519120268523693\n",
      "R-square:  -3.082974144924096\n",
      "Adj R-square:  -19.41487072462048\n"
     ]
    }
   ],
   "source": [
    "#init = glorot_normal(seed=None) # 給 LSTM\n",
    "#init_d = RandomUniform(minval=-0.05, maxval=0.05) # 給 Dense layer\n",
    "#nadam = optimizers.Nadam(lr=0.002,clipvalue=0.5)\n",
    "from tensorflow.keras.initializers import RandomUniform\n",
    "init_d = RandomUniform(minval=-0.05, maxval=0.05) # 給 Dense layer\n",
    "model = Sequential()\n",
    "model.add(TCN(input_shape=(x_train.shape[1],x_train.shape[2]),\n",
    "        kernel_size=3,\n",
    "        nb_filters=20,\n",
    "        nb_stacks=1,\n",
    "        dilations=(1, 2, 4, 8),\n",
    "        kernel_initializer='glorot_uniform',\n",
    "        use_skip_connections=True,\n",
    "        use_batch_norm=False,\n",
    "        use_weight_norm=False,\n",
    "        use_layer_norm=True,\n",
    "        dropout_rate = 0.05,\n",
    "        return_sequences=True\n",
    "        ))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(TCN(\n",
    "        kernel_size=3,\n",
    "        nb_filters=20,\n",
    "        nb_stacks=1,\n",
    "        dilations=(1, 2, 4, 8),\n",
    "        kernel_initializer='glorot_uniform',\n",
    "        use_skip_connections=True,\n",
    "        use_batch_norm=False,\n",
    "        use_weight_norm=False,\n",
    "        use_layer_norm=True,\n",
    "        dropout_rate = 0.05,\n",
    "        return_sequences=False\n",
    "        ))  \n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1,kernel_initializer=init_d))  \n",
    "model.summary()\n",
    "\n",
    "print('Train...')\n",
    "model.compile(optimizer = \"nadam\" , loss=\"mse\")\n",
    "history = model.fit(x_train, y_train, epochs=120, batch_size=12, validation_split=0.1, shuffle=True)\n",
    "#Save Model\n",
    "model.save('TCN_model_doric.h5')  # creates a HDF5 file \n",
    "del model\n",
    "\n",
    "custom_ob = {'TCN': TCN}\n",
    "# model = load_model('TCN_model_1.h5', custom_objects=custom_ob)\n",
    "model = load_model('TCN_model_doric.h5', custom_objects=custom_ob)\n",
    "print('Load Success')\n",
    "\n",
    "import time\n",
    "t1 = time.time()\n",
    "y_pred = model.predict(x_test)\n",
    "t2 = time.time()\n",
    "print('Predict time: ',t2-t1)\n",
    "y_pred = scaler.inverse_transform(y_pred)#Undo scaling\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score,mean_absolute_error\n",
    "rmse_lstm = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print('RMSE: ',rmse_lstm)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print('MAE: ',mae)\n",
    "\n",
    "r2 =  r2_score(y_test, y_pred)\n",
    "print('R-square: ',r2)\n",
    "\n",
    "\n",
    "n = len(y_test)\n",
    "p = 12\n",
    "Adj_r2 = 1-(1-r2)*(n-1)/(n-p-1)\n",
    "print('Adj R-square: ',Adj_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e84845",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5f8e4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee42622b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
