{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11f5b696",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from tcn import TCN\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error, r2_score,mean_absolute_error\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential , load_model , Model\n",
    "from keras.layers import Dense, Dropout , LSTM , Bidirectional ,GRU ,Flatten,Add,BatchNormalization\n",
    "from keras_layer_normalization import LayerNormalization\n",
    "from keras.initializers import  glorot_normal, RandomUniform\n",
    "from keras import optimizers,Input\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17670aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160, 13) (144, 13) (40, 13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:32: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9ffd7bdfbb346dfa43d8ddda3f2b8aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:44: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7de9d857e738459b892bff3d921b36f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:\n",
      "(120, 24, 12) (120,)\n",
      "Test size:\n",
      "(16, 24, 12) (16,)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"station_bike _Duke.csv\")\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "df = df.set_index(\"timestamp\")\n",
    "#df.head()\n",
    "\n",
    "df[\"hour\"] = df.index.hour\n",
    "df[\"day_of_month\"] = df.index.day\n",
    "df[\"day_of_week\"]  = df.index.dayofweek\n",
    "df[\"month\"] = df.index.month\n",
    "\n",
    "training_data_len = math.ceil(len(df) * 0.9) # taking 90% of data to train and 10% of data to test\n",
    "testing_data_len = len(df) - training_data_len\n",
    "\n",
    "time_steps = 24\n",
    "train, test = df.iloc[0:training_data_len], df.iloc[(training_data_len-time_steps):len(df)]\n",
    "print(df.shape, train.shape, test.shape)\n",
    "train_trans = train[['t1','t2', 'hum', 'wind_speed']].to_numpy()\n",
    "test_trans = test[['t1','t2', 'hum', 'wind_speed']].to_numpy()\n",
    "\n",
    "scaler = RobustScaler() # Handles outliers\n",
    "#scaler = MinMaxScaler(feature_range=(0, 1)) # scale to (0,1)\n",
    "train.loc[:, ['t1','t2','hum', 'wind_speed']]=scaler.fit_transform(train_trans)\n",
    "test.loc[:, ['t1','t2', 'hum', 'wind_speed']]=scaler.fit_transform(test_trans)\n",
    "\n",
    "train['cnt'] = scaler.fit_transform(train[['cnt']])\n",
    "test['cnt'] = scaler.fit_transform(test[['cnt']])\n",
    "\n",
    "#Split the data into x_train and y_train data sets\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in tqdm(range(len(train) - time_steps)):\n",
    "    x_train.append(train.drop(columns='cnt').iloc[i:i + time_steps].to_numpy())\n",
    "    y_train.append(train.loc[:,'cnt'].iloc[i + time_steps])\n",
    "\n",
    "#Convert x_train and y_train to numpy arrays\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "#Create the x_test and y_test data sets\n",
    "x_test = []\n",
    "y_test = df.loc[:,'cnt'].iloc[training_data_len:len(df)]\n",
    "\n",
    "for i in tqdm(range(len(test) - time_steps)):\n",
    "    x_test.append(test.drop(columns='cnt').iloc[i:i + time_steps].to_numpy())\n",
    "    # y_test.append(test.loc[:,'cnt'].iloc[i + time_steps])\n",
    "\n",
    "#Convert x_test and y_test to numpy arrays\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# All 12 columns of the data\n",
    "print('Train size:')\n",
    "print(x_train.shape, y_train.shape)\n",
    "print('Test size:')\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd9605b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 24, 12)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 24, 24)       1800        input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "seq_self_attention_3 (SeqSelfAt (None, 24, 24)       577         bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 24, 24)       0           seq_self_attention_3[0][0]       \n",
      "                                                                 bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_3 (LayerNor (None, 24, 24)       48          add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 576)          0           layer_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 12)           6924        flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 12)           0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 588)          0           dropout_3[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            589         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 1)            2           dense_8[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 9,940\n",
      "Trainable params: 9,940\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Concatenate\n",
    "init = glorot_normal(seed=None) # 給 GRU\n",
    "init_d = RandomUniform(minval=-0.05, maxval=0.05) # 給 Dense layer\n",
    "\n",
    "def Encoder(layer):\n",
    "    shortcut = layer\n",
    "    layer = SeqSelfAttention(\n",
    "                     attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n",
    "                     kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     bias_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     attention_regularizer_weight=0.0001)(layer)\n",
    "    layer = Add()([layer,shortcut])\n",
    "    layer = LayerNormalization()(layer)\n",
    "    layer = Flatten()(layer)\n",
    "    \n",
    "    shortcut2 = layer\n",
    "    layer = Dense(12,kernel_initializer=init_d)(layer)\n",
    "    layer = Dropout(0.15)(layer)\n",
    "    layer = Concatenate()([layer,shortcut2])\n",
    "    output = Dense(1,kernel_initializer=init_d)(layer)\n",
    "    return output\n",
    "\n",
    "def Decoder(layer):\n",
    "    shortcut = layer\n",
    "    layer = SeqSelfAttention(\n",
    "                     attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n",
    "                     kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     bias_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     attention_regularizer_weight=0.0001)(layer)\n",
    "    layer = Add()([layer,shortcut])\n",
    "    layer = LayerNormalization()(layer)\n",
    "    layer = SeqSelfAttention(\n",
    "                     attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n",
    "                     kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     bias_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     attention_regularizer_weight=0.0001)(layer)\n",
    "    layer = LayerNormalization()(layer)\n",
    "    \n",
    "    layer = Flatten()(layer)\n",
    "    shortcut2 = layer\n",
    "    layer = Dense(10,kernel_initializer=init_d)(layer)\n",
    "    #layer = Dropout(0.2)(layer)\n",
    "    layer = Concatenate()([layer,shortcut2])\n",
    "    output = Dense(1,kernel_initializer=init_d)(layer)\n",
    "    return output\n",
    "\n",
    "def Bi_GRU(layer,unit):\n",
    "    output = Bidirectional(GRU(unit, dropout=0.1, recurrent_dropout=0.1, return_sequences=True,\n",
    "                            kernel_initializer=init))(layer)\n",
    "    return output\n",
    "\n",
    "#start = Input(shape = (x_train.shape[1],x_train.shape[2]))\n",
    "start = Input(shape = (x_train.shape[1:]))\n",
    "start2 = Input(shape = (x_train.shape[1:]))\n",
    "x = Bi_GRU(start,12)\n",
    "x = Encoder(x)\n",
    "\n",
    "# y = Bi_GRU(start2,8)\n",
    "# y = Decoder(y)\n",
    "\n",
    "#Merge = Add()([x,x])\n",
    "Last = Dense(1)(x)\n",
    "model = Model([start,start2] , Last)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e0d57cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 108 samples, validate on 12 samples\n",
      "Epoch 1/500\n",
      "108/108 [==============================] - 2s 17ms/step - loss: 1.5022 - val_loss: 1.1752\n",
      "Epoch 2/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.3625 - val_loss: 0.9401\n",
      "Epoch 3/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.2980 - val_loss: 0.8276\n",
      "Epoch 4/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.0988 - val_loss: 0.7354\n",
      "Epoch 5/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.0464 - val_loss: 0.6995\n",
      "Epoch 6/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.9962 - val_loss: 0.6023\n",
      "Epoch 7/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.0434 - val_loss: 0.5373\n",
      "Epoch 8/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.0815 - val_loss: 0.7561\n",
      "Epoch 9/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.7911 - val_loss: 0.6271\n",
      "Epoch 10/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.8983 - val_loss: 0.5076\n",
      "Epoch 11/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6585 - val_loss: 0.5842\n",
      "Epoch 12/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6861 - val_loss: 0.5195\n",
      "Epoch 13/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6828 - val_loss: 0.4794\n",
      "Epoch 14/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6674 - val_loss: 0.5542\n",
      "Epoch 15/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5728 - val_loss: 0.5296\n",
      "Epoch 16/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5068 - val_loss: 0.4914\n",
      "Epoch 17/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6873 - val_loss: 0.5265\n",
      "Epoch 18/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6009 - val_loss: 0.5904\n",
      "Epoch 19/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5127 - val_loss: 0.4577\n",
      "Epoch 20/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4006 - val_loss: 0.5843\n",
      "Epoch 21/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4477 - val_loss: 0.4729\n",
      "Epoch 22/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4141 - val_loss: 0.4438\n",
      "Epoch 23/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4008 - val_loss: 0.4668\n",
      "Epoch 24/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.3481 - val_loss: 0.4164\n",
      "Epoch 25/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.3348 - val_loss: 0.3764\n",
      "Epoch 26/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.4074 - val_loss: 0.3990\n",
      "Epoch 27/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.3588 - val_loss: 0.3881\n",
      "Epoch 28/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.3467 - val_loss: 0.3990\n",
      "Epoch 29/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.3705 - val_loss: 0.3887\n",
      "Epoch 30/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.3031 - val_loss: 0.2809\n",
      "Epoch 31/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.2842 - val_loss: 0.3401\n",
      "Epoch 32/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.2414 - val_loss: 0.3480\n",
      "Epoch 33/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.3222 - val_loss: 0.4977\n",
      "Epoch 34/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.2526 - val_loss: 0.3822\n",
      "Epoch 35/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.2337 - val_loss: 0.3560\n",
      "Epoch 36/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.3634 - val_loss: 0.3504\n",
      "Epoch 37/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.3369 - val_loss: 0.4126\n",
      "Epoch 38/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2157 - val_loss: 0.3881\n",
      "Epoch 39/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.2900 - val_loss: 0.3140\n",
      "Epoch 40/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1825 - val_loss: 0.3462\n",
      "Epoch 41/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1888 - val_loss: 0.3530\n",
      "Epoch 42/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.2851 - val_loss: 0.3534\n",
      "Epoch 43/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1684 - val_loss: 0.2202\n",
      "Epoch 44/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1735 - val_loss: 0.2723\n",
      "Epoch 45/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2004 - val_loss: 0.3620\n",
      "Epoch 46/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1470 - val_loss: 0.2957\n",
      "Epoch 47/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1416 - val_loss: 0.3288\n",
      "Epoch 48/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1739 - val_loss: 0.3402\n",
      "Epoch 49/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2282 - val_loss: 0.2980\n",
      "Epoch 50/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1363 - val_loss: 0.3080\n",
      "Epoch 51/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1993 - val_loss: 0.3116\n",
      "Epoch 52/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1599 - val_loss: 0.3439\n",
      "Epoch 53/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1922 - val_loss: 0.3066\n",
      "Epoch 54/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1701 - val_loss: 0.3211\n",
      "Epoch 55/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.2917 - val_loss: 0.3283\n",
      "Epoch 56/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.3958 - val_loss: 0.3098\n",
      "Epoch 57/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2179 - val_loss: 0.2931\n",
      "Epoch 58/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1988 - val_loss: 0.2771\n",
      "Epoch 59/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1451 - val_loss: 0.2676\n",
      "Epoch 60/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1336 - val_loss: 0.3225\n",
      "Epoch 61/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1545 - val_loss: 0.2878\n",
      "Epoch 62/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1151 - val_loss: 0.2772\n",
      "Epoch 63/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1625 - val_loss: 0.2702\n",
      "Epoch 64/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1840 - val_loss: 0.2641\n",
      "Epoch 65/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1487 - val_loss: 0.2237\n",
      "Epoch 66/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1694 - val_loss: 0.2322\n",
      "Epoch 67/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1665 - val_loss: 0.2674\n",
      "Epoch 68/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1755 - val_loss: 0.2736\n",
      "Epoch 69/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1837 - val_loss: 0.2500\n",
      "Epoch 70/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1336 - val_loss: 0.2937\n",
      "Epoch 71/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.2622 - val_loss: 0.3779\n",
      "Epoch 72/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1821 - val_loss: 0.2708\n",
      "Epoch 73/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1928 - val_loss: 0.2864\n",
      "Epoch 74/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1813 - val_loss: 0.2923\n",
      "Epoch 75/500\n",
      "108/108 [==============================] - 1s 8ms/step - loss: 0.1202 - val_loss: 0.3031\n",
      "Epoch 76/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1972 - val_loss: 0.3633\n",
      "Epoch 77/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1381 - val_loss: 0.3123\n",
      "Epoch 78/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1166 - val_loss: 0.2867\n",
      "Epoch 79/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1108 - val_loss: 0.2747\n",
      "Epoch 80/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.3303 - val_loss: 0.3058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1619 - val_loss: 0.3424\n",
      "Epoch 82/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1031 - val_loss: 0.2988\n",
      "Epoch 83/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1111 - val_loss: 0.2465\n",
      "Epoch 84/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1617 - val_loss: 0.2437\n",
      "Epoch 85/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1218 - val_loss: 0.2243\n",
      "Epoch 86/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1376 - val_loss: 0.3062\n",
      "Epoch 87/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1833 - val_loss: 0.3368\n",
      "Epoch 88/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1324 - val_loss: 0.3004\n",
      "Epoch 89/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1181 - val_loss: 0.3034\n",
      "Epoch 90/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1112 - val_loss: 0.3093\n",
      "Epoch 91/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1345 - val_loss: 0.2678\n",
      "Epoch 92/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1126 - val_loss: 0.2578\n",
      "Epoch 93/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1172 - val_loss: 0.2519\n",
      "Epoch 94/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1091 - val_loss: 0.3096\n",
      "Epoch 95/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0931 - val_loss: 0.3334\n",
      "Epoch 96/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1167 - val_loss: 0.2504\n",
      "Epoch 97/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1084 - val_loss: 0.2537\n",
      "Epoch 98/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1252 - val_loss: 0.2658\n",
      "Epoch 99/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1667 - val_loss: 0.2338\n",
      "Epoch 100/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1230 - val_loss: 0.2572\n",
      "Epoch 101/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1412 - val_loss: 0.2226\n",
      "Epoch 102/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0985 - val_loss: 0.3325\n",
      "Epoch 103/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1254 - val_loss: 0.2498\n",
      "Epoch 104/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1347 - val_loss: 0.2895\n",
      "Epoch 105/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1577 - val_loss: 0.2884\n",
      "Epoch 106/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1081 - val_loss: 0.2835\n",
      "Epoch 107/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1112 - val_loss: 0.2560\n",
      "Epoch 108/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0995 - val_loss: 0.2638\n",
      "Epoch 109/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1076 - val_loss: 0.2394\n",
      "Epoch 110/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1034 - val_loss: 0.2537\n",
      "Epoch 111/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1162 - val_loss: 0.2492\n",
      "Epoch 112/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1530 - val_loss: 0.2871\n",
      "Epoch 113/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.2054 - val_loss: 0.2730\n",
      "Epoch 114/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1254 - val_loss: 0.3004\n",
      "Epoch 115/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0833 - val_loss: 0.2335\n",
      "Epoch 116/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0869 - val_loss: 0.2719\n",
      "Epoch 117/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0804 - val_loss: 0.3037\n",
      "Epoch 118/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1736 - val_loss: 0.3382\n",
      "Epoch 119/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1202 - val_loss: 0.3040\n",
      "Epoch 120/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0934 - val_loss: 0.3299\n",
      "Epoch 121/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1980 - val_loss: 0.2811\n",
      "Epoch 122/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1141 - val_loss: 0.2510\n",
      "Epoch 123/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1121 - val_loss: 0.3194\n",
      "Epoch 124/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1117 - val_loss: 0.3132\n",
      "Epoch 125/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1223 - val_loss: 0.2732\n",
      "Epoch 126/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0943 - val_loss: 0.3028\n",
      "Epoch 127/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0975 - val_loss: 0.2336\n",
      "Epoch 128/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1744 - val_loss: 0.2125\n",
      "Epoch 129/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0979 - val_loss: 0.2596\n",
      "Epoch 130/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0884 - val_loss: 0.2685\n",
      "Epoch 131/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0783 - val_loss: 0.2297\n",
      "Epoch 132/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0858 - val_loss: 0.2508\n",
      "Epoch 133/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1340 - val_loss: 0.3261\n",
      "Epoch 134/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1099 - val_loss: 0.2556\n",
      "Epoch 135/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0920 - val_loss: 0.2355\n",
      "Epoch 136/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0827 - val_loss: 0.2059\n",
      "Epoch 137/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1047 - val_loss: 0.2750\n",
      "Epoch 138/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1141 - val_loss: 0.2905\n",
      "Epoch 139/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0874 - val_loss: 0.2452\n",
      "Epoch 140/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0863 - val_loss: 0.3027\n",
      "Epoch 141/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1695 - val_loss: 0.2918\n",
      "Epoch 142/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.2208 - val_loss: 0.2840\n",
      "Epoch 143/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1375 - val_loss: 0.2283\n",
      "Epoch 144/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1796 - val_loss: 0.2415\n",
      "Epoch 145/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1388 - val_loss: 0.2815\n",
      "Epoch 146/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1589 - val_loss: 0.2900\n",
      "Epoch 147/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0943 - val_loss: 0.2534\n",
      "Epoch 148/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0875 - val_loss: 0.2851\n",
      "Epoch 149/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0640 - val_loss: 0.2918\n",
      "Epoch 150/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0955 - val_loss: 0.2866\n",
      "Epoch 151/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1376 - val_loss: 0.2753\n",
      "Epoch 152/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1049 - val_loss: 0.2545\n",
      "Epoch 153/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0885 - val_loss: 0.2617\n",
      "Epoch 154/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0797 - val_loss: 0.2797\n",
      "Epoch 155/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0726 - val_loss: 0.2209\n",
      "Epoch 156/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0764 - val_loss: 0.2224\n",
      "Epoch 157/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0795 - val_loss: 0.2439\n",
      "Epoch 158/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1070 - val_loss: 0.2936\n",
      "Epoch 159/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0950 - val_loss: 0.2793\n",
      "Epoch 160/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0773 - val_loss: 0.2807\n",
      "Epoch 161/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1179 - val_loss: 0.2669\n",
      "Epoch 162/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0826 - val_loss: 0.2752\n",
      "Epoch 163/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0837 - val_loss: 0.2069\n",
      "Epoch 164/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1232 - val_loss: 0.2168\n",
      "Epoch 165/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1070 - val_loss: 0.2406\n",
      "Epoch 166/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0847 - val_loss: 0.2358\n",
      "Epoch 167/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0959 - val_loss: 0.2806\n",
      "Epoch 168/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1035 - val_loss: 0.2236\n",
      "Epoch 169/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0853 - val_loss: 0.2997\n",
      "Epoch 170/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0600 - val_loss: 0.2583\n",
      "Epoch 171/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0719 - val_loss: 0.2595\n",
      "Epoch 172/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0846 - val_loss: 0.2384\n",
      "Epoch 173/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0693 - val_loss: 0.2500\n",
      "Epoch 174/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0852 - val_loss: 0.2721\n",
      "Epoch 175/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1216 - val_loss: 0.2862\n",
      "Epoch 176/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0891 - val_loss: 0.2738\n",
      "Epoch 177/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0738 - val_loss: 0.2513\n",
      "Epoch 178/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0607 - val_loss: 0.2690\n",
      "Epoch 179/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1626 - val_loss: 0.2776\n",
      "Epoch 180/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1242 - val_loss: 0.2277\n",
      "Epoch 181/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0998 - val_loss: 0.2871\n",
      "Epoch 182/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0739 - val_loss: 0.3391\n",
      "Epoch 183/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0751 - val_loss: 0.2945\n",
      "Epoch 184/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0934 - val_loss: 0.3136\n",
      "Epoch 185/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0855 - val_loss: 0.2963\n",
      "Epoch 186/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0784 - val_loss: 0.3026\n",
      "Epoch 187/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1049 - val_loss: 0.2887\n",
      "Epoch 188/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1241 - val_loss: 0.3060\n",
      "Epoch 189/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0756 - val_loss: 0.3017\n",
      "Epoch 190/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0742 - val_loss: 0.2812\n",
      "Epoch 191/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0681 - val_loss: 0.2943\n",
      "Epoch 192/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0882 - val_loss: 0.3168\n",
      "Epoch 193/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0480 - val_loss: 0.2844\n",
      "Epoch 194/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0578 - val_loss: 0.2818\n",
      "Epoch 195/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0614 - val_loss: 0.2503\n",
      "Epoch 196/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0762 - val_loss: 0.2584\n",
      "Epoch 197/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0747 - val_loss: 0.2309\n",
      "Epoch 198/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.2238 - val_loss: 0.2458\n",
      "Epoch 199/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0766 - val_loss: 0.2475\n",
      "Epoch 200/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1132 - val_loss: 0.2441\n",
      "Epoch 201/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1038 - val_loss: 0.2651\n",
      "Epoch 202/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0862 - val_loss: 0.2538\n",
      "Epoch 203/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0640 - val_loss: 0.2713\n",
      "Epoch 204/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0693 - val_loss: 0.2824\n",
      "Epoch 205/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0715 - val_loss: 0.2929\n",
      "Epoch 206/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0769 - val_loss: 0.2414\n",
      "Epoch 207/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0869 - val_loss: 0.2819\n",
      "Epoch 208/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0757 - val_loss: 0.3102\n",
      "Epoch 209/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1011 - val_loss: 0.2713\n",
      "Epoch 210/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0716 - val_loss: 0.2647\n",
      "Epoch 211/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0608 - val_loss: 0.2742\n",
      "Epoch 212/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0725 - val_loss: 0.2946\n",
      "Epoch 213/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0667 - val_loss: 0.2656\n",
      "Epoch 214/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0622 - val_loss: 0.2690\n",
      "Epoch 215/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0883 - val_loss: 0.2511\n",
      "Epoch 216/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0680 - val_loss: 0.2281\n",
      "Epoch 217/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0721 - val_loss: 0.2912\n",
      "Epoch 218/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0807 - val_loss: 0.4130\n",
      "Epoch 219/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0926 - val_loss: 0.2181\n",
      "Epoch 220/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0679 - val_loss: 0.2509\n",
      "Epoch 221/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0639 - val_loss: 0.2836\n",
      "Epoch 222/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1038 - val_loss: 0.2385\n",
      "Epoch 223/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1709 - val_loss: 0.3019\n",
      "Epoch 224/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0938 - val_loss: 0.2579\n",
      "Epoch 225/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1937 - val_loss: 0.2636\n",
      "Epoch 226/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0962 - val_loss: 0.2489\n",
      "Epoch 227/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0765 - val_loss: 0.2698\n",
      "Epoch 228/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0674 - val_loss: 0.2785\n",
      "Epoch 229/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0811 - val_loss: 0.2713\n",
      "Epoch 230/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0605 - val_loss: 0.2692\n",
      "Epoch 231/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0621 - val_loss: 0.2775\n",
      "Epoch 232/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1023 - val_loss: 0.2500\n",
      "Epoch 233/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0689 - val_loss: 0.2646\n",
      "Epoch 234/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0712 - val_loss: 0.2287\n",
      "Epoch 235/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.2192 - val_loss: 0.2233\n",
      "Epoch 236/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0822 - val_loss: 0.2328\n",
      "Epoch 237/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0738 - val_loss: 0.2509\n",
      "Epoch 238/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1065 - val_loss: 0.2895\n",
      "Epoch 239/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0587 - val_loss: 0.2431\n",
      "Epoch 240/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0748 - val_loss: 0.2192\n",
      "Epoch 241/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0791 - val_loss: 0.2542\n",
      "Epoch 242/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0681 - val_loss: 0.2335\n",
      "Epoch 243/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0703 - val_loss: 0.2748\n",
      "Epoch 244/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0885 - val_loss: 0.2864\n",
      "Epoch 245/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1023 - val_loss: 0.2787\n",
      "Epoch 246/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0762 - val_loss: 0.2708\n",
      "Epoch 247/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0696 - val_loss: 0.2838\n",
      "Epoch 248/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0987 - val_loss: 0.2360\n",
      "Epoch 249/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0735 - val_loss: 0.2756\n",
      "Epoch 250/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0588 - val_loss: 0.2447\n",
      "Epoch 251/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0571 - val_loss: 0.2381\n",
      "Epoch 252/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0665 - val_loss: 0.2316\n",
      "Epoch 253/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0685 - val_loss: 0.2714\n",
      "Epoch 254/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0714 - val_loss: 0.2371\n",
      "Epoch 255/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0650 - val_loss: 0.2638\n",
      "Epoch 256/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0603 - val_loss: 0.2463\n",
      "Epoch 257/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0642 - val_loss: 0.2665\n",
      "Epoch 258/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0631 - val_loss: 0.2752\n",
      "Epoch 259/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0686 - val_loss: 0.2579\n",
      "Epoch 260/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0815 - val_loss: 0.2518\n",
      "Epoch 261/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0954 - val_loss: 0.2513\n",
      "Epoch 262/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0560 - val_loss: 0.2464\n",
      "Epoch 263/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0549 - val_loss: 0.2454\n",
      "Epoch 264/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0810 - val_loss: 0.2603\n",
      "Epoch 265/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0785 - val_loss: 0.2510\n",
      "Epoch 266/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0697 - val_loss: 0.2786\n",
      "Epoch 267/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0580 - val_loss: 0.3026\n",
      "Epoch 268/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0582 - val_loss: 0.2662\n",
      "Epoch 269/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1039 - val_loss: 0.2614\n",
      "Epoch 270/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0660 - val_loss: 0.2567\n",
      "Epoch 271/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0675 - val_loss: 0.2398\n",
      "Epoch 272/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0957 - val_loss: 0.2720\n",
      "Epoch 273/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0601 - val_loss: 0.2717\n",
      "Epoch 274/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0663 - val_loss: 0.2242\n",
      "Epoch 275/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0659 - val_loss: 0.2541\n",
      "Epoch 276/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0551 - val_loss: 0.2430\n",
      "Epoch 277/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0766 - val_loss: 0.2644\n",
      "Epoch 278/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0831 - val_loss: 0.2829\n",
      "Epoch 279/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0573 - val_loss: 0.2316\n",
      "Epoch 280/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0892 - val_loss: 0.2858\n",
      "Epoch 281/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0570 - val_loss: 0.2769\n",
      "Epoch 282/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0621 - val_loss: 0.2450\n",
      "Epoch 283/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0633 - val_loss: 0.2369\n",
      "Epoch 284/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0516 - val_loss: 0.2535\n",
      "Epoch 285/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0622 - val_loss: 0.2358\n",
      "Epoch 286/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0632 - val_loss: 0.2268\n",
      "Epoch 287/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0720 - val_loss: 0.2283\n",
      "Epoch 288/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0520 - val_loss: 0.2401\n",
      "Epoch 289/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0716 - val_loss: 0.2677\n",
      "Epoch 290/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0887 - val_loss: 0.2682\n",
      "Epoch 291/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0689 - val_loss: 0.2578\n",
      "Epoch 292/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0660 - val_loss: 0.2550\n",
      "Epoch 293/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0955 - val_loss: 0.3216\n",
      "Epoch 294/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0757 - val_loss: 0.2797\n",
      "Epoch 295/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0577 - val_loss: 0.2866\n",
      "Epoch 296/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0483 - val_loss: 0.2562\n",
      "Epoch 297/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0664 - val_loss: 0.2366\n",
      "Epoch 298/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0627 - val_loss: 0.2426\n",
      "Epoch 299/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0521 - val_loss: 0.2398\n",
      "Epoch 300/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0465 - val_loss: 0.2418\n",
      "Epoch 301/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0544 - val_loss: 0.2363\n",
      "Epoch 302/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0567 - val_loss: 0.2621\n",
      "Epoch 303/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0563 - val_loss: 0.2744\n",
      "Epoch 304/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1591 - val_loss: 0.2521\n",
      "Epoch 305/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0674 - val_loss: 0.2463\n",
      "Epoch 306/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0554 - val_loss: 0.2550\n",
      "Epoch 307/500\n",
      "108/108 [==============================] - 1s 8ms/step - loss: 0.0603 - val_loss: 0.2309\n",
      "Epoch 308/500\n",
      "108/108 [==============================] - 1s 8ms/step - loss: 0.0548 - val_loss: 0.2457\n",
      "Epoch 309/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0682 - val_loss: 0.2720\n",
      "Epoch 310/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0576 - val_loss: 0.2495\n",
      "Epoch 311/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0614 - val_loss: 0.2252\n",
      "Epoch 312/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1567 - val_loss: 0.2341\n",
      "Epoch 313/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0657 - val_loss: 0.2475\n",
      "Epoch 314/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0530 - val_loss: 0.2639\n",
      "Epoch 315/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0482 - val_loss: 0.2358\n",
      "Epoch 316/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0808 - val_loss: 0.2783\n",
      "Epoch 317/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0626 - val_loss: 0.3424\n",
      "Epoch 318/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1046 - val_loss: 0.2746\n",
      "Epoch 319/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0547 - val_loss: 0.2485\n",
      "Epoch 320/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0651 - val_loss: 0.2400\n",
      "Epoch 321/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0789 - val_loss: 0.2440\n",
      "Epoch 322/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1333 - val_loss: 0.2706\n",
      "Epoch 323/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0509 - val_loss: 0.2741\n",
      "Epoch 324/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0617 - val_loss: 0.2451\n",
      "Epoch 325/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0668 - val_loss: 0.2549\n",
      "Epoch 326/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0571 - val_loss: 0.3274\n",
      "Epoch 327/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0825 - val_loss: 0.2643\n",
      "Epoch 328/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0584 - val_loss: 0.2518\n",
      "Epoch 329/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0573 - val_loss: 0.2706\n",
      "Epoch 330/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1209 - val_loss: 0.2725\n",
      "Epoch 331/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0562 - val_loss: 0.2664\n",
      "Epoch 332/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0755 - val_loss: 0.2843\n",
      "Epoch 333/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0581 - val_loss: 0.2546\n",
      "Epoch 334/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0492 - val_loss: 0.2659\n",
      "Epoch 335/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0538 - val_loss: 0.2584\n",
      "Epoch 336/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0633 - val_loss: 0.2725\n",
      "Epoch 337/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0461 - val_loss: 0.2768\n",
      "Epoch 338/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0506 - val_loss: 0.2695\n",
      "Epoch 339/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0517 - val_loss: 0.2589\n",
      "Epoch 340/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0531 - val_loss: 0.2652\n",
      "Epoch 341/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0537 - val_loss: 0.2820\n",
      "Epoch 342/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0553 - val_loss: 0.2562\n",
      "Epoch 343/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0731 - val_loss: 0.3314\n",
      "Epoch 344/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0681 - val_loss: 0.2955\n",
      "Epoch 345/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0649 - val_loss: 0.2274\n",
      "Epoch 346/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0636 - val_loss: 0.2460\n",
      "Epoch 347/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0539 - val_loss: 0.2547\n",
      "Epoch 348/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0839 - val_loss: 0.2366\n",
      "Epoch 349/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0545 - val_loss: 0.2564\n",
      "Epoch 350/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0629 - val_loss: 0.2327\n",
      "Epoch 351/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0542 - val_loss: 0.2385\n",
      "Epoch 352/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0666 - val_loss: 0.2284\n",
      "Epoch 353/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0901 - val_loss: 0.2398\n",
      "Epoch 354/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0937 - val_loss: 0.2580\n",
      "Epoch 355/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0561 - val_loss: 0.2365\n",
      "Epoch 356/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0513 - val_loss: 0.2399\n",
      "Epoch 357/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0490 - val_loss: 0.2325\n",
      "Epoch 358/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0582 - val_loss: 0.2305\n",
      "Epoch 359/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0567 - val_loss: 0.2348\n",
      "Epoch 360/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0662 - val_loss: 0.2409\n",
      "Epoch 361/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0636 - val_loss: 0.2423\n",
      "Epoch 362/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0563 - val_loss: 0.2635\n",
      "Epoch 363/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0674 - val_loss: 0.2458\n",
      "Epoch 364/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0473 - val_loss: 0.2836\n",
      "Epoch 365/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0578 - val_loss: 0.2600\n",
      "Epoch 366/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0422 - val_loss: 0.2630\n",
      "Epoch 367/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0450 - val_loss: 0.2254\n",
      "Epoch 368/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0650 - val_loss: 0.2519\n",
      "Epoch 369/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0556 - val_loss: 0.2469\n",
      "Epoch 370/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0476 - val_loss: 0.2708\n",
      "Epoch 371/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0409 - val_loss: 0.2573\n",
      "Epoch 372/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0483 - val_loss: 0.2378\n",
      "Epoch 373/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0496 - val_loss: 0.2693\n",
      "Epoch 374/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0528 - val_loss: 0.2671\n",
      "Epoch 375/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0530 - val_loss: 0.2374\n",
      "Epoch 376/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0619 - val_loss: 0.2380\n",
      "Epoch 377/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0576 - val_loss: 0.2474\n",
      "Epoch 378/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0591 - val_loss: 0.2661\n",
      "Epoch 379/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0550 - val_loss: 0.2641\n",
      "Epoch 380/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0478 - val_loss: 0.2329\n",
      "Epoch 381/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0406 - val_loss: 0.2246\n",
      "Epoch 382/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0456 - val_loss: 0.2315\n",
      "Epoch 383/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0438 - val_loss: 0.2399\n",
      "Epoch 384/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0465 - val_loss: 0.2470\n",
      "Epoch 385/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0479 - val_loss: 0.2324\n",
      "Epoch 386/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0543 - val_loss: 0.2554\n",
      "Epoch 387/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0451 - val_loss: 0.2305\n",
      "Epoch 388/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0761 - val_loss: 0.2456\n",
      "Epoch 389/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0535 - val_loss: 0.2483\n",
      "Epoch 390/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0418 - val_loss: 0.2471\n",
      "Epoch 391/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0720 - val_loss: 0.2247\n",
      "Epoch 392/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0805 - val_loss: 0.2348\n",
      "Epoch 393/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0596 - val_loss: 0.2238\n",
      "Epoch 394/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0521 - val_loss: 0.2114\n",
      "Epoch 395/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0421 - val_loss: 0.1993\n",
      "Epoch 396/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0583 - val_loss: 0.2323\n",
      "Epoch 397/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0409 - val_loss: 0.2247\n",
      "Epoch 398/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0443 - val_loss: 0.2211\n",
      "Epoch 399/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0444 - val_loss: 0.2380\n",
      "Epoch 400/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0399 - val_loss: 0.2285\n",
      "Epoch 401/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0490 - val_loss: 0.2630\n",
      "Epoch 402/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0412 - val_loss: 0.2605\n",
      "Epoch 403/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0404 - val_loss: 0.2301\n",
      "Epoch 404/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0600 - val_loss: 0.2469\n",
      "Epoch 405/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0443 - val_loss: 0.2586\n",
      "Epoch 406/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0418 - val_loss: 0.2365\n",
      "Epoch 407/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0414 - val_loss: 0.2479\n",
      "Epoch 408/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0563 - val_loss: 0.3388\n",
      "Epoch 409/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0555 - val_loss: 0.2318\n",
      "Epoch 410/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0427 - val_loss: 0.2583\n",
      "Epoch 411/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0517 - val_loss: 0.2738\n",
      "Epoch 412/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0402 - val_loss: 0.2221\n",
      "Epoch 413/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0353 - val_loss: 0.2483\n",
      "Epoch 414/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0447 - val_loss: 0.2510\n",
      "Epoch 415/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0557 - val_loss: 0.2601\n",
      "Epoch 416/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0602 - val_loss: 0.2730\n",
      "Epoch 417/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0511 - val_loss: 0.2457\n",
      "Epoch 418/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0443 - val_loss: 0.2352\n",
      "Epoch 419/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0394 - val_loss: 0.2396\n",
      "Epoch 420/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0466 - val_loss: 0.2795\n",
      "Epoch 421/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0435 - val_loss: 0.2580\n",
      "Epoch 422/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0552 - val_loss: 0.2164\n",
      "Epoch 423/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0489 - val_loss: 0.2427\n",
      "Epoch 424/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0455 - val_loss: 0.2268\n",
      "Epoch 425/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0486 - val_loss: 0.2145\n",
      "Epoch 426/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0430 - val_loss: 0.2583\n",
      "Epoch 427/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0396 - val_loss: 0.2357\n",
      "Epoch 428/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0501 - val_loss: 0.2679\n",
      "Epoch 429/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0475 - val_loss: 0.2728\n",
      "Epoch 430/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0388 - val_loss: 0.2556\n",
      "Epoch 431/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0513 - val_loss: 0.2579\n",
      "Epoch 432/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0464 - val_loss: 0.2632\n",
      "Epoch 433/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0401 - val_loss: 0.2611\n",
      "Epoch 434/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0807 - val_loss: 0.2462\n",
      "Epoch 435/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0625 - val_loss: 0.2832\n",
      "Epoch 436/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0443 - val_loss: 0.2408\n",
      "Epoch 437/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0340 - val_loss: 0.2306\n",
      "Epoch 438/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0387 - val_loss: 0.2481\n",
      "Epoch 439/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0390 - val_loss: 0.2600\n",
      "Epoch 440/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1396 - val_loss: 0.2412\n",
      "Epoch 441/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0759 - val_loss: 0.2419\n",
      "Epoch 442/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0439 - val_loss: 0.2594\n",
      "Epoch 443/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1449 - val_loss: 0.2421\n",
      "Epoch 444/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0611 - val_loss: 0.2653\n",
      "Epoch 445/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0678 - val_loss: 0.2484\n",
      "Epoch 446/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0635 - val_loss: 0.2347\n",
      "Epoch 447/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0459 - val_loss: 0.2539\n",
      "Epoch 448/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0470 - val_loss: 0.2610\n",
      "Epoch 449/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0358 - val_loss: 0.2504\n",
      "Epoch 450/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0424 - val_loss: 0.2456\n",
      "Epoch 451/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0707 - val_loss: 0.2343\n",
      "Epoch 452/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0427 - val_loss: 0.2323\n",
      "Epoch 453/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0438 - val_loss: 0.2267\n",
      "Epoch 454/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0576 - val_loss: 0.2636\n",
      "Epoch 455/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0442 - val_loss: 0.2785\n",
      "Epoch 456/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0511 - val_loss: 0.2533\n",
      "Epoch 457/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0460 - val_loss: 0.2697\n",
      "Epoch 458/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0399 - val_loss: 0.2637\n",
      "Epoch 459/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0424 - val_loss: 0.2498\n",
      "Epoch 460/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0449 - val_loss: 0.2344\n",
      "Epoch 461/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0353 - val_loss: 0.2231\n",
      "Epoch 462/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0317 - val_loss: 0.2538\n",
      "Epoch 463/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1147 - val_loss: 0.2333\n",
      "Epoch 464/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0585 - val_loss: 0.2592\n",
      "Epoch 465/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0616 - val_loss: 0.2522\n",
      "Epoch 466/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0479 - val_loss: 0.2288\n",
      "Epoch 467/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0421 - val_loss: 0.2615\n",
      "Epoch 468/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0963 - val_loss: 0.2514\n",
      "Epoch 469/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0424 - val_loss: 0.2398\n",
      "Epoch 470/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0468 - val_loss: 0.2460\n",
      "Epoch 471/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0362 - val_loss: 0.2675\n",
      "Epoch 472/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0503 - val_loss: 0.2478\n",
      "Epoch 473/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0430 - val_loss: 0.2315\n",
      "Epoch 474/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0530 - val_loss: 0.2581\n",
      "Epoch 475/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0430 - val_loss: 0.2320\n",
      "Epoch 476/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0528 - val_loss: 0.2276\n",
      "Epoch 477/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0397 - val_loss: 0.2294\n",
      "Epoch 478/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0630 - val_loss: 0.2783\n",
      "Epoch 479/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0765 - val_loss: 0.2757\n",
      "Epoch 480/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0548 - val_loss: 0.2527\n",
      "Epoch 481/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0507 - val_loss: 0.2514\n",
      "Epoch 482/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0356 - val_loss: 0.2634\n",
      "Epoch 483/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0469 - val_loss: 0.2524\n",
      "Epoch 484/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0389 - val_loss: 0.2526\n",
      "Epoch 485/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0448 - val_loss: 0.2595\n",
      "Epoch 486/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0449 - val_loss: 0.2768\n",
      "Epoch 487/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0392 - val_loss: 0.2824\n",
      "Epoch 488/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0403 - val_loss: 0.2498\n",
      "Epoch 489/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0326 - val_loss: 0.2689\n",
      "Epoch 490/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0506 - val_loss: 0.2843\n",
      "Epoch 491/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0320 - val_loss: 0.2620\n",
      "Epoch 492/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0460 - val_loss: 0.2406\n",
      "Epoch 493/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0338 - val_loss: 0.2534\n",
      "Epoch 494/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0457 - val_loss: 0.2640\n",
      "Epoch 495/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0475 - val_loss: 0.2398\n",
      "Epoch 496/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0569 - val_loss: 0.2278\n",
      "Epoch 497/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0537 - val_loss: 0.2654\n",
      "Epoch 498/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0512 - val_loss: 0.2233\n",
      "Epoch 499/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0429 - val_loss: 0.2471\n",
      "Epoch 500/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0360 - val_loss: 0.2591\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 24, 12)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 24, 24)       1800        input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "seq_self_attention_3 (SeqSelfAt (None, 24, 24)       577         bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 24, 24)       0           seq_self_attention_3[0][0]       \n",
      "                                                                 bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_3 (LayerNor (None, 24, 24)       48          add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 576)          0           layer_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 12)           6924        flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 12)           0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 588)          0           dropout_3[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            589         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 1)            2           dense_8[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 9,940\n",
      "Trainable params: 9,940\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Predict time:  0.3011934757232666\n",
      "RMSE:  23.29852426367362\n",
      "RMSE2:  17.136769127685508\n",
      "MAE:  16.027589001542044\n",
      "MAE2:  16.027589001542044\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'mse score')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAABRM0lEQVR4nO2dd3hUVfrHP2dSCYGEhE6AAIJIUZCgoIJdsWJXrNhQ195d+7r7s66uZbGtYlsbthUVwQKCCgiR3ntJKCmQ3mfO749z78ydySQZAkMg836eJ8/MLXPn3MnM+Z63nPcorTWCIAhC5OJq6gYIgiAITYsIgSAIQoQjQiAIghDhiBAIgiBEOCIEgiAIEU50Uzdgd2nbtq1OT09v6mYIgiAcUPz55595Wut2wY4dcEKQnp5OZmZmUzdDEAThgEIptamuY+IaEgRBiHBECARBECIcEQJBEIQI54CLEQiCEJlUV1eTlZVFRUVFUzdlvyY+Pp60tDRiYmJCfo0IgSAIBwRZWVm0atWK9PR0lFJN3Zz9Eq01+fn5ZGVl0aNHj5BfJ64hQRAOCCoqKkhNTRURqAelFKmpqbttNYkQCIJwwCAi0DCN+YwiRghWbS/m+R9WkV9S2dRNEQRB2K+IGCFYl1vCK9PWkitCIAhCI0lMTGzqJoSFiBGC+BhzqxXVniZuiSAIwv5FxAhBXHQUAJXV7iZuiSAIBzpaa+69914GDBjAwIED+fTTTwHYtm0bI0eOZNCgQQwYMIBff/0Vt9vN2LFjvef+61//auLW1yZi0kfjoo3mVdaIRSAIBzp/+2YZy7cW7dVr9uvcmsfO6h/SuV9++SULFy5k0aJF5OXlMXToUEaOHMlHH33EqaeeykMPPYTb7aasrIyFCxeSnZ3N0qVLASgoKNir7d4bRIxFEB9jWQQiBIIg7CG//fYbY8aMISoqig4dOnDssccyb948hg4dyjvvvMPjjz/OkiVLaNWqFT179mT9+vXceuutTJkyhdatWzd182sRcRZBhbiGBOGAJ9SR+75m5MiRzJw5k++++46xY8dy1113ceWVV7Jo0SKmTp3K66+/zsSJE5kwYUJTN9WPiLEIvDECsQgEQdhDRowYwaefforb7SY3N5eZM2dyxBFHsGnTJjp06MD111/Pddddx/z588nLy8Pj8XD++efzj3/8g/nz5zd182sRORZBjB0jEItAEIQ949xzz2X27NkcdthhKKV49tln6dixI++99x7PPfccMTExJCYm8v7775Odnc3VV1+Nx2MGoU899VQTt742ESME8ZZFIOmjgiA0lpKSEsDM3n3uued47rnn/I5fddVVXHXVVbVetz9aAU4ixzUkFoEgCEJQIkYIYqMsIRCLQBAEwY+wCYFSaoJSKkcptbSB84YqpWqUUheEqy0ALpciNtpFhVgEgiAIfoTTIngXGFXfCUqpKOAZ4IcwtsNLXLRLLAJBEIQAwiYEWuuZwM4GTrsV+ALICVc7nMRFR0n6qCAIQgBNFiNQSnUBzgVeC+HccUqpTKVUZm5ubqPfMz7GJcFiQRCEAJoyWPwicL/WusEhutb6Ta11htY6o127do1+Q3ENCYIg1KYphSAD+EQptRG4AHhVKXVOON/QuIbEIhAEIfzUt3bBxo0bGTBgwD5sTf002YQyrbV3ZWWl1LvAt1rr/4XzPeNiXBIjEARBCCBsQqCU+hg4DmirlMoCHgNiALTWr4frfesjPjpKis4JQnPg+wdg+5K9e82OA+G0p+s8/MADD9C1a1duvvlmAB5//HGio6OZPn06u3btorq6mn/84x+MHj16t962oqKCm266iczMTKKjo3nhhRc4/vjjWbZsGVdffTVVVVV4PB6++OILOnfuzEUXXURWVhZut5tHHnmEiy++eI9uG8IoBFrrMbtx7thwtcNJy7gosguq98VbCYLQzLj44ou54447vEIwceJEpk6dym233Ubr1q3Jy8tj2LBhnH322bu1gPz48eNRSrFkyRJWrlzJKaecwurVq3n99de5/fbbueyyy6iqqsLtdjN58mQ6d+7Md999B0BhYeFeubeIqTUEkNQidq8vZiEIQhNQz8g9XAwePJicnBy2bt1Kbm4ubdq0oWPHjtx5553MnDkTl8tFdnY2O3bsoGPHjiFf97fffuPWW28FoG/fvnTv3p3Vq1czfPhw/u///o+srCzOO+88evfuzcCBA7n77ru5//77OfPMMxkxYsReubeIKTEB0CYhhoJysQgEQWgcF154IZ9//jmffvopF198MR9++CG5ubn8+eefLFy4kA4dOlBRUbFX3uvSSy9l0qRJtGjRgtNPP51p06bRp08f5s+fz8CBA3n44Yd54okn9sp7RZRFkJwQQ1mVm8oat3d9AkEQhFC5+OKLuf7668nLy2PGjBlMnDiR9u3bExMTw/Tp09m0adNuX3PEiBF8+OGHnHDCCaxevZrNmzdz8MEHs379enr27Mltt93G5s2bWbx4MX379iUlJYXLL7+c5ORk3nrrrb1yXxElBEkJsQAUllfTvpUIgSAIu0f//v0pLi6mS5cudOrUicsuu4yzzjqLgQMHkpGRQd++fXf7mn/5y1+46aabGDhwINHR0bz77rvExcUxceJEPvjgA2JiYujYsSMPPvgg8+bN495778XlchETE8NrrzU4HzcklNZ6r1xoX5GRkaEzMzMb9dpvFm3l1o8X8OOdI+ndodVebpkgCOFkxYoVHHLIIU3djAOCYJ+VUupPrXVGsPMjKkaQnBADIHECQRAEBxHlGkpuYVxDBWUiBIIghJ8lS5ZwxRVX+O2Li4vjjz/+aKIWBSeyhMC2CMqqmrglgiA0Bq31buXoNzUDBw5k4cKF+/Q9G+PujyjXkL1cZZVbykwIwoFGfHw8+fn5jeroIgWtNfn5+cTHx+/W6yLKIrCXq6yWekOCcMCRlpZGVlYWe1KKPhKIj48nLS1tt14TUUIQYwuBW0YUgnCgERMTQ48ePRo+UdhtIso1FB1lfIviGhIEQfARUUIQ47ItAhECQRAEm4gSApdLEe1SIgSCIAgOIkoIwMQJaiRGIAiC4CUChUBJjEAQBMFBxAlBbLRLXEOCIAgOIk4Iol0uqmvENSQIgmATNiFQSk1QSuUopZbWcfwypdRipdQSpdQspdRh4WqLk5hoCRYLgiA4CadF8C4wqp7jG4BjtdYDgb8Db4axLV5iolwSIxAEQXAQzsXrZyql0us5PsuxOQfYvTnRjSRWsoYEQRD82F9iBNcC39d1UCk1TimVqZTK3NM6IzFREiwWBEFw0uRCoJQ6HiME99d1jtb6Ta11htY6o127dnv0ftGSPioIguBHkwqBUupQ4C1gtNY6P6xvtm0xTHmQthSKRSAIguCgyYRAKdUN+BK4Qmu9OuxvuHMdzBlPqiqS6qOCIAgOwhYsVkp9DBwHtFVKZQGPATEAWuvXgUeBVOBVa8WhmroWVt4ruMzqZHEuD9XVYhEIgiDYhDNraEwDx68DrgvX+9ciyhIC5RGLQBAEwUGTB4v3GS6jeXFRbokRCIIgOIgcIbAtAkQIBEEQnESOEFgxgliXR9YsFgRBcBA5QhDlE4IqiREIgiB4iRwhsGMESlxDgiAITiJHCGyLQLmpESEQBEHwEjlCYMUIYpRb0kcFQRAcRI4QRNmuIQ9Vbg9uj4iBIAgCRJIQWBZBq1gjADtLq5qyNYIgCPsNESQExiJoHasA2LKrjH6PTmHK0u1N2SpBEIQmJ3KEwAoWt4oxFsHsdfmUVbl5ZdqapmyVIAhCkxM5QmBZBC2NHrBsayEAHVvHN1WLBEEQ9gsiRwgsi6ClZREs21oEQHsRAkEQIpzIEQIrWByvPMRGudiUXwZAu8TYpmyVIAhCkxM5QmBZBMpTQ6p0/oIgCF4iRwiUAhUFnmraJsZ5d9fIfAJBECKcyBECMFaBu5q2DotAJpYJghDpRJYQuGLAU0OqWASCIAhewiYESqkJSqkcpdTSOo4rpdTLSqm1SqnFSqnDw9UWL1HR4K6mdXyMd5dYBIIgRDrhtAjeBUbVc/w0oLf1Nw54LYxtMbhiwFPtt6vGI5VIBUGIbMImBFrrmcDOek4ZDbyvDXOAZKVUp3C1B7BiBDV4tM8KEItAEIRIpyljBF2ALY7tLGtfLZRS45RSmUqpzNzc3Ma/oysaPNWcO9j3NjVSkloQhAjngAgWa63f1FpnaK0z2rVr1/gLWVlDh3VNZuPTZ9A1pYVYBIIgRDxNKQTZQFfHdpq1L3wExAiiXS7JGhIEIeJpSiGYBFxpZQ8NAwq11tvC+o5R0eCu8W26lFgEgiBEPNHhurBS6mPgOKCtUioLeAyIAdBavw5MBk4H1gJlwNXhaouXWhaBkqwhQRAinrAJgdZ6TAPHNXBzuN4/KFaMwLspFoEgCMKBESzea7iiweNzDRmLQIRAEITIJiQhUEp1V0qdZD1voZRqFd5mhQmxCARBEGrRoBAopa4HPgfesHalAf8LY5vCR3QLqCn3bbpcMo9AEISIJxSL4GbgaKAIQGu9BmgfzkaFjdgEqCrzbopFIAiCEJoQVGqtq+wNpVQ0cGD2nrEtoarUuxkdJVlDgiAIoQjBDKXUg0ALpdTJwGfAN+FtVpiIaQnVYhEIgiA4CUUI7gdygSXADZj8/4fD2aiwEZtgLAKr6Fy0S7FpZxn5JZVN3DBBEISmo14hUEpFASu01v/RWl+otb7Aen5gDqNjW4J2Q43p+KNcioKyao5+ZloTN0wQBKHpqFcItNZuYJVSqts+ak94iWlpHi33ULTL3H5FtcQJBEGIXEKZWdwGWKaUmgt4I61a67PD1qpwEZtgHqtKICGFKJdq2vYIgiDsB4QiBI+EvRX7iljLIqiyLQIRAkEQhAaFQGs9QynVARhq7Zqrtc4Jb7PChNc1ZAybAzPQIQiCsHcJZWbxRcBc4ELgIuAPpdQF4W5YWPC6howQVNVIbEAQBCEU19BDwFDbClBKtQN+wpSdOLAIcA1VuUUIBEEQQplH4ApwBeWH+Lr9jwDXkFgEgiAIoVkEU5RSU4GPre2Lge/D16Qw4rUIRAgEQRBsQgkW36uUOg84xtr1ptb6q/A2K0yIa0gQBKEWoQSLewCTtdZ3aa3vwlgI6aFcXCk1Sim1Sim1Vin1QJDj3ZRS05VSC5RSi5VSp+/2HewOMVawWFxDgiAIXkLx9X8GOHtMt7WvXqzyFOOB04B+wBilVL+A0x4GJmqtBwOXAK+G0uhGEx0HKkpcQ4IgCA5CEYJoZxlq63lsCK87AlirtV5vveYTYHTAORpobT1PAraGcN3Go5RVilpcQ4IgCDahCEGuUspbTkIpNRrIC+F1XYAtju0sa5+Tx4HLlVJZmKqmt4Zw3T0jJkFcQ4IgCA5CEYIbgQeVUpuVUlswZalv2EvvPwZ4V2udBpwOfKCUqtUmpdQ4pVSmUiozNzd3z97RsThNckLMnl1LEAShGdCgEGit12mth2H8/IdorY/SWq8N4drZQFfHdpq1z8m1wETrfWYD8UDbIG14U2udobXOaNeuXQhvXQ+O5SrfuiqDFjFRtIoLJYtWEASheRJK1tDtSqnWmMqjLyql5iulTgnh2vOA3kqpHkqpWEwweFLAOZuBE633OQQjBHs45G+AmJam+ijQKakFVwzvTrUsVykIQgQTimvoGq11EXAKkApcATzd0Iu01jXALcBUYAUmO2iZUuoJR8zhbuB6pdQizIS1sWFf9CbWf7nKaJeixi3l5wRBiFxC8YnYtZpPB963OvOQ6jdrrSdjgsDOfY86ni8Hjg6xrXuH2AQospKTSnKIcWlqPBqtNSHeliAIQrMiFIvgT6XUDxghmKqUaoX/vIIDi9hEEyyuKoOXBtE/byqALGIvCELEEopFcC0wCFivtS5TSqUCV4e1VeHETh+tKoXqUlpX5wK9qfFooqOaunGCIAj7nlBqDXmA+Y7tfEwF0gMTO2vIU202tVnIvtrtIT5GlEAQhMjjwCwnvSfEJ0FNOVQWAz4hkICxIAiRSuQJQVI387hzPQAxtkUgKaSCIEQoIQmBUuoYpdTV1vN2VkXSA5NkSwjy1wEQ4xGLQBCEyCaUCWWPYcpK/NXaFQP8N5yNCiteITCTo20hkKwhQRAilVAsgnOBszEzi9FabwVahbNRYaVVR3DFeIUg2lMBmGCxIAhCJBKKEFRZs301gFKqZXibFGZcUZDYwTupLNp2DYlFIAhChBKKEExUSr0BJCulrgd+Av4T3maFmbhEqCgAINptLII7P11IQVkVE+dt4br35jVh4wRBEPYtocwj+KdS6mSgCDgYeFRr/WPYWxZOYltC3moAoiyLYNnWIsZPX0tplZtf14Sy3IIgCELzIJRgcUtgmtb6Xowl0EIpdWAX8o9tCdrEBGyLAODnFTlU1XiorPFIzEAQhIghFNfQTCBOKdUFmIKpPvpuOBsVdmITvU9VjU8I1ueVsq2wHIDSypp93ixBEISmIBQhUFrrMuA84DWt9YVA//A2K8zE+uLdLssi6N/ZLJ2cX2KWZy4RIRAEIUIISQiUUsOBy4DvrH0HdlEehxDE6krevXoot57QG4CCMlODqLTS3SRNEwRB2NeEIgR3YCaTfWWtRdATmB7WVoUbp2uoupzjDm5Pq3gTN99VJhaBIAiRRShZQzOAGY7t9cBt4WxU2HFYBGg3uKtJiDVGTmWNCRKLEAiCECk0KARKqQzgQSDdeb7W+tDwNSvMxAbMiasuIzFgAXsJFguCECmEsjDNh8C9wBJ2c2UypdQo4CVMTOEtrXWttY6VUhcBj2NmLi/SWl+6O+/RKGoJQQUJca39dolFIAhCpBCKEORqrSft7oWVUlHAeOBkIAuYp5SaZK1TbJ/TGxN/OFprvUsp1X5336dROGIEANSU0zKujd+ukgoRAkEQIoNQhOAxpdRbwM9Apb1Ta/1lA687AlhrxRRQSn0CjAaWO865Hhivtd5lXTNnN9reeGIS/Lery0loJa4hQRAik1CE4GqgL6b8tO0a0kBDQtAF2OLYzgKODDinD4BS6neM++hxrfWUENq0Z8QFFE+tLic22kVslIsqa0ZxSZUIgSAIkUEoQjBUa31wGN+/N3AckAbMVEoN1FoXOE9SSo0DxgF069Ztz9+1hb8byF62MiEuiqoyD7/E3snSrDHgedRUKxUEQWjGhDKPYJZSql8jrp0NdHVsp1n7nGQBk7TW1VrrDcBqjDD4obV+U2udobXOaNeuXSOaEkCLZP/t98+GDTNpGRtNNDWku3ZwZvaL8Hy49E8QBGH/IRQhGAYsVEqtUkotVkotUUotDuF184DeSqkeSqlY4BIgMOj8P4w1gFKqLcZVtD7UxjeaQIsAYPVUWsZFkWTW3zGU5oa9KYIgCE1NKK6hUY25sNa6Ril1CzAV4/+fYM1MfgLItDKRpgKnKKWWA27gXq11fmPeb7cISBW1GkzLuGjcqiTsby8IgrA/EcrM4k2NvbjWejIwOWDfo47nGrjL+tt3KFV7n3aTFB+NCxECQRAii1BcQ5HBwo95d/MpHOTa2tQtEQRB2KeIENhUFgJwvGth07ZDEARhHyNCEMBhrnX+O7Qsai8IQvMmcoXg0s9g9Phauzupnf47HCuYCYIgNEciVwj6nAKDL2/4vKqy8LdFEAShCYlcIQiVahECQRCaNyIEDVFd3tQtEARBCCsiBGe/Av3Oqft4dWnw/VmZkLMyLE0SBEHYl4gQHH6l+auLuiyCt06EVwOLqQqCIBx4iBAAJKWZR1c0dBtOReuevmMSLBYEoZkjQgDQuot57DQIrplCdVK675gEiwVBaOaEUnSu+ROXCJdOhM6DAYhyOWoRbVsIPUYEr1gqCILQDBCLwKbPqZBolkz2E4Jfn4f3RzdRowRBEMKPCEEw2vX13962qGnaIQiCsA8QIQiCOvFRxlQ95NsRHe9/gseDIAhCc0GEIAixcXHM9vTne/dQsyMh1f8Ed+W+b5QgCEKYECGohzuqb+a32KOhfJf/AWchOo973zZKEARhLyNCUA+VxLKCXiaFtNKxclmNwyKQEhSCIBzghFUIlFKjrEXv1yqlHqjnvPOVUloplRHO9jSGHE+SeVKa49spQiAIQjMibEKglIoCxgOnAf2AMUqpfkHOawXcDvwRrrbsCWvLWwKwbv06KmssN5BTCJZ+AY8nQdG2JmidsEc8ngST72vqVghCkxNOi+AIYK3Wer3Wugr4BAiWkP934Blgv1wBZosnBYBPv/qCpz//DapK/WMEmW+bx42/NUHrhEZjx3bmvtG07RCE/YBwCkEXYItjO8va50UpdTjQVWv9XX0XUkqNU0plKqUyc3Nz935LgzCwSxI927WkPL4jAA/GfMxjK8+G98/xtwjsGccFm0K67nuzNvJZ5paGTxTCi7j0BMFLkwWLlVIu4AXg7obO1Vq/qbXO0FpntGvXLvyNA7659Rim3X0crZNT/A9kzYX8Nb7tymLzuGNZSNf9dN4WJi3aupdaKTQaWYJUELyEUwiyga6O7TRrn00rYADwi1JqIzAMmLS/BYzbJsZ6n2+lrXni7PQLrNF9UWide3m1m8qaBiakFW+XtQ7CjRQTFAQv4RSCeUBvpVQPpVQscAkwyT6otS7UWrfVWqdrrdOBOcDZWuvMMLZpt0ltGUu5NmLwlxpjvOhyxwL3VZZFUFEY0vXKq9xUuxsQghcOkbUOwk21ZRGoqKZthyDsB4St+qjWukYpdQswFYgCJmitlymlngAytdaT6r/C/kFqYhwjK18klmpiVQ1EQ86ObXQIPNEpBDVVUJYPrTvVul55tZuqhiwC7ThetM1YG2lDGn0PQhBsi8AlBXgFIay/Aq31ZGBywL5H6zj3uHC2pbGkJsaSSzIAHbSxBFwVBQDopDRUYZY5sbLIZKK4q2DSrbDkM3gkH6L8P+LyqhCEwMnrRxtReTw0i0MIETtGIEIgCLIeQUO0bRnnfV6OcRHFVptO2dPtGKKWfGIOVpUYl055ga8WUWURJPiCzTVuD1VuT8OuIRutjQiAsTKiY+s/Xwgdr0UgriFBkBITDdAp2Vd5tBzzPKlkPbiiKR35GN+6j+Rnt1nQhpId/gXp8tfCrk1QthM+HkNlgQkoh2wROFMcS3MhZwX8+d4e3c8BSVUpuGv27jXtGEEkCcG2RbB4YlO3Iny4q83gSdhtRAga4IgevhH9T/ee5DswZCzlcSncUn07k911BHbfPhleOhT+fBdWTUbNfhWAKneIX1ZnZktpDrw+Ar65zdcpTr4PFn7s/5otc43w2JTthNK8+t9HazPLdtr/hdaufc2TneGzq/buNe3PNpKCxW+MhC+vb/zr3TWms90fKdsJf28Ls8c3dUsOSEQIGiAuOoobRvbkmqN70D21pe9Ax0OpqDazU4tIqP8i7ioAVN5q/h49AU9NiGWsq0q9TwvztoLH+hGW7DBrIsx9A/53o+/8ymIjPl9c59v3bA94rlf972O7n2Y+W3/JhR3LYdfG0Nq+t7A7npXf7t3rRnKMwFkxV2v4442GBwtgMtmeOyh87doT7PTthR81bTsOUEQIQuCvpx/Co2f5l0l6f1Exz05ZBUBxQ0KQa85rsfFHroj+iQHuVZA9H57vC6X5/ufWVPmeOyyCrVmOmcvF26Eoq/b7bF9qHneuq789gexyXHvuG7BysrfNfrw2HF46bPeuvac4rZtAqsr8P6/dwXa7HWiuofJd8MllUJLT8Ll14cxwy1kO398HX93Q8Ovy14KVKLHfoS1xU6r+84SgiBA0kklrKvluiSk058Ly+Sd3D37yup/9NjNYAr/9C4q3wfrp/udWOcpdO37scRWOH37xVvOjtCkvMCO77UvMdpseu3MrsGuD//YnY2D8Ef6lt5uKMsdIdd5b/see7ATvndnwNfLX1XZpHKhCsPAjYx3N/Gfjr+HszG3rtHhH6K9vrPiGE/v/KULQKEQIGskuWnmfz/P0ZVufy+HcOgqYOUZgWzztGKaW4Ym13EyVRf7nOrcLfTWJknPm+fYXbTOdm80z3WH+e5BjzXiOdbiwQqEud8+yL+v3CVeV+mZWhwuny+K7uyEr0zzas7u3NFC0tng7vHI4/PS4/35vx2EJQVXpvvN/79rY+OC3vWxqdWn959VHeYHvue1+1LuxwFIwa7SpsQctSrq0xiCfWiPZpX1CUE00ywc/Cin1jMQv/Yw1g+7nW88wBqu1aLf1wwssTWHXLgLT6VmkbHdUNy3eajo4J5v/gBKrIF9VkJF8sHWWV34Hz6RDbh3lLCbdCtOfNM+DdVzf3gUvDvDvWBrDt3fBH28GP1YW4Dp760RjGbx2lG9ffZkieVZdqHXTTKZQhSW0ttvN7gCf7AyfXr77bd9dCrOMe+2XJxt5Aeted7donvMzcloE9iBF78bcloLNu/fe+wJ7hr8IQaOQT62RFOI/6n5jxnpKPDH+J3XJgN6nwJVfQ59TWN3zamZ5+hOj3LjWTDHn7AqoWlrhsAj+fAcSO8LhARkzJblGMGISYNQzEJdkOja703SKic2sl/zFo6YKvhxnfM4bf4e0oXDRB7Vfl22JUaDlsuJbWGzNoVhgvW7pl7BpNnx+Daz5yf/8X56BJ9oGF6TMt+H7e2vvh9pCEOo5WsN/z4fv7zfbMQnGjfS0Vf7KDha7q31tWj2l/vfJ+tOkYNo0ZlRv15BaN233Xws+0Q2xpIkXZwaaU7hDFYJqR5G+QrEImhvyqTUSN/6+5bkbdzL2gyW+HcNvgau/h8s+g57HAaa8xDzPwVTqGJTdsW6d7z9aKwnw1R57H3Qe7N3cGN3TzCmoLIaEtjDsRuh8mLEs6hOCnx6Ht0/xCc3nV/ssh+KtJr6RWKtwBrS0qr0GltD49DLf9q/Pm7TVz6+Gd0aZxXo+PN8EwssLTJbKL0+arKd5//G/fmBxvdzVJpV1w0yzbbuGrv0RHs6FqLja7Vzyee12b5gBa3/yucu0B7Is91p1ue8zqiqFr2/2f21dFsZbJ5gUTDCd4dNdYcaz8L+bg3/mwchZbh5jE2sfK8mBqQ/Bsz1heR0VWOzR/NqfasdM6sO57nYwi8DOJMqeD1/eUHst7qWOzzhwDe/6mPWKGWiEG9vFVZcQ1FT6l48PxuY5sOC/e7ddu8OSz2HrQpP1lz1/n761CMFuojvXXfMnM8vhkmnVsdZM4PKqGiqIY47nEN/O/LWw5gfzvLIEVnzjf9GMa6DPqd7NXVEplhAUQZzlnmrdJbgQBPq8CzbB5tnmeWA6Zpvups2BxLQwj04hsDtp743tgmn/8G0npJrHeW+Z+MXPT0AnK9vo+/vgi+uNSBRtrV1cb5E1L+LXF2DyvTDjabPmQ9cjzOf51y1w81xzzoi7IambSXsNZG6A4Gxb6Huev86XjVRZBIscKYceN7xzOnxzO7x3Nqz/pfa1wXQY1WUw/f9g4X+N9RHoMtm2GBZ96r/PFgJbhMt3+cR5/nsw+9/m/zjxiuDv6+yEl/3P91xrY5HZI2OPxz/Y78y+si2CDb9C3mrr/BrzP/7P8cbS27neiM3n15g4kFMsQ3UFejzww8Pw7umhnb8n2K4hlLG2bDHPWWmev3w4vDTI/zVaQ6GjIPKEU2sPCupjyefwVLfQBwE2m2bD84f4D2BWToYvroU3jzXlab69c/euuYeIEOwm6pop9K14p66jvqdBRnzl1ryDr92Wf7t1F0jtbf7p5QXw6jAToLVp2c5kQbTuTG6n4/io5gSKXMlmlOwnBJ1NAM+uilpZbDqhYF/QnBXmMT7Jf39yd0hKq31+RSGsmgK/v+Tbt+EX3/MUa46Cs6Pte6bp+G0/+MrvzGgsxnKnLZkIC943WVNOqkp9LpP102GuFTewhQUgOg5aJMNDO+CER+DIcabjtDs6rc1oapVfiSt/18f7o/3XoHay9AvYPMtMAtwww2R3uatrf5brf4HoFr7tLX+YGMNXN5m4i8cNb4yAr8b5u/vsIHfBZmOZPHeQsSwqS4xVFfh52GyeY74nWZnQsj206uS/psKaH4xFNuMZs/3Lk/BUF981nJZm8TYzQ/29M437Ecwg4YX+vnP+eN2I29IvjDA6CTWFNNC6zVtr7nfxRPj0CpPlVlNlPss9zUSyRS9rLnxwLqz63nzvXj0Sln1lfh/FW/1dk9/fB//qB1vm+V+rKqBE+a5Nwa3Er26EykJ4rrcZydfF1gWw5kff9pY5pi3THRM4nYMRMLErrc2fbVmHERGC3SU6lgriGj6vZdtau8qrzJdwkucoKlP7mY7svDeM737yvX5ZQl/0+Bvc6AsQzzvqdR6suY4CV7LpxCocQtDB8QNu0caMNt8YAb+/6NvfyqqE+tNjZuRYVQa9TvAdb9PdP5Xy1vnQYaAZtX98sb9AObOMbDeN02JISoNuw33bbfsYt8dhl0Cf08w+5fIFt20WfmQ6h8QAyySh9mdJTLwRSVuIln1lHhd9Yka12gNpR5h9fUZZn0Fn81iWB9l/1r4mmJm38UlwzF1me/0v8OGF/iPHyhIoyoZDzoLRr/r2F2wxP+jyXfCEY0GjTbOs+/sYti82mT9l+TDlr2YkDsbVFCgEOx1pvb88BZkTTGA/9SBzT84U4uVfW22wYk5zXjePO5abTv/DC8x2YkczGFgbEMMBx6gan9sppRcUBlg6wTqlgs3+nXlJDrzQ17f91U1GVEpzzWe8YhL88Iixgt4fbRITdodti2D8kea+c1bUtsYqi8zIGmDHUt/+neutfct9A43/ng//Gug7p3CLuf/SfJj9Krw82Hw3l3/tK/HirvZN8Kwph7dO8onFplmmPd/eaSySN48zn3/RNvPdsL/3OzfAq8ON+2zDr/7tz1trhGrCqfBcT3hxoPn/h7gA1u4iQtAI4qJD+Nhs37qDsmrzo68hmrXnT4FBY6DLEGjXF1ZP9Tv37hW9/Vw1dqG6ApVkRoLF2yG+tTnY9yzfC1s7RvXOUXxca/MHZuToqYaDTvYdD5wD0bozJHUJ3mHu3Oh7HmhZAETFQOfDfdtVxcZaSWwPYz42KZvlBVASkPk0+R4zEhp6rf9+p0UQSEpP8/jdXaYz3TDDd6yrJQRpQ+G2BXDT73BLHQLgpN0hcNJj0HWY2V4/3T9AOukW80Nv3RkGXerbX9eqZzOfNSN5exb4ac/AwWf4RuNgRq6BI+1Zr8BiqzNzTvBzRRkxcHYqm+eYx+Vfw7tn+jr1bQvN52rT6wTTmdiWYSAPOf4nUbFw0fu1rdvsTJgwysRyFn5sBgEvDvR/n0D34aKPaseH8tf5/l9ZDhFc9j9454z6s8HWzzCiOOm22pY0mM5/lRX8dwrstoXmf/maY6BSWegvdr8+b1KUn+sJU/9qvpNzXoOJV5oSLyW5ZiDgxFNt3K6T74N3TvN13M5R/wt9jXh5rVFtXIU/PGz+9+0s4WzZzojL3P/42l5ZZNoULB62FxAhaASJcSGUJQgiBBVVvgCcX+G5pC7my2iRpWuPgKut+kSFKtnsKNnuswiirR8s+MUT/Oh1PIwNiAu07WOyaZTL5xbqf555jGkB8cnBr7XDERSPijEdBsCACyB9BAy63LTj8CuhwwATAAYjBEoZ105FgX8WU1dHrOCwS/zfL6qez7tNuu/5f070xRhO/rtvtB0dZwQjIQXaHmTaBPi58sBkYIHpZMGItI1z9G1bH0lp/hOYgqV0nvWyEdO3TvTt63okjP533fdks/gT+PI60+EXb4PjHjT3ddwDvgSCld8aUd253ifKGx2jy+z53hInAHQ61Iiyc7nVfqON1TX0evN/P/NFOP9tuHsVdBwA920wAnHef4yQ7NroizXNeNr4vME3AgfjDrEZ9hfzWBpgARZuhrWWK3DXJjPKnj3e1JXa9JuZ1Pjy4bUtJfBZpXW5qWa9Yv7/KT39P4/tS/wt2pH3wpn/Mt9Zm8UBcR3w/85/d6dv/kynQTDiHnDFmBH83IC5RCsCgv4lO8xfmx6+35crGgacD8Ot+IT9eaHxpguDsXIHOZI09iIRWGhlzxmYlsQvq3LrPymIENgxAvB17ICJFYDpkG/J5MznjM+yxu0hOsrlfQ6wOOZQiG1lRny2EID5MT+6E1Aw+HJ4eZDv2AkPw9F3mE6746HGPXHEDdDzWPNldEWbY2B+7GdZlkSL5Nr3lXqQ6RTjk43ffNhffF/2bsPgCEdRs7NfMSmq9g/Ldk/FJ5t5DzkOM3fIWN/ksORu8Nds4+P9alztDBYnMfFw3c+mo7UtjJP+BkffZvz7YEbuTtr2NiPGxA7+VsnQ68xiQlaWFyc+atxIiz+FNf4Wm9/9eAkYwZ7xPAy5ynwu39xhROPCd40gNoSK8s1xsIPCnQ6Dgy03ly06395h/sB04Kun+tJ6obbv+ZCzYMoD5vkJD0PmO8ZFmdILXNa4MONq/9fYSQ+HXmTcik52bYTpVqJAdZmxQtv0MEFvMN/L9o7kiKNug1kv+7arS+GQs813aNn/YOqDvmN2IHvhR0b47O/o9qUm5bghTnrcxEicczZ+f9HfZXroxeb7cPAZRkw3z6r/mic8bBIjWliuvwsmQGov831qKP3YZudGk+l3wTtG/GKtEjUej4kZdhtmPsfoOP84S3J3M5AJA2IRNIKXxwzmP1c2sLSy1UlnF5R73Trl1T4rwM8isIUgIRVSe1FgzVourXQIh8d0MrmutnCU5U8NzGN3RZkfc0oPnz8coPvRvh/R2S9DxrUw6imzLyHFxAdsoqJ9LqdgFkHaUPPYtg88thO6O0zs9BG1z3e6dToNMo8tkv1FAMwoudeJcJll+sYlmk4eGs5xT3P8Lx7OMZ0NmBTe89/2WTk2yd3Mo52Bk9TVdJBR0abjskfWMfE+C2vdNBh4of91bKG04xTOuMhJfzPCAtDuYLjme7h6sr8IXPY5nPGCb/suh7vmPMcEu2/vMFaX7eoCM3Lvcax5bv+f0oaamJPNkLHm0Z6NnNTNWDHnv20slRH3wF3LTUfoCrErsDtnJ9sdo+UfHzUZTx0Hwu2LjKWaZM3dSOwIxz9kRun2/JeDTjb/JzDWTyBpQ40L7bOxJgHik8uM6yUUeh5nrFL7ve3PwYn9XWjVwfyPbE75Pzj1SbjmB/iL5XZLPQiOvMm4WOdbsQKvJX1u8DaMegYunegTDjCWUGIH8/uzRQDM/6D7cDNgOPuV2vN6QhlANBKxCBpB6/gYTu7Xgen3HMeWnWVcOSGI6aoUhWXVHP30NK4c3p0nRg+gvMrXcfstTmN3KG37+F2iqKKapATTgdsWgQZjroNfcLkW10wxJbDBP3+682C/eQmMetr/y+iknX97GPWMzw9u5WtrralJ7kVMwTpo35da2AvzJKT6lu60Oy7bsgET5L4iYLRpxy26HE5DeI6+i183l3NYlSI5werUomJg4AW1T+55vDXiijfrRwy/GYbdFPzC7Rz3NPAin/tjxD1GYMEE9T01xuUy9SHTubcKMicjkN5WjOY7KzDdujOc/k9fu/ufa/zSK781zx2LHAFw+Re++ywv8H2PTvmHCfJ3GGCyn3qfYgYALmswEOwzCZX2h5iihvdvNAH0L683fu6LPzRiPukWk9V2yYe+GJdtOaX0NOJ6m+U2OvIG0+k5J6uNfhVyV5iAa79zfEHtld+aEbIz7fnQi40FdMHbRoTzVpuYhe2eadvbvObW+abjfaqL/73cucwcd3Ll10bMhlzlb3Hf8Ku5n7hEc2/vWXE5+/UHn2bE2l1lhHb6/5lg8zArLnT55zD9KWM5FG/z/14Fo9/Z1j1e4rPwgngZ9hZhFQKl1CjgJcyaxW9prZ8OOH4XcB1QA+QC12itN9W60H5Kj7Yt6dG27ro+uSWmA5652riRyqvdJMRGUVbl9pawBkyHkD4CznnN7/UllbWFw6O1GUUNuty4P+qiTXe44n/wwTn+WUW1biLIKN6mi2OkbS+VufI782gFvN6dtZHnt/+VieOOoB9BSE43j33P8O2zO6zuRxlX0qyXg1sfnQfBDTMdPv26mdJxHH/5eT4XfLeCf17YQIXUXseboPGsl2D++/UX6evQn+/bX4fevpTTe59sfoyluWZka8cHbCGNb206id3lpL/5sq+crjVXlPFf9z8XDg6Si29beeDvxrMtxspi00mPuNsI7d7gnFeh4AFzvRZt4C+zjX/ftiovDeJfb3ewcT8dNsZ/v/35xcTDVd8Yq2vQpf5xl17HG3fRhpkm68jbjtdMTMo5V8ce5JTlweoffJ10ah1l2IOlS/c8znznAul0qO95j5Fw8zz/mEd8komxRMcZoe17Jn6uwi5DjBiU5JhEA2f8qT5Gjzeu3u/uNt+5MBE2IVBKRQHjgZOBLGCeUmqS1nq547QFQIbWukwpdRPwLHBxuNoULs47vAtfzjdZBEMrXuXyI7pwO1BYboJ0cdEmLbO8yk3v9okszi5k9Y4STrMz1tqk1w7kAsUVTiEwXyqPB/PlPyeEBTh6HV/vWseFZdW0jIvyxiFqYZvNrR0jKdtqsWb8/romjxISyCqPCy4EA84zI3pnZ2unRfY81oigPTIORqcGOnULWyj9BLY+2h4EJz5uRqm9T6n31Js2nwCcwEalYNwMMyIO1ZUSCsfcUfexxPaNH8HHtYJT9/JiQ/FJxu3jxOlaDIZSMPKe+s/pMdL8BdJxoBGJt04yM8O7H23cTUHSs71cMCF4xtGlnxmLYut8M0N9T2jXJ4jF7IhFxARxQ4H5f+6Oiycq2gzWbgniddiLhNMiOAJYq7VeD6CU+gQYDXiFQGvtrME8B9gHVb/2Pi9cNIjSyhqmLttBLsksKU5kytJtKGtks7WwnB1FFZRVuemaksDBNR7mbaynzr5FSaVvZnCNJQT1JNTtFlU1Hg574geuGNadv5/jG3Hnl1SS0jLWtF0pM/JxpojaWTpWbr7d8cbUJSauqNojskPOND/GwXXMnt1XtEyFY3ZzBmdSF/Mn7FsGX26EoHxX/SJgE6wcdZ9TzJ/HLTWJAgjnp9EFcDqxs6x9dXEt8H2wA0qpcUqpTKVUZm5uA9k6TURCrE9Tf1qxgxv/O59tBSazo7iihiOf/JmV24tpERPF4d3bsDiroMFrvj97E3d+upD0B77zjnhrQl34vgF2lhpr5X8LffnQG/NKGfKPn3h31kbfie36+Pu7o2LgrpXe9MdKK+hdVhXiSBzMZK2Hc3xB6f2EWevy+Gl53XX5tayH23TYwdjAgH1jcEXJugUB7BfBYqXU5UAGcGyw41rrN4E3ATIyMvbLX6MnSCexJqd2Oej4GBedWsdTVFFDZY2buOgoNuWXkpwQS1KLGLYX+gJnzhTVamtqfJVDCArKqoiLjqJF7O4vrpJnxS+ck+M27zRT639ekcPVR9fjN2/tS5u0LQKn9dIgStUO0u0l1B78wC/9j0lf3fj0GUGP13g0MVHSgTQJ8UlmLkOwzB9hjwmnRZANdHVsp1n7/FBKnQQ8BJyttQ5xMd/9j2CukdU7atf6yS2uJDXRdII7S6vweDTHPvcLl701h5LKGoY99XOt1wAUlJqO1pl2OuiJH7nwjQbynusgv9Q/fgEQ5TKdnNsTutbaFsH9XyxhU/4eLJaylwjnqD3k+IMQHmJayEg+TIRTCOYBvZVSPZRSscAlgN80O6XUYOANjAjswSKsTc8Dp/Xli5uG0zbRN9Jdtb22EKzJKSE10WQ65JdUsdHqPJdmF3H/F4u95x3VK5Xe7X1T+3cUG0vBFgK7w1uaHbBOQIjkB1gE54z/nU/mGU+e29GZHvnkT9z72aLaF7Aod7iE7vt8cZ3nhRu/CXqNYMrS7Q2eU1mzd9xygrC/ETYh0FrXALcAU4EVwESt9TKl1BNKKStJlueAROAzpdRCpVQdRdj3f9omxjGke4pf513kyPqxGXtUOqktjRDklVSyyBEr+G6xrxrnqf07ku5ITc0tNh13VY2HvJJKVmyrv/Ttv6etYdiTwa0L+70BYqNduD2ahVsK+GaRWS3N47AIdhRV8tmfdS9EUlDmK19QvpdHzGt2FLM2J7QSv1V72Enf+N+GaxCJRSA0V8IaI9BaTwYmB+x71PH8pHC+f1Pw70sHs6usmpNemFHr2KNn9uOaY3qwIc+2AgpZEWA19OvUmuXbioiPcdEmwZcnnmMJQXm1m4x/BKkcGcA/fzAzQO04xPM/rOKwtGRO6mcCv/klvg68rMpfsOx4R0NuFrdHU+qwCBZnFfLO7xvqjy/sBif/y+Rz1+Wzd1JZ0/hOujogAO/xaFyu2i6I/d0ieG/WRl6ZtobMh33puJkbd5JdUM7oQZLpJNSN5FDtZVIT4+jVrvYksxuO7clVR6Vb5xiL4J8/rPazAu48qQ+9LIsi2uWiTYJvsoxtEdQE8d/X12HnFFVSUe3mlWlrue593xrI9mS34oqaWhk/9ltUVNff8ZVW1bZ4/vbN8iBnhh+7k26Mg6iw3D/QXZdls79bBI9NWkZeSZWfsF3w+mxu/2Rh0zVKOCAQIQgDwTJXLhzS1RuMbRUX7RfzSmoRQ/fUBC7MSKNVvDHSyqpqvOUlGiKwI3Py9PcrWba1dhxhqze1tdpvBjP4LILiivozgUqCuL7CkVUTSspspS1ajVCCwM8vmMDB/m8R2JRV7t+CdSBQWFbN2pwSzn31d0a9GGSmcTNDhCBMfH7jcA5NS6Jja5Pu1jLOl52jlKJ9K19QeUj3Nsy493g6J7eglVXiuriyhti6JmkFkLlxFy/8uNrr23daCN8t2eadF5DgSDPN2mWEoKSyhtIAIbCzhgJjHD+v2EGmYyJcoIAAdE0x5RYy/vEj1747r9bxxrCjuOFkMts1VOPZ/c46UAjq6kj3d4vApiSIkAVajdkF5UxbWfeciUjnojdmc9ILM1iwuYCVQZI+mhsiBGEiIz2FSbccw1c3H8UTo/vTKamF3/EXL/YVfnOma14wxNQ/OaVfR6KD+KmDcd37mbz88xrvvAV7spiNHQSOcZS03lZYQUyUwqN9gWMb2zUSaBFc+14mF7w+27ttl8B45Mx+fH3z0RzRI8UbtM0rqeLnlXUnglVUu0OeHGdbLzZ2jMWJ/b6NGbUXlgUIQR2T4w4ci6C2EAS2/cyXf+WadzNlklwdrAqS+t2cESEIM52SWnDl8PRa+4f3SmXCWFPUzSkEvTu0YuPTZ3BQ+8RaNYBax/vH9gd1TSaphc999N85m6h2e/hpRe2RXrtWcRSWV1NV42FHcSVuj6ZfJzOzN3uXf0dbUlHDzR/N5ypHVVXnaNi2PGyLYFDXJA7rmkz/zq0pLK+uZWE4WZtTwtqcYvo+MoUbPqidqbOjqIJhT/7Mim0+d9bmfN8asp//mcXx//yFOevz/V5nd3SNyR6qZRE4RtTOjrLyQLEIgnz+gf+TXZb4OQVi4rwtPDd1ZXgbJ+yXiBA0Ifacg+6pwctAnzu4C+cM6swDp5mStU5XzVmHdWbC2KGcNsC3nOUHczZx4vMzuP8LUx/+g2t99etPsbKF/rcgm9WWqZuRbsoar8v1H2HnFFfy3eJtfu/nnPGctaucB79awhPfmDUFEuOMGCW3iKW4ooZsxwg+sJM96YUZnPSC8bn+vDKn1oj0h+U72F5Uwb+n+1YEc47ObOtmW6G/eNmuoVnr8ustExGM2jECX4fv7CgPFIugNIhrqy4rxyka932xmPHT14WtXZHA+Olr6ftI0Eo5+zUiBE3IoWnJvHHFEB4+I2jdTlrGRfPiJYO55ugeHNQ+kUuG+iZqP3h6X1JaxnLXKX3o0bYlh3dLBnxlIpITYhjR21e/3H5+3xeLudry3Z91mFm8Zl2ufymMI3oE1L0H1uf5zvk0czMf/bHZKyCJlqWSbAW3nRPp7NnGH/6xibP//Vut687dsJNJVucO4LbcRc7Kq07rwJ6AVxwQv3B20te9n+mXOePx6HoD3wWBriGrc6yodrNoS4F3/wNfLOGfU31rB5dW1rBye+Mm9E1fleN3X3uTYBaBve+9WRv9ZrwHC/gLtQk1Pfm5qauoqPbsUTpzUyBC0MSc2r9jg7WCYqNd/HjnSJ46byAHtU/knEGdvTGH9q3imX7PcXz5l6N58eJB3td8e+sxftc4pV8Hbjnet8zdjcf2It2yRNYF1ESaeMNwPrzuSL99ax3nBI4a7TWcgwnBQqsjfeirpSzOql0S++I353Dbxwu8VsRWy/LIdQSIV24vptrtobzKzbaC2sfBkTVk4azu+twPqxj4+A9+s6Cd7Cz1v5ZtETw+aRkXvznHu7+82u1nqVz9zjxGvfgrE37bUK9LZV1uSS2xvfqdeZz20q/MWJ3rNymvIbTW/LR8R71lQGzXltPaKq2soaLazWOTlnH+q76yJMFEozGdWGlljd9ExOZGMCurPnaVVrO9sIJvF29t+OT9ABGCAwSlFEopfrhjJP9ydPhORvRu631Ma2M6+dcvH8LLYwbjcinuPqUPfx/dn9l/PYEHTutLUosY4qJd3s7XycEdzepMsdEuXMpUQgXonFS76JctBO2sTKivFmR7t+1Fedq1qr/I3B+Wz9+OV9ij5cO7JZNbXMkVb//BSS/M8Bbdyy2uZElWIRutwHFVQOB59jpzvcKyal77xQiXHWReklXI2pxipizdTt9Hvq+VXmsHp2cHxCGceDyauZbYPPHtcsZPXxc08Kq1Ztz7mTzgKB/iDJJfNWEutzny/D0ezVu/rqcoiAXz9cJsJi/ZznXvZzJ1Wd0lMUq9Fo3vfUoqa8gpsuaOODr/QMsK6k9HDsbUZdvp/9hUb4mS5kBgosbuWk75peY7e8tHC2pN2Nwf2S+qjwqhE2zGq01qYhyf3Tjcr8zFKEcMQSnFFY7AtVLKz6Xy633He5+3TYxj1gMn0K5VHHdPXOR134wb2ZPHv1lObLSLqhoP8TEu7/yIYT1SGdwtmQWbC+jYOp7TBnbkv3M2sXxrUa3MJCexUS5+XZPHe7M2sijAahjSvQ3zNxcwZ73/+g05xZWcZbmaNj59ht8otn2rOG8wedRLvhzwdbkl9Ovc2vu6lrFRVFR7yNy0i2P7tOP+UX2589OFzFmfT+fkFkTVUeBsZ2mV1wXnJLug3CvANiu2FbMut5TSSjdz1ufTMjaa9q39RXGlJXrLthZy5dtzyS+tYl1uCU+d51sVa31uid/EsD/W53P6wE5+11HKrMdSYo1enfMhSiprmPD7hlptLqmsYdX2Yn5c7hOWwrJq2reqv8rnB7M3srWwgvtH9eUDa5CwYPMuLj2yW72vawxLswtJahHjTU0OB1prfly+gxMP6UCUSxEb7aLGYUEWh1hdN8qlcHs0u0qrWW8NPPJLqkhI2b+7WrEImhlD01NIdsxIbohnLzCdTeekeLqmJPj92DontyAmysUNx/YEYHC3ZK4Yns65g7vw9lUZ/Hz3sX4uKJdLcdIhJiidmhjLTcf1IjbKxeVv/4HWvnkMKS1jufHYXnz1l6N49+qhHNEjhUmLtnpF4PiDfbGNId19Syymtowl2qUYmt6GaY7U1CVZhfy+1jd6HzWgI0uzi/h5xQ62OaydBZsL/NxWzqBw5+QW9OvcmiHpbZi1Lp97Plvk/SEHcvjff+Sc8b/X2r/Sqv+UX1LpdZN8t8QI6PaiCi55cw5n/fu3WgXu4mJcaK054+XfvFVh7dE7mMD4tIBU3Lkbd/lta629i3LZI1BnptD3S7b7rzNhUVJZzVUT5npLkgDc/snCoK6nLTvLvC6uR75exmu/GCto6Vbzf8sJYb5HqGwvrOC69+axNqeEM1/5jUvfmtPwi/aAzE27GPfBn3y/1Mz0DxwEOC0C81kHd4PZc3/yHe7GwHRugH9OXcXrM/afwPz+LVNC2LkooysXDkmrNyOmf+ck/nvtkQxMSyLKpep0TZlzTUpqeZWb9q3iueOkPjz1/QruOaUPVx6VztcLt3L5kd38Zl//tiaP39bmebdvPbE30621GNLaJDA0vQ2VNR5evexwthdWsHlnGfMcHeGHf5gR6TmDOjO8Vyqx0S7en72Ja9/LpGfblgzoksSkRVuZ8PsG3pnlPyq2LZtOlsvr9AGd+OiPzbXua8wRXfl4rr/ro1tKAskJMd7Yx29r8/hpxQ4mZm7hkiO6cf2Innz+ZxYu5SvbAaYUhJMtO8t54lv/0hweramscVNR5WH4U9NqtWfl9iJ+WZVDYlw0GekptdxA8zfv8iv3EWzuBcBPy3PYXuTvGly+rYgl2YUM6NyaKJfy/q8uemM22wor+Pelvjkwy7YWeYPt2Y7Z6t8v2U7XlASWbS3kuhE9KauqYfPOMvp2DG0xondmbeCnFTlkbtrl/Yy01nu03kR9bLEsvNnr8jnz0M61yoz89aslTL5tBPExURz6+A8cdVAqb1yRUes6sdEuyqvdfp1/fkAMqqii2htruvHYOtZT3seIEAgopYiPqT9gfUzvEJYHBPpZQjCsVyoA143owUUZXb3lMq4YVnt9277WfIZzBnWmT8dWDEpLpm1iLHklVcRGu/jsxqO856a1SSAjPYUuyS28gdwvF2RzxqGdePES00HZcYO+HVvx4XVHkpoYR0rLWN6dtdE7aj4sLYlHzuzHt4vNzOtuliV09EGpnH94Gl/MNxVXu6cmcNXwdK4+Op3TB3biirfn4lJw0iEduH5kT/q0b8VHczczfVWOd8R9eLdkPvpjs1dQrj2mB2//ZgTopEPa89OK2hPt3vl9o9/29FW5HPzwFP/PqWMrdpVV0SW5BfM3FzD2HZP99dH1R3pnsNvXCrze8oAMpQFdWrM0u4jvlmwjGFOWbuec8b/zyJn9uPaYHhSWV3utq1s+WuA978xXjJvtyB4pLM4qZFthOVe+PddvUaa0Ngms2FbEK9PWMPWOkfTuYOJP78/eyFcLshl9WGfGHNnNb20Mu1SIM6NrxbZicksqObaPz2Jcml3IrHV5jBvZi4KyKlrERnmvk1NUwX9+Xc+4kb0ajFHZ9zZnfT5VNZ5aNb3W55byz6mruG9UX4qtZWlXbi8iPbWl32/HdpM6RdhZ4BFgusO621laRUrLui14j0eztbC2y3FvI0Ig7FXat4pn6h0jvXMjlFIN1kw65qC2DOuZwj2nHuz9wn96w3DGT1tLemrtAn4AR/ZM5fkLD+PuzxZRVePh+hE9vcfS27bk65uP5pBOrYm11lu4+fiD2FVWxdij0lm0pYCrjkpHKcWQ7m0449BOHJaW7G3v8xcdxjG9U/lx+Q7uH9WX7lYbDrY6sDevyPBWcQW46bhepLVpwdwNOznmoLa8d80R/P3b5RRX1HD2oM50S0nwCsGDpx/CTytySE6IYd5DJ7E4q4CXfl7rDaoHIz7GRUW1h1P7d+TOk/tQXuXmkEd9InHtu5neEeyhaUl+2VlpbVrQq10iM6zr28cvHtqNpdlL63xP223x92+Xs6Oowtu+/157JI9NWlpr7snpAzvxx4adQa0XZ4nvF35czdPnH8rbv67n5WlmVLxgcwHzNu5i/GWHAyYo/sbM9d7XdEqKJ6e4ktNf/hWAqXeM5OCOrSitrGHsO3PJK6liaHoKY/4zhxYxUVw/sifXHN2Da9/LZEl2ISWVbp46byAf/rGJj+du5v1rjiQxLpqtBeUkxEaxOKvQu3TsutxSNu/0v7eDO7SipLKG6atyuNiRwj3qxV/pnprAW1dmeMUtWGA4v7SKhVsKmLUujy07y/lkns/i/Hphdq1qvX+sz2dgWhIJsdG8PnMdz05ZxXmDu/CPcwf4LYm7N1EH2hTzjIwMnZmZ2fCJQrNn5upcrpwwlzFHdOOp8wY2dXPYsrOMpIQYWsf7C5/WmkmLtqI1nDO4C4Xl1cREKb8f9ZodxWzZVcaQ7ilU1riprPbwxsx1FJbXcNfJfbjpv38yYexQOiebtOEr3v6DX9fkccGQND7/M4tol+LlMYMZ0bstP63YQX5JFaf060in5HgWbingwtdnc3CHVnw8bhhfzs9i7FHpHPnkzxx9UFvuOeVg2raKJa+4irzSStbllHCvY5EhlzKxp1EDOjL2qHTySqoYP30tyQkxVLs9DOuZymFdk7n4jTms2FbE6EGd+b9zBzJv406ufsdXb6pVXLRfxtKQ7m148PRDmLE6l5d/XkPXlBZ0SW5RKzHghL7t6ZaS4LW42ibG0bNtS2/WFpjCjc5sp5MO6cBPK3Z4y6hkdG/DHxt853dLSQga8AczR+fJyb504E/HDWNRVgFPTl7pdSUG8tIlg1izo4R/T1/L3Sf34auF2azPDe6OAxOT657aktnr8+nZtiVDurehZ7tETh/YkWOf+wWAMw7txNSl2+mUHE/WrnKuPqoHj54VfM5RKCil/tRa1/ZnIUIgHMB4PKaDPW1gR3+3QgRQWlnD7HX5HN+3Pa9MW8OI3u38AuuBZG7cSVKLGO/IFcznp1TwarnLtxaREBvF279t4PaTevutvFcX1W4P8zftYmh6ije77euF2Tz9/Uq2FVbw3jVHeMuWfHTdkRx1UFvv647/5y/eQohjj0ona1cZB3dsxfjp67jpuF5cMaw7Rz1d29pwktamBQVlvmq68TEuZt53PCe/MNMrElcO787PK3L8Zr/bdEqKp6La7S2/ce7gLny1IJu5D52IxwMjn5teSwTsLCEnj53Vj9TEOG77eAF1MaBLax46vR9j/lN/ELxTUjwTbxjOazPW8em8LXx32zEhx1kCESEQBKHJKKowNa7aJsaxZWcZBWXVDExL8jtna0E5q7YXc3zf9n77V+8opltKAvExUWzZWcausipiolws3FJAu8Q4Nu0s45xBnVmcVUjXlATatYpjweZdjH1nHveeejA3H38Q63NL+GbRNm44tqfXn5+5cSdLswtp1yqeHUUVdE1JoEPrOKYs3c6rv6zj6INS+ddFg2jviL3kFFVw6oszufaYHlyY0dVksUW5WJxVwILNBYyfvpac4kqeveBQLsroyoa8UlrERPG3b5axtaCcT8YNp9rjYdLCrRzVK5VuKQkc9ND3xMe4eOys/uQUVfKvn1b73f/fR/fniuHp7Cqt4vjnf+GcQV14/Oz+jfo/iBAIghBRFFVUW+t+7F6WUbXbw+KsAg7v1iboa6vdHm8V30B2FFXw2i/ruOHYnn7VhuvLdiqrqiEuOsobZP5z004WZxVSXu2mf+ckRhzU1mtdbcgrpXtKQr1zieqjyYRAKTUKeAmIAt7SWj8dcDwOeB8YAuQDF2utN9Z3TRECQRCE3ac+IQjbhDKlVBQwHjgN6AeMUUoFRjquBXZprQ8C/gU8E672CIIgCMEJ58ziI4C1Wuv1Wusq4BNgdMA5o4H3rOefAyeqcM0YEQRBEIISTiHoAjinYmZZ+4Keo7WuAQqB1MALKaXGKaUylVKZubl151sLgiAIu88BUWtIa/2m1jpDa53Rrl27hl8gCIIghEw4hSAb6OrYTrP2BT1HKRUNJGGCxoIgCMI+IpxCMA/orZTqoZSKBS4BJgWcMwm4ynp+ATBNH2j5rIIgCAc4Yas1pLWuUUrdAkzFpI9O0FovU0o9AWRqrScBbwMfKKXWAjsxYiEIgiDsQ8JadE5rPRmYHLDvUcfzCuDCcLZBEARBqJ8DbmaxUioX2NTIl7cF8ho8q3kh9xwZyD1HBntyz9211kGzbQ44IdgTlFKZdc2sa67IPUcGcs+RQbju+YBIHxUEQRDChwiBIAhChBNpQvBmUzegCZB7jgzkniODsNxzRMUIBEEQhNpEmkUgCIIgBCBCIAiCEOFEjBAopUYppVYppdYqpR5o6vbsLZRSE5RSOUqppY59KUqpH5VSa6zHNtZ+pZR62foMFiulDm+6ljcepVRXpdR0pdRypdQypdTt1v5me99KqXil1Fyl1CLrnv9m7e+hlPrDurdPrXIuKKXirO211vH0Jr2BRqKUilJKLVBKfWttN+v7BVBKbVRKLVFKLVRKZVr7wvrdjgghCHGRnAOVd4FRAfseAH7WWvcGfra2wdx/b+tvHPDaPmrj3qYGuFtr3Q8YBtxs/T+b831XAidorQ8DBgGjlFLDMIs5/cta3GkXZrEnaD6LPt0OrHBsN/f7tTleaz3IMWcgvN9trXWz/wOGA1Md238F/trU7dqL95cOLHVsrwI6Wc87Aaus528AY4KddyD/AV8DJ0fKfQMJwHzgSMws02hrv/d7jqnxNdx6Hm2dp5q67bt5n2lWp3cC8C2gmvP9Ou57I9A2YF9Yv9sRYREQ2iI5zYkOWutt1vPtQAfrebP7HCwXwGDgD5r5fVtukoVADvAjsA4o0GZRJ/C/r5AWfdrPeRG4D/BY26k07/u10cAPSqk/lVLjrH1h/W6Hteic0PRorbVSqlnmCCulEoEvgDu01kXOVU6b431rrd3AIKVUMvAV0LdpWxQ+lFJnAjla6z+VUsc1cXP2NcdorbOVUu2BH5VSK50Hw/HdjhSLIJRFcpoTO5RSnQCsxxxrf7P5HJRSMRgR+FBr/aW1u9nfN4DWugCYjnGNJFuLOoH/fR3oiz4dDZytlNqIWe/8BOAlmu/9etFaZ1uPORjBP4Iwf7cjRQhCWSSnOeFc8OcqjA/d3n+llWkwDCh0mJsHDMoM/d8GVmitX3Acarb3rZRqZ1kCKKVaYGIiKzCCcIF1WuA9H7CLPmmt/6q1TtNap2N+r9O01pfRTO/XRinVUinVyn4OnAIsJdzf7aYOjOzDAMzpwGqMX/Whpm7PXryvj4FtQDXGP3gtxjf6M7AG+AlIsc5VmOypdcASIKOp29/Iez4G40ddDCy0/k5vzvcNHAossO55KfCotb8nMBdYC3wGxFn7463ttdbxnk19D3tw78cB30bC/Vr3t8j6W2b3VeH+bkuJCUEQhAgnUlxDgiAIQh2IEAiCIEQ4IgSCIAgRjgiBIAhChCNCIAiCEOGIEAjNAqWUVko979i+Ryn1+B5c7xir2udK62+c41g7q8LlAqXUiIDX/aJMlduF1t/njW1DHe3aqJRquzevKQhSYkJoLlQC5ymlntJa5+3JhZRSHYGPgHO01vOtjneqUipba/0dcCKwRGt9XR2XuExrnbknbRCEfYlYBEJzoQaznuudgQeUUulKqWlWvfaflVLdGrjWzcC7Wuv5AJaw3Ac8oJQaBDwLjLZG/C1CaZxS6l2l1OtKqUyl1Gqrlo69zsA7Vv35BUqp4639UUqpfyqlllrtvtVxuVuVUvOt1/S1zj/WYYUssGenCkIoiBAIzYnxwGVKqaSA/a8A72mtDwU+BF5u4Dr9gT8D9mUC/bXWC4FHgU+1qRdfHuT1Hzo65ecc+9MxdWPOAF5XSsVjREdrrQcCY4D3rP3jrPMHOdptk6e1PhxTe/4ea989wM1a60HACCBYuwQhKCIEQrNBa10EvA/cFnBoOMbVA/ABpkRFOLnMEolBWut7Hfsnaq09Wus1wHpM9dBjgP8CaK1XApuAPsBJwBvaKrmstd7puI5dZO9PjFgA/A68oJS6DUjWvlLNgtAgIgRCc+NFTL2llntwjeXAkIB9QzC1X/aEwHouja3vUmk9urHifFrrp4HrgBbA77bLSBBCQYRAaFZYI+eJ+JYwBJiFqWAJcBnwawOXGQ+MteIBKKVSMUsfPruHzbtQKeVSSvXCFBdbZbXlMut9+gDdrP0/AjfYJZeVUin1XVgp1UtrvURr/Qym2q4IgRAyIgRCc+R5wJlieStwtVJqMXAFZh1clFI3KqVuDHyxNmV8Lwf+Yy0KMguYoLX+JsT3d8YIfnLs34ypjPk9cKPWugJ4FXAppZYAnwJjtdaVwFvW+YuVUouASxt4zzvswDKmEu33IbZVEKT6qCDsC5RS72JKKe/VeQWCsDcQi0AQBCHCEYtAEAQhwhGLQBAEIcIRIRAEQYhwRAgEQRAiHBECQRCECEeEQBAEIcL5fzwVty7hrnWFAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "opt = optimizers.Adam(lr=0.001,decay = 0.0001)\n",
    "print('Train...')\n",
    "# model.compile(optimizer = opt , loss=\"mse\")\n",
    "model.compile(optimizer = \"adam\" , loss=\"mse\")\n",
    "history = model.fit([x_train,x_train], y_train, epochs = 500, batch_size=8, validation_split=0.1, shuffle=True)\n",
    "# history = model.fit(x_train, y_train, epochs = 500, batch_size=6, validation_split=0.1, shuffle=True)\n",
    "model.summary()\n",
    "#Save Model\n",
    "model.save('GRU_Single_Attention_model_Duke.h5')  # creates a HDF5 file \n",
    "del model\n",
    "\n",
    "custom_ob = {'LayerNormalization': LayerNormalization , 'SeqSelfAttention':SeqSelfAttention}\n",
    "model = load_model('GRU_Single_Attention_model_Duke.h5', custom_objects=custom_ob)\n",
    "t1 = time.time()\n",
    "# y_pred = model.predict([x_test,x_test])\n",
    "y_pred2 = model.predict(x_test)\n",
    "y_pred = model.predict(x_train)\n",
    "t2 = time.time()\n",
    "print('Predict time: ',t2-t1)\n",
    "y_pred = scaler.inverse_transform(y_pred)#Undo scaling\n",
    "rmse_lstm2 = np.sqrt(mean_squared_error(y_test, y_pred2))\n",
    "rmse_lstm = np.sqrt(mean_squared_error(y_train, y_pred))\n",
    "print('RMSE: ',rmse_lstm)\n",
    "print('RMSE2: ',rmse_lstm2)\n",
    "mae = mean_absolute_error(y_test, y_pred2)\n",
    "mae = mean_absolute_error(y_train, y_pred)\n",
    "print('MAE: ',mae)\n",
    "print('MAE2: ',mae)\n",
    "# r22 =  r2_score(y_test, y_pred2)\n",
    "# r2 =  r2_score(y_train, y_pred)\n",
    "# print('R-square: ',r2)\n",
    "# print('R-square2: ',r22)\n",
    "\n",
    "# n = len(y_test)\n",
    "# p = 12\n",
    "# Adj_r2 = 1-(1-r2)*(n-1)/(n-p-1)\n",
    "# Adj_r22 = 1-(1-r22)*(n-1)/(n-p-1)\n",
    "# print('Adj R-square: ',Adj_r2)\n",
    "# print('Adj R-square2: ',Adj_r22)\n",
    "\n",
    "plt.plot(history.history[\"loss\"],label=\"loss\")\n",
    "plt.plot(history.history[\"val_loss\"],label=\"val_loss\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"No. Of Epochs\")\n",
    "plt.ylabel(\"mse score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b43b5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 108 samples, validate on 12 samples\n",
      "Epoch 1/200\n",
      "108/108 [==============================] - 2s 19ms/step - loss: 2.0550 - val_loss: 1.7855\n",
      "Epoch 2/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.9769 - val_loss: 1.6904\n",
      "Epoch 3/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.9031 - val_loss: 1.5972\n",
      "Epoch 4/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.8223 - val_loss: 1.5120\n",
      "Epoch 5/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.7609 - val_loss: 1.4402\n",
      "Epoch 6/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.7269 - val_loss: 1.3746\n",
      "Epoch 7/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.6703 - val_loss: 1.3188\n",
      "Epoch 8/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.6235 - val_loss: 1.2675\n",
      "Epoch 9/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.5872 - val_loss: 1.2233\n",
      "Epoch 10/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.5268 - val_loss: 1.1797\n",
      "Epoch 11/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.4833 - val_loss: 1.1390\n",
      "Epoch 12/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.4578 - val_loss: 1.1001\n",
      "Epoch 13/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.3909 - val_loss: 1.0646\n",
      "Epoch 14/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.3508 - val_loss: 1.0289\n",
      "Epoch 15/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.3030 - val_loss: 0.9991\n",
      "Epoch 16/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.2653 - val_loss: 0.9707\n",
      "Epoch 17/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.2496 - val_loss: 0.9396\n",
      "Epoch 18/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.2172 - val_loss: 0.9158\n",
      "Epoch 19/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 1.2372 - val_loss: 0.8971\n",
      "Epoch 20/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.1964 - val_loss: 0.8768\n",
      "Epoch 21/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.1482 - val_loss: 0.8578\n",
      "Epoch 22/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.1405 - val_loss: 0.8403\n",
      "Epoch 23/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 1.1542 - val_loss: 0.8228\n",
      "Epoch 24/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.1146 - val_loss: 0.8064\n",
      "Epoch 25/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.0968 - val_loss: 0.7905\n",
      "Epoch 26/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.0790 - val_loss: 0.7766\n",
      "Epoch 27/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.0640 - val_loss: 0.7702\n",
      "Epoch 28/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.0260 - val_loss: 0.7612\n",
      "Epoch 29/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.0290 - val_loss: 0.7485\n",
      "Epoch 30/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.0160 - val_loss: 0.7361\n",
      "Epoch 31/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.0540 - val_loss: 0.7220\n",
      "Epoch 32/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.9853 - val_loss: 0.7135\n",
      "Epoch 33/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.9838 - val_loss: 0.6987\n",
      "Epoch 34/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.9908 - val_loss: 0.6949\n",
      "Epoch 35/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.9556 - val_loss: 0.6931\n",
      "Epoch 36/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.9249 - val_loss: 0.6971\n",
      "Epoch 37/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.9055 - val_loss: 0.6882\n",
      "Epoch 38/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.9162 - val_loss: 0.6770\n",
      "Epoch 39/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.9596 - val_loss: 0.6726\n",
      "Epoch 40/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.8642 - val_loss: 0.6583\n",
      "Epoch 41/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.8465 - val_loss: 0.6584\n",
      "Epoch 42/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.8205 - val_loss: 0.6517\n",
      "Epoch 43/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.8667 - val_loss: 0.6603\n",
      "Epoch 44/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.8007 - val_loss: 0.6419\n",
      "Epoch 45/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.8332 - val_loss: 0.6471\n",
      "Epoch 46/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.8243 - val_loss: 0.6360\n",
      "Epoch 47/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.8711 - val_loss: 0.6373\n",
      "Epoch 48/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.7810 - val_loss: 0.6278\n",
      "Epoch 49/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.8157 - val_loss: 0.6114\n",
      "Epoch 50/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.8113 - val_loss: 0.6178\n",
      "Epoch 51/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.7175 - val_loss: 0.5870\n",
      "Epoch 52/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.7559 - val_loss: 0.5928\n",
      "Epoch 53/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.7015 - val_loss: 0.5923\n",
      "Epoch 54/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6939 - val_loss: 0.5796\n",
      "Epoch 55/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.7402 - val_loss: 0.6052\n",
      "Epoch 56/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6799 - val_loss: 0.5718\n",
      "Epoch 57/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6474 - val_loss: 0.5612\n",
      "Epoch 58/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6702 - val_loss: 0.5473\n",
      "Epoch 59/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6759 - val_loss: 0.5428\n",
      "Epoch 60/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6306 - val_loss: 0.5840\n",
      "Epoch 61/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6657 - val_loss: 0.5438\n",
      "Epoch 62/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6067 - val_loss: 0.5750\n",
      "Epoch 63/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5765 - val_loss: 0.5490\n",
      "Epoch 64/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6499 - val_loss: 0.5166\n",
      "Epoch 65/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6238 - val_loss: 0.5287\n",
      "Epoch 66/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6543 - val_loss: 0.5163\n",
      "Epoch 67/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5804 - val_loss: 0.5062\n",
      "Epoch 68/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5625 - val_loss: 0.5005\n",
      "Epoch 69/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6009 - val_loss: 0.5018\n",
      "Epoch 70/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.5487 - val_loss: 0.4704\n",
      "Epoch 71/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5566 - val_loss: 0.4858\n",
      "Epoch 72/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5691 - val_loss: 0.5070\n",
      "Epoch 73/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5369 - val_loss: 0.5235\n",
      "Epoch 74/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5044 - val_loss: 0.5079\n",
      "Epoch 75/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5076 - val_loss: 0.4929\n",
      "Epoch 76/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5301 - val_loss: 0.4525\n",
      "Epoch 77/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5376 - val_loss: 0.5043\n",
      "Epoch 78/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5376 - val_loss: 0.5479\n",
      "Epoch 79/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4727 - val_loss: 0.5591\n",
      "Epoch 80/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5452 - val_loss: 0.5436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.4951 - val_loss: 0.4978\n",
      "Epoch 82/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.5441 - val_loss: 0.4711\n",
      "Epoch 83/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5643 - val_loss: 0.4464\n",
      "Epoch 84/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.5185 - val_loss: 0.4644\n",
      "Epoch 85/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4697 - val_loss: 0.4242\n",
      "Epoch 86/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5226 - val_loss: 0.4503\n",
      "Epoch 87/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.5910 - val_loss: 0.4655\n",
      "Epoch 88/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.4874 - val_loss: 0.4179\n",
      "Epoch 89/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5123 - val_loss: 0.4517\n",
      "Epoch 90/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5528 - val_loss: 0.4473\n",
      "Epoch 91/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4807 - val_loss: 0.4709\n",
      "Epoch 92/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.4828 - val_loss: 0.4249\n",
      "Epoch 93/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.5059 - val_loss: 0.4184\n",
      "Epoch 94/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.5079 - val_loss: 0.3922\n",
      "Epoch 95/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4615 - val_loss: 0.4467\n",
      "Epoch 96/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4600 - val_loss: 0.4394\n",
      "Epoch 97/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5311 - val_loss: 0.4745\n",
      "Epoch 98/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5097 - val_loss: 0.4607\n",
      "Epoch 99/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4852 - val_loss: 0.4802\n",
      "Epoch 100/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5271 - val_loss: 0.4582\n",
      "Epoch 101/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4430 - val_loss: 0.4410\n",
      "Epoch 102/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4923 - val_loss: 0.4363\n",
      "Epoch 103/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.4527 - val_loss: 0.4275\n",
      "Epoch 104/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.4505 - val_loss: 0.4689\n",
      "Epoch 105/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.4259 - val_loss: 0.4663\n",
      "Epoch 106/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4977 - val_loss: 0.4152\n",
      "Epoch 107/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.4388 - val_loss: 0.4038\n",
      "Epoch 108/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4465 - val_loss: 0.4009\n",
      "Epoch 109/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4717 - val_loss: 0.4674\n",
      "Epoch 110/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4300 - val_loss: 0.4021\n",
      "Epoch 111/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.4170 - val_loss: 0.4224\n",
      "Epoch 112/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4468 - val_loss: 0.4275\n",
      "Epoch 113/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3948 - val_loss: 0.4224\n",
      "Epoch 114/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4626 - val_loss: 0.4649\n",
      "Epoch 115/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.4219 - val_loss: 0.4344\n",
      "Epoch 116/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4089 - val_loss: 0.4145\n",
      "Epoch 117/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4542 - val_loss: 0.4221\n",
      "Epoch 118/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4528 - val_loss: 0.4230\n",
      "Epoch 119/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4176 - val_loss: 0.4352\n",
      "Epoch 120/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.4515 - val_loss: 0.5025\n",
      "Epoch 121/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3769 - val_loss: 0.4200\n",
      "Epoch 122/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4499 - val_loss: 0.4634\n",
      "Epoch 123/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4620 - val_loss: 0.4206\n",
      "Epoch 124/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4466 - val_loss: 0.4872\n",
      "Epoch 125/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.4292 - val_loss: 0.4470\n",
      "Epoch 126/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.4176 - val_loss: 0.4280\n",
      "Epoch 127/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3996 - val_loss: 0.4569\n",
      "Epoch 128/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4328 - val_loss: 0.4240\n",
      "Epoch 129/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4272 - val_loss: 0.4114\n",
      "Epoch 130/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4058 - val_loss: 0.4283\n",
      "Epoch 131/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.4230 - val_loss: 0.4000\n",
      "Epoch 132/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3589 - val_loss: 0.4066\n",
      "Epoch 133/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3981 - val_loss: 0.3988\n",
      "Epoch 134/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4030 - val_loss: 0.3905\n",
      "Epoch 135/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.4318 - val_loss: 0.4086\n",
      "Epoch 136/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4527 - val_loss: 0.4205\n",
      "Epoch 137/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.4380 - val_loss: 0.3909\n",
      "Epoch 138/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.4326 - val_loss: 0.3904\n",
      "Epoch 139/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.3623 - val_loss: 0.4133\n",
      "Epoch 140/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4302 - val_loss: 0.4513\n",
      "Epoch 141/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.3862 - val_loss: 0.4137\n",
      "Epoch 142/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4457 - val_loss: 0.4080\n",
      "Epoch 143/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.3968 - val_loss: 0.4065\n",
      "Epoch 144/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.3827 - val_loss: 0.4443\n",
      "Epoch 145/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.4025 - val_loss: 0.4651\n",
      "Epoch 146/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.4405 - val_loss: 0.4308\n",
      "Epoch 147/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3289 - val_loss: 0.3932\n",
      "Epoch 148/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.4146 - val_loss: 0.3989\n",
      "Epoch 149/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4165 - val_loss: 0.4109\n",
      "Epoch 150/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3868 - val_loss: 0.4348\n",
      "Epoch 151/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.3690 - val_loss: 0.4121\n",
      "Epoch 152/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.3944 - val_loss: 0.5081\n",
      "Epoch 153/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4044 - val_loss: 0.4349\n",
      "Epoch 154/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4348 - val_loss: 0.4094\n",
      "Epoch 155/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4334 - val_loss: 0.4000\n",
      "Epoch 156/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3859 - val_loss: 0.3649\n",
      "Epoch 157/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4075 - val_loss: 0.4072\n",
      "Epoch 158/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4487 - val_loss: 0.4168\n",
      "Epoch 159/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4295 - val_loss: 0.4601\n",
      "Epoch 160/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4092 - val_loss: 0.3987\n",
      "Epoch 161/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3510 - val_loss: 0.4181\n",
      "Epoch 162/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3580 - val_loss: 0.4346\n",
      "Epoch 163/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3228 - val_loss: 0.4155\n",
      "Epoch 164/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3754 - val_loss: 0.4356\n",
      "Epoch 165/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3891 - val_loss: 0.3436\n",
      "Epoch 166/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3898 - val_loss: 0.4070\n",
      "Epoch 167/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3957 - val_loss: 0.4420\n",
      "Epoch 168/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3868 - val_loss: 0.3835\n",
      "Epoch 169/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4078 - val_loss: 0.3748\n",
      "Epoch 170/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3971 - val_loss: 0.3808\n",
      "Epoch 171/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3930 - val_loss: 0.4327\n",
      "Epoch 172/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4203 - val_loss: 0.4136\n",
      "Epoch 173/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.3617 - val_loss: 0.4518\n",
      "Epoch 174/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4236 - val_loss: 0.4267\n",
      "Epoch 175/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4283 - val_loss: 0.5736\n",
      "Epoch 176/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4531 - val_loss: 0.4494\n",
      "Epoch 177/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3613 - val_loss: 0.3591\n",
      "Epoch 178/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3591 - val_loss: 0.3980\n",
      "Epoch 179/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4179 - val_loss: 0.3682\n",
      "Epoch 180/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4119 - val_loss: 0.3821\n",
      "Epoch 181/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3615 - val_loss: 0.3726\n",
      "Epoch 182/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3709 - val_loss: 0.4464\n",
      "Epoch 183/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3544 - val_loss: 0.4595\n",
      "Epoch 184/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3978 - val_loss: 0.3844\n",
      "Epoch 185/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3850 - val_loss: 0.4227\n",
      "Epoch 186/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3682 - val_loss: 0.4289\n",
      "Epoch 187/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3709 - val_loss: 0.4794\n",
      "Epoch 188/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4489 - val_loss: 0.4381\n",
      "Epoch 189/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4195 - val_loss: 0.4499\n",
      "Epoch 190/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3617 - val_loss: 0.4385\n",
      "Epoch 191/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3472 - val_loss: 0.4153\n",
      "Epoch 192/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3568 - val_loss: 0.3858\n",
      "Epoch 193/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3572 - val_loss: 0.4204\n",
      "Epoch 194/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3373 - val_loss: 0.4425\n",
      "Epoch 195/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4143 - val_loss: 0.3965\n",
      "Epoch 196/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3251 - val_loss: 0.4413\n",
      "Epoch 197/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4059 - val_loss: 0.4002\n",
      "Epoch 198/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3214 - val_loss: 0.4496\n",
      "Epoch 199/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3699 - val_loss: 0.4552\n",
      "Epoch 200/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3277 - val_loss: 0.4041\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_6 (Bidirection (None, 24, 12)            684       \n",
      "_________________________________________________________________\n",
      "layer_normalization_5 (Layer (None, 24, 12)            24        \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 12)                684       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 1,405\n",
      "Trainable params: 1,405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Saved\n",
      "Predict time:  0.39294886589050293\n",
      "RMSE:  20.315989408226955\n",
      "RMSE2:  17.32014988360664\n",
      "MAE:  15.823254639761789\n",
      "MAE2:  15.823254639761789\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'mse score')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "init = glorot_normal(seed=None) # 給 LSTM\n",
    "init_d = RandomUniform(minval=-0.05, maxval=0.05) # 給 Dense layer\n",
    "nadam = optimizers.Nadam(lr=0.0015,clipvalue=0.5)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(GRU(6, kernel_initializer=init ,return_sequences = True,kernel_regularizer=regularizers.l2(0.01)\n",
    "                             ,recurrent_regularizer = regularizers.l2(0.01) ,input_shape=(x_train.shape[1],x_train.shape[2]))))\n",
    "model.add(LayerNormalization())\n",
    "model.add(Bidirectional(GRU(6,kernel_initializer=init,kernel_regularizer=regularizers.l2(0.01),recurrent_regularizer = regularizers.l2(0.01))))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(units=1, kernel_initializer=init_d))\n",
    "model.compile(optimizer = nadam , loss=\"mse\")\n",
    "history = model.fit(x_train, y_train, epochs=200, batch_size=24, validation_split=0.1, shuffle=True)\n",
    "#model summary\n",
    "model.summary()\n",
    "#Save Model\n",
    "model.save('GRU_model_DUKE.h5')  # creates a HDF5 file \n",
    "print('Model Saved')\n",
    "del model  # deletes the existing model\n",
    "\n",
    "custom_ob = {'LayerNormalization': LayerNormalization , 'SeqSelfAttention':SeqSelfAttention}\n",
    "model = load_model('GRU_model_DUKE.h5', custom_objects=custom_ob)\n",
    "t1 = time.time()\n",
    "# y_pred = model.predict([x_test,x_test])\n",
    "y_pred2 = model.predict(x_test)\n",
    "y_pred = model.predict(x_train)\n",
    "t2 = time.time()\n",
    "print('Predict time: ',t2-t1)\n",
    "y_pred = scaler.inverse_transform(y_pred)#Undo scaling\n",
    "rmse_lstm2 = np.sqrt(mean_squared_error(y_test, y_pred2))\n",
    "rmse_lstm = np.sqrt(mean_squared_error(y_train, y_pred))\n",
    "print('RMSE: ',rmse_lstm)\n",
    "print('RMSE2: ',rmse_lstm2)\n",
    "mae = mean_absolute_error(y_test, y_pred2)\n",
    "mae = mean_absolute_error(y_train, y_pred)\n",
    "print('MAE: ',mae)\n",
    "print('MAE2: ',mae)\n",
    "# r22 =  r2_score(y_test, y_pred2)\n",
    "# r2 =  r2_score(y_train, y_pred)\n",
    "# print('R-square: ',r2)\n",
    "# print('R-square2: ',r22)\n",
    "\n",
    "# n = len(y_test)\n",
    "# p = 12\n",
    "# Adj_r2 = 1-(1-r2)*(n-1)/(n-p-1)\n",
    "# Adj_r22 = 1-(1-r22)*(n-1)/(n-p-1)\n",
    "# print('Adj R-square: ',Adj_r2)\n",
    "# print('Adj R-square2: ',Adj_r22)\n",
    "\n",
    "plt.plot(history.history[\"loss\"],label=\"loss\")\n",
    "plt.plot(history.history[\"val_loss\"],label=\"val_loss\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"No. Of Epochs\")\n",
    "plt.ylabel(\"mse score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b210c78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40ae215",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
