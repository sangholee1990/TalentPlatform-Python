{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bff3f9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from tcn import TCN\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error, r2_score,mean_absolute_error\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential , load_model , Model\n",
    "from keras.layers import Dense, Dropout , LSTM , Bidirectional ,GRU ,Flatten,Add,BatchNormalization\n",
    "from keras_layer_normalization import LayerNormalization\n",
    "from keras.initializers import  glorot_normal, RandomUniform\n",
    "from keras import optimizers,Input\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae6cb065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(735, 16) (662, 16) (97, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:32: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a077f94f99a94a2f992dc6fb234544c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/638 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:44: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98f366ba560f4db0b054a4431dfc4da0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:\n",
      "(638, 24, 15) (638,)\n",
      "Test size:\n",
      "(73, 24, 15) (73,)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"Mode2_bike_region.csv\")\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "df = df.set_index(\"timestamp\")\n",
    "#df.head()\n",
    "\n",
    "df[\"hour\"] = df.index.hour\n",
    "df[\"day_of_month\"] = df.index.day\n",
    "df[\"day_of_week\"]  = df.index.dayofweek\n",
    "df[\"month\"] = df.index.month\n",
    "\n",
    "training_data_len = math.ceil(len(df) * 0.9) # taking 90% of data to train and 10% of data to test\n",
    "testing_data_len = len(df) - training_data_len\n",
    "\n",
    "time_steps = 24\n",
    "train, test = df.iloc[0:training_data_len], df.iloc[(training_data_len-time_steps):len(df)]\n",
    "print(df.shape, train.shape, test.shape)\n",
    "train_trans = train[['t1','t2', 'hum', 'wind_speed']].to_numpy()\n",
    "test_trans = test[['t1','t2', 'hum', 'wind_speed']].to_numpy()\n",
    "\n",
    "scaler = RobustScaler() # Handles outliers\n",
    "#scaler = MinMaxScaler(feature_range=(0, 1)) # scale to (0,1)\n",
    "train.loc[:, ['t1','t2','hum', 'wind_speed']]=scaler.fit_transform(train_trans)\n",
    "test.loc[:, ['t1','t2', 'hum', 'wind_speed']]=scaler.fit_transform(test_trans)\n",
    "\n",
    "train['cnt'] = scaler.fit_transform(train[['cnt']])\n",
    "test['cnt'] = scaler.fit_transform(test[['cnt']])\n",
    "\n",
    "#Split the data into x_train and y_train data sets\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in tqdm(range(len(train) - time_steps)):\n",
    "    x_train.append(train.drop(columns='cnt').iloc[i:i + time_steps].to_numpy())\n",
    "    y_train.append(train.loc[:,'cnt'].iloc[i + time_steps])\n",
    "\n",
    "#Convert x_train and y_train to numpy arrays\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "#Create the x_test and y_test data sets\n",
    "x_test = []\n",
    "y_test = df.loc[:,'cnt'].iloc[training_data_len:len(df)]\n",
    "\n",
    "for i in tqdm(range(len(test) - time_steps)):\n",
    "    x_test.append(test.drop(columns='cnt').iloc[i:i + time_steps].to_numpy())\n",
    "    # y_test.append(test.loc[:,'cnt'].iloc[i + time_steps])\n",
    "\n",
    "#Convert x_test and y_test to numpy arrays\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# All 12 columns of the data\n",
    "print('Train size:')\n",
    "print(x_train.shape, y_train.shape)\n",
    "print('Test size:')\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "847ebce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 24, 15)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 24, 32)       3072        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 24, 15)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "seq_self_attention_2 (SeqSelfAt (None, 24, 32)       1025        bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 24, 64)       9216        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 24, 32)       0           seq_self_attention_2[0][0]       \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "seq_self_attention_1 (SeqSelfAt (None, 24, 64)       4097        bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_2 (LayerNor (None, 24, 32)       64          add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 24, 64)       0           seq_self_attention_1[0][0]       \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "seq_self_attention_3 (SeqSelfAt (None, 24, 32)       1025        layer_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 24, 64)       128         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_3 (LayerNor (None, 24, 32)       64          seq_self_attention_3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 1536)         0           layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 768)          0           layer_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           24592       flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           12304       flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 16)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 16)           0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1552)         0           dropout_1[0][0]                  \n",
      "                                                                 flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 784)          0           dropout_2[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            1553        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            785         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 1)            0           dense_2[0][0]                    \n",
      "                                                                 dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            2           add_3[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 57,927\n",
      "Trainable params: 57,927\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Concatenate\n",
    "init = glorot_normal(seed=None) # 給 GRU\n",
    "init_d = RandomUniform(minval=-0.05, maxval=0.05) # 給 Dense layer\n",
    "\n",
    "def Encoder(layer):\n",
    "    shortcut = layer\n",
    "    layer = SeqSelfAttention(\n",
    "                     attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n",
    "                     kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     bias_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     attention_regularizer_weight=0.0001)(layer)\n",
    "    layer = Add()([layer,shortcut])\n",
    "    layer = LayerNormalization()(layer)\n",
    "    layer = Flatten()(layer)\n",
    "    \n",
    "    shortcut2 = layer\n",
    "    layer = Dense(16,kernel_initializer=init_d)(layer)\n",
    "    layer = Dropout(0.5)(layer)\n",
    "    layer = Concatenate()([layer,shortcut2])\n",
    "    output = Dense(1,kernel_initializer=init_d)(layer)\n",
    "    return output\n",
    "\n",
    "def Decoder(layer):\n",
    "    shortcut = layer\n",
    "    layer = SeqSelfAttention(\n",
    "                     attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n",
    "                     kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     bias_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     attention_regularizer_weight=0.0001)(layer)\n",
    "    layer = Add()([layer,shortcut])\n",
    "    layer = LayerNormalization()(layer)\n",
    "    layer = SeqSelfAttention(\n",
    "                     attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n",
    "                     kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     bias_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     attention_regularizer_weight=0.0001)(layer)\n",
    "    layer = LayerNormalization()(layer)\n",
    "    \n",
    "    layer = Flatten()(layer)\n",
    "    shortcut2 = layer\n",
    "    layer = Dense(16,kernel_initializer=init_d)(layer)\n",
    "    layer = Dropout(0.3)(layer)\n",
    "    layer = Concatenate()([layer,shortcut2])\n",
    "    output = Dense(1,kernel_initializer=init_d)(layer)\n",
    "    return output\n",
    "\n",
    "def Bi_GRU(layer,unit):\n",
    "    output = Bidirectional(GRU(unit, dropout=0.25, recurrent_dropout=0.25, return_sequences=True,\n",
    "                            kernel_initializer=init))(layer)\n",
    "    return output\n",
    "\n",
    "#start = Input(shape = (x_train.shape[1],x_train.shape[2]))\n",
    "start = Input(shape = (x_train.shape[1:]))\n",
    "start2 = Input(shape = (x_train.shape[1:]))\n",
    "x = Bi_GRU(start,32)\n",
    "x = Encoder(x)\n",
    "\n",
    "y = Bi_GRU(start2,16)\n",
    "y = Decoder(y)\n",
    "\n",
    "Merge = Add()([x,y])\n",
    "Last = Dense(1)(Merge)\n",
    "model = Model([start,start2] , Last)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b175b681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 574 samples, validate on 64 samples\n",
      "Epoch 1/500\n",
      "574/574 [==============================] - 4s 7ms/step - loss: 2.0966 - val_loss: 3.1065\n",
      "Epoch 2/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.9261 - val_loss: 2.9465\n",
      "Epoch 3/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.8096 - val_loss: 1.9482\n",
      "Epoch 4/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.7310 - val_loss: 2.9197\n",
      "Epoch 5/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.6683 - val_loss: 2.4983\n",
      "Epoch 6/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.6266 - val_loss: 2.1385\n",
      "Epoch 7/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.5673 - val_loss: 2.0872\n",
      "Epoch 8/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.5351 - val_loss: 1.7392\n",
      "Epoch 9/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.4877 - val_loss: 2.4149\n",
      "Epoch 10/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.4659 - val_loss: 1.5317\n",
      "Epoch 11/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.3996 - val_loss: 1.6811\n",
      "Epoch 12/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.3704 - val_loss: 2.8273\n",
      "Epoch 13/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.3471 - val_loss: 2.3327\n",
      "Epoch 14/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.3284 - val_loss: 3.6236\n",
      "Epoch 15/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.2577 - val_loss: 3.0426\n",
      "Epoch 16/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.2803 - val_loss: 2.8781\n",
      "Epoch 17/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.2386 - val_loss: 3.0356\n",
      "Epoch 18/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.1943 - val_loss: 2.4505\n",
      "Epoch 19/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.2283 - val_loss: 2.5694\n",
      "Epoch 20/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.2145 - val_loss: 2.2839\n",
      "Epoch 21/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.2067 - val_loss: 2.6096\n",
      "Epoch 22/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.1523 - val_loss: 2.3566\n",
      "Epoch 23/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.1401 - val_loss: 2.4207\n",
      "Epoch 24/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.1081 - val_loss: 2.3130\n",
      "Epoch 25/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.0869 - val_loss: 2.4623\n",
      "Epoch 26/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.1094 - val_loss: 2.9856\n",
      "Epoch 27/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.0705 - val_loss: 2.8781\n",
      "Epoch 28/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.0836 - val_loss: 2.7686\n",
      "Epoch 29/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.0456 - val_loss: 3.0272\n",
      "Epoch 30/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.0342 - val_loss: 2.9611\n",
      "Epoch 31/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.0168 - val_loss: 2.0797\n",
      "Epoch 32/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.0295 - val_loss: 3.1181\n",
      "Epoch 33/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.9944 - val_loss: 2.9144\n",
      "Epoch 34/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.9857 - val_loss: 3.2684\n",
      "Epoch 35/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.9875 - val_loss: 3.1593\n",
      "Epoch 36/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.0122 - val_loss: 3.3156\n",
      "Epoch 37/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.9617 - val_loss: 3.1617\n",
      "Epoch 38/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.9534 - val_loss: 3.0114\n",
      "Epoch 39/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.9366 - val_loss: 2.6009\n",
      "Epoch 40/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.9634 - val_loss: 3.0968\n",
      "Epoch 41/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.9259 - val_loss: 3.8980\n",
      "Epoch 42/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.9066 - val_loss: 3.6687\n",
      "Epoch 43/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.9084 - val_loss: 3.4574\n",
      "Epoch 44/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.9063 - val_loss: 3.3045\n",
      "Epoch 45/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.8900 - val_loss: 2.9220\n",
      "Epoch 46/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.9102 - val_loss: 3.5295\n",
      "Epoch 47/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.9020 - val_loss: 3.2055\n",
      "Epoch 48/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.8512 - val_loss: 3.1577\n",
      "Epoch 49/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.8585 - val_loss: 3.7126\n",
      "Epoch 50/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.8651 - val_loss: 3.5470\n",
      "Epoch 51/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.8346 - val_loss: 4.5744\n",
      "Epoch 52/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.8392 - val_loss: 4.2625\n",
      "Epoch 53/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.8500 - val_loss: 4.9820\n",
      "Epoch 54/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.8134 - val_loss: 5.2240\n",
      "Epoch 55/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.8477 - val_loss: 4.7763\n",
      "Epoch 56/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.8236 - val_loss: 4.5498\n",
      "Epoch 57/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.8147 - val_loss: 3.7280\n",
      "Epoch 58/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7901 - val_loss: 5.2439\n",
      "Epoch 59/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.8309 - val_loss: 6.3494\n",
      "Epoch 60/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7987 - val_loss: 5.3825\n",
      "Epoch 61/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7787 - val_loss: 5.4121\n",
      "Epoch 62/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7704 - val_loss: 5.2812\n",
      "Epoch 63/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7779 - val_loss: 5.0055\n",
      "Epoch 64/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7777 - val_loss: 5.2076\n",
      "Epoch 65/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7676 - val_loss: 5.6532\n",
      "Epoch 66/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7353 - val_loss: 5.5312\n",
      "Epoch 67/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7743 - val_loss: 4.9207\n",
      "Epoch 68/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7353 - val_loss: 5.7860\n",
      "Epoch 69/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7690 - val_loss: 5.1530\n",
      "Epoch 70/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7348 - val_loss: 4.6264\n",
      "Epoch 71/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7356 - val_loss: 4.6545\n",
      "Epoch 72/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7457 - val_loss: 3.5432\n",
      "Epoch 73/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7319 - val_loss: 4.0085\n",
      "Epoch 74/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7118 - val_loss: 4.6372\n",
      "Epoch 75/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7018 - val_loss: 3.9132\n",
      "Epoch 76/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7107 - val_loss: 3.0144\n",
      "Epoch 77/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6938 - val_loss: 2.9206\n",
      "Epoch 78/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6965 - val_loss: 2.5374\n",
      "Epoch 79/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6795 - val_loss: 3.6133\n",
      "Epoch 80/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6798 - val_loss: 4.2150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7071 - val_loss: 4.3351\n",
      "Epoch 82/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6752 - val_loss: 2.8675\n",
      "Epoch 83/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6850 - val_loss: 3.4970\n",
      "Epoch 84/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6768 - val_loss: 3.5027\n",
      "Epoch 85/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6711 - val_loss: 2.9047\n",
      "Epoch 86/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6570 - val_loss: 4.0078\n",
      "Epoch 87/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6478 - val_loss: 2.9336\n",
      "Epoch 88/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6365 - val_loss: 3.3395\n",
      "Epoch 89/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6620 - val_loss: 3.9413\n",
      "Epoch 90/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6742 - val_loss: 2.9794\n",
      "Epoch 91/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6643 - val_loss: 3.0141\n",
      "Epoch 92/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6421 - val_loss: 3.8977\n",
      "Epoch 93/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6356 - val_loss: 4.6924\n",
      "Epoch 94/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6427 - val_loss: 2.9178\n",
      "Epoch 95/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6458 - val_loss: 3.1022\n",
      "Epoch 96/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6162 - val_loss: 3.6665\n",
      "Epoch 97/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5997 - val_loss: 4.0459\n",
      "Epoch 98/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6125 - val_loss: 3.1152\n",
      "Epoch 99/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6140 - val_loss: 2.8976\n",
      "Epoch 100/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5965 - val_loss: 3.5713\n",
      "Epoch 101/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5905 - val_loss: 3.6463\n",
      "Epoch 102/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5943 - val_loss: 3.5056\n",
      "Epoch 103/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5673 - val_loss: 3.5869\n",
      "Epoch 104/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5755 - val_loss: 3.8254\n",
      "Epoch 105/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5841 - val_loss: 2.9363\n",
      "Epoch 106/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5799 - val_loss: 4.1464\n",
      "Epoch 107/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5844 - val_loss: 3.1365\n",
      "Epoch 108/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5990 - val_loss: 3.6914\n",
      "Epoch 109/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5676 - val_loss: 4.4495\n",
      "Epoch 110/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5876 - val_loss: 4.1092\n",
      "Epoch 111/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5738 - val_loss: 4.9609\n",
      "Epoch 112/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5515 - val_loss: 3.3629\n",
      "Epoch 113/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5573 - val_loss: 3.3071\n",
      "Epoch 114/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5415 - val_loss: 4.6184\n",
      "Epoch 115/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5604 - val_loss: 4.2309\n",
      "Epoch 116/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5736 - val_loss: 3.0730\n",
      "Epoch 117/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5134 - val_loss: 3.6093\n",
      "Epoch 118/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5373 - val_loss: 3.5962\n",
      "Epoch 119/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5695 - val_loss: 4.3764\n",
      "Epoch 120/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5579 - val_loss: 3.9143\n",
      "Epoch 121/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5438 - val_loss: 3.1978\n",
      "Epoch 122/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5587 - val_loss: 2.8562\n",
      "Epoch 123/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5202 - val_loss: 3.4543\n",
      "Epoch 124/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5214 - val_loss: 3.7406\n",
      "Epoch 125/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5221 - val_loss: 4.2146\n",
      "Epoch 126/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5206 - val_loss: 4.0258\n",
      "Epoch 127/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5136 - val_loss: 4.0938\n",
      "Epoch 128/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4976 - val_loss: 4.1003\n",
      "Epoch 129/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5090 - val_loss: 4.5175\n",
      "Epoch 130/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5079 - val_loss: 4.0582\n",
      "Epoch 131/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5021 - val_loss: 3.9088\n",
      "Epoch 132/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4973 - val_loss: 3.1611\n",
      "Epoch 133/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5074 - val_loss: 2.6236\n",
      "Epoch 134/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5143 - val_loss: 2.9933\n",
      "Epoch 135/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5057 - val_loss: 1.9484\n",
      "Epoch 136/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4676 - val_loss: 3.3123\n",
      "Epoch 137/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4705 - val_loss: 3.5546\n",
      "Epoch 138/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4391 - val_loss: 2.6596\n",
      "Epoch 139/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4664 - val_loss: 3.1112\n",
      "Epoch 140/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4767 - val_loss: 2.7494\n",
      "Epoch 141/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4822 - val_loss: 2.6332\n",
      "Epoch 142/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4819 - val_loss: 3.4741\n",
      "Epoch 143/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4706 - val_loss: 2.7916\n",
      "Epoch 144/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4705 - val_loss: 1.4368\n",
      "Epoch 145/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4828 - val_loss: 2.5410\n",
      "Epoch 146/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4513 - val_loss: 2.9249\n",
      "Epoch 147/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4391 - val_loss: 2.2882\n",
      "Epoch 148/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4694 - val_loss: 2.5350\n",
      "Epoch 149/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4606 - val_loss: 2.7606\n",
      "Epoch 150/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4341 - val_loss: 3.0899\n",
      "Epoch 151/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4268 - val_loss: 3.1566\n",
      "Epoch 152/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4400 - val_loss: 2.9304\n",
      "Epoch 153/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4179 - val_loss: 3.4665\n",
      "Epoch 154/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4598 - val_loss: 3.0570\n",
      "Epoch 155/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4540 - val_loss: 3.1952\n",
      "Epoch 156/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4050 - val_loss: 2.6458\n",
      "Epoch 157/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4090 - val_loss: 2.6822\n",
      "Epoch 158/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4297 - val_loss: 2.2773\n",
      "Epoch 159/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4517 - val_loss: 2.6650\n",
      "Epoch 160/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4394 - val_loss: 2.2735\n",
      "Epoch 161/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4053 - val_loss: 2.3483\n",
      "Epoch 162/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4132 - val_loss: 1.9416\n",
      "Epoch 163/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4009 - val_loss: 2.6710\n",
      "Epoch 164/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3950 - val_loss: 2.2451\n",
      "Epoch 165/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3960 - val_loss: 1.7839\n",
      "Epoch 166/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4117 - val_loss: 1.8918\n",
      "Epoch 167/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3984 - val_loss: 1.6336\n",
      "Epoch 168/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3885 - val_loss: 1.5066\n",
      "Epoch 169/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3595 - val_loss: 1.8483\n",
      "Epoch 170/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3952 - val_loss: 1.5349\n",
      "Epoch 171/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3958 - val_loss: 1.7198\n",
      "Epoch 172/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3939 - val_loss: 2.5771\n",
      "Epoch 173/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3898 - val_loss: 2.2189\n",
      "Epoch 174/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3756 - val_loss: 2.0855\n",
      "Epoch 175/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3855 - val_loss: 1.3905\n",
      "Epoch 176/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3660 - val_loss: 2.0577\n",
      "Epoch 177/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3751 - val_loss: 2.0814\n",
      "Epoch 178/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3859 - val_loss: 1.7003\n",
      "Epoch 179/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3811 - val_loss: 2.6642\n",
      "Epoch 180/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3896 - val_loss: 2.4904\n",
      "Epoch 181/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3564 - val_loss: 3.2386\n",
      "Epoch 182/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3548 - val_loss: 3.3148\n",
      "Epoch 183/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3699 - val_loss: 2.9724\n",
      "Epoch 184/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3842 - val_loss: 2.0664\n",
      "Epoch 185/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3708 - val_loss: 2.2944\n",
      "Epoch 186/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3793 - val_loss: 2.7506\n",
      "Epoch 187/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3625 - val_loss: 2.6017\n",
      "Epoch 188/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3561 - val_loss: 2.4167\n",
      "Epoch 189/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3663 - val_loss: 2.2068\n",
      "Epoch 190/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3769 - val_loss: 1.5779\n",
      "Epoch 191/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3627 - val_loss: 1.6324\n",
      "Epoch 192/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3663 - val_loss: 2.2250\n",
      "Epoch 193/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3714 - val_loss: 2.1814\n",
      "Epoch 194/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3490 - val_loss: 2.0494\n",
      "Epoch 195/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3195 - val_loss: 2.5552\n",
      "Epoch 196/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3606 - val_loss: 1.5969\n",
      "Epoch 197/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3588 - val_loss: 1.4027\n",
      "Epoch 198/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3393 - val_loss: 2.2249\n",
      "Epoch 199/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3305 - val_loss: 1.8607\n",
      "Epoch 200/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3564 - val_loss: 1.3314\n",
      "Epoch 201/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3285 - val_loss: 2.1591\n",
      "Epoch 202/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3476 - val_loss: 1.4455\n",
      "Epoch 203/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3219 - val_loss: 1.7316\n",
      "Epoch 204/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3327 - val_loss: 2.0843\n",
      "Epoch 205/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3350 - val_loss: 2.7813\n",
      "Epoch 206/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3359 - val_loss: 3.0688\n",
      "Epoch 207/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3429 - val_loss: 2.9265\n",
      "Epoch 208/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3270 - val_loss: 2.5676\n",
      "Epoch 209/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3435 - val_loss: 2.7839\n",
      "Epoch 210/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3272 - val_loss: 1.6199\n",
      "Epoch 211/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3246 - val_loss: 1.8568\n",
      "Epoch 212/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3296 - val_loss: 2.4803\n",
      "Epoch 213/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3331 - val_loss: 2.2296\n",
      "Epoch 214/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3385 - val_loss: 1.7847\n",
      "Epoch 215/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3328 - val_loss: 1.6846\n",
      "Epoch 216/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3461 - val_loss: 1.7180\n",
      "Epoch 217/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2956 - val_loss: 1.5591\n",
      "Epoch 218/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3179 - val_loss: 2.2137\n",
      "Epoch 219/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3264 - val_loss: 2.2843\n",
      "Epoch 220/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3083 - val_loss: 2.8855\n",
      "Epoch 221/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3133 - val_loss: 2.6454\n",
      "Epoch 222/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3131 - val_loss: 2.7817\n",
      "Epoch 223/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3000 - val_loss: 2.7998\n",
      "Epoch 224/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3087 - val_loss: 2.0258\n",
      "Epoch 225/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3230 - val_loss: 2.1071\n",
      "Epoch 226/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3131 - val_loss: 1.6818\n",
      "Epoch 227/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2985 - val_loss: 1.7359\n",
      "Epoch 228/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3197 - val_loss: 1.3940\n",
      "Epoch 229/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3018 - val_loss: 1.3914\n",
      "Epoch 230/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2957 - val_loss: 2.2548\n",
      "Epoch 231/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3124 - val_loss: 1.4750\n",
      "Epoch 232/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2954 - val_loss: 1.9787\n",
      "Epoch 233/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3033 - val_loss: 2.0780\n",
      "Epoch 234/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2927 - val_loss: 1.8886\n",
      "Epoch 235/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2923 - val_loss: 2.0204\n",
      "Epoch 236/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2896 - val_loss: 2.3585\n",
      "Epoch 237/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2913 - val_loss: 2.1556\n",
      "Epoch 238/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2832 - val_loss: 1.6790\n",
      "Epoch 239/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2928 - val_loss: 1.6991\n",
      "Epoch 240/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2946 - val_loss: 1.5217\n",
      "Epoch 241/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2826 - val_loss: 1.3175\n",
      "Epoch 242/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2911 - val_loss: 1.1586\n",
      "Epoch 243/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2809 - val_loss: 1.2615\n",
      "Epoch 244/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2730 - val_loss: 1.4799\n",
      "Epoch 245/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2933 - val_loss: 1.6005\n",
      "Epoch 246/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2798 - val_loss: 1.1613\n",
      "Epoch 247/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2913 - val_loss: 1.0485\n",
      "Epoch 248/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2865 - val_loss: 0.8815\n",
      "Epoch 249/500\n",
      "574/574 [==============================] - ETA: 0s - loss: 0.280 - 2s 3ms/step - loss: 0.2836 - val_loss: 0.8617\n",
      "Epoch 250/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2823 - val_loss: 1.0801\n",
      "Epoch 251/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2714 - val_loss: 1.2838\n",
      "Epoch 252/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2728 - val_loss: 1.2909\n",
      "Epoch 253/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2708 - val_loss: 1.3668\n",
      "Epoch 254/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2667 - val_loss: 1.1535\n",
      "Epoch 255/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2797 - val_loss: 1.3140\n",
      "Epoch 256/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2795 - val_loss: 1.3550\n",
      "Epoch 257/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2830 - val_loss: 1.3124\n",
      "Epoch 258/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2606 - val_loss: 0.9649\n",
      "Epoch 259/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2704 - val_loss: 1.1304\n",
      "Epoch 260/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2645 - val_loss: 1.2217\n",
      "Epoch 261/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2570 - val_loss: 1.2836\n",
      "Epoch 262/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2759 - val_loss: 1.3090\n",
      "Epoch 263/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2382 - val_loss: 1.6711\n",
      "Epoch 264/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2748 - val_loss: 1.5995\n",
      "Epoch 265/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2740 - val_loss: 1.0698\n",
      "Epoch 266/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2432 - val_loss: 0.9963\n",
      "Epoch 267/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2588 - val_loss: 0.9063\n",
      "Epoch 268/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2656 - val_loss: 0.7813\n",
      "Epoch 269/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2464 - val_loss: 0.9870\n",
      "Epoch 270/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2645 - val_loss: 1.0042\n",
      "Epoch 271/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2532 - val_loss: 1.0044\n",
      "Epoch 272/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2638 - val_loss: 1.0290\n",
      "Epoch 273/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2410 - val_loss: 1.2232\n",
      "Epoch 274/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2580 - val_loss: 0.9123\n",
      "Epoch 275/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2428 - val_loss: 1.1457\n",
      "Epoch 276/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2411 - val_loss: 1.2476\n",
      "Epoch 277/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2618 - val_loss: 1.1693\n",
      "Epoch 278/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2489 - val_loss: 1.3759\n",
      "Epoch 279/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2469 - val_loss: 1.4565\n",
      "Epoch 280/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2440 - val_loss: 1.0842\n",
      "Epoch 281/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2335 - val_loss: 1.1625\n",
      "Epoch 282/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2513 - val_loss: 1.3572\n",
      "Epoch 283/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2703 - val_loss: 1.1702\n",
      "Epoch 284/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2421 - val_loss: 1.3720\n",
      "Epoch 285/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2612 - val_loss: 1.1419\n",
      "Epoch 286/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2238 - val_loss: 0.8252\n",
      "Epoch 287/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2308 - val_loss: 0.9918\n",
      "Epoch 288/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2619 - val_loss: 0.8651\n",
      "Epoch 289/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2406 - val_loss: 0.8325\n",
      "Epoch 290/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2348 - val_loss: 0.8217\n",
      "Epoch 291/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2229 - val_loss: 0.7971\n",
      "Epoch 292/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2160 - val_loss: 0.9805\n",
      "Epoch 293/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2412 - val_loss: 0.9465\n",
      "Epoch 294/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2313 - val_loss: 1.2229\n",
      "Epoch 295/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2409 - val_loss: 1.2492\n",
      "Epoch 296/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2372 - val_loss: 1.1116\n",
      "Epoch 297/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2365 - val_loss: 0.9392\n",
      "Epoch 298/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2101 - val_loss: 1.1568\n",
      "Epoch 299/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2413 - val_loss: 0.9256\n",
      "Epoch 300/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2365 - val_loss: 0.7022\n",
      "Epoch 301/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2239 - val_loss: 0.7615\n",
      "Epoch 302/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2327 - val_loss: 0.6509\n",
      "Epoch 303/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2227 - val_loss: 0.7875\n",
      "Epoch 304/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2195 - val_loss: 0.9653\n",
      "Epoch 305/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2401 - val_loss: 0.8873\n",
      "Epoch 306/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2162 - val_loss: 1.0535\n",
      "Epoch 307/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2216 - val_loss: 0.8700\n",
      "Epoch 308/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2216 - val_loss: 0.7637\n",
      "Epoch 309/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2353 - val_loss: 0.8247\n",
      "Epoch 310/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2291 - val_loss: 0.9335\n",
      "Epoch 311/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2242 - val_loss: 1.0806\n",
      "Epoch 312/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2287 - val_loss: 0.7905\n",
      "Epoch 313/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2259 - val_loss: 0.7594\n",
      "Epoch 314/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2164 - val_loss: 0.8995\n",
      "Epoch 315/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2307 - val_loss: 1.0136\n",
      "Epoch 316/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2227 - val_loss: 1.3570\n",
      "Epoch 317/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2160 - val_loss: 1.1247\n",
      "Epoch 318/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2134 - val_loss: 0.9707\n",
      "Epoch 319/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2223 - val_loss: 1.0318\n",
      "Epoch 320/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2225 - val_loss: 0.9065\n",
      "Epoch 321/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2171 - val_loss: 0.8305\n",
      "Epoch 322/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2246 - val_loss: 0.7049\n",
      "Epoch 323/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2113 - val_loss: 0.8872\n",
      "Epoch 324/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2202 - val_loss: 1.0340\n",
      "Epoch 325/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2034 - val_loss: 1.0487\n",
      "Epoch 326/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2051 - val_loss: 1.2482\n",
      "Epoch 327/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2253 - val_loss: 0.8524\n",
      "Epoch 328/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2097 - val_loss: 0.8916\n",
      "Epoch 329/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2014 - val_loss: 1.0374\n",
      "Epoch 330/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2151 - val_loss: 0.7925\n",
      "Epoch 331/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2151 - val_loss: 0.7474\n",
      "Epoch 332/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2166 - val_loss: 1.2340\n",
      "Epoch 333/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2157 - val_loss: 0.9597\n",
      "Epoch 334/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2017 - val_loss: 0.8344\n",
      "Epoch 335/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2027 - val_loss: 0.9932\n",
      "Epoch 336/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2136 - val_loss: 1.1699\n",
      "Epoch 337/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2041 - val_loss: 1.1655\n",
      "Epoch 338/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2001 - val_loss: 0.9286\n",
      "Epoch 339/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1946 - val_loss: 0.7729\n",
      "Epoch 340/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2053 - val_loss: 1.1222\n",
      "Epoch 341/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1960 - val_loss: 0.9715\n",
      "Epoch 342/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1967 - val_loss: 1.2423\n",
      "Epoch 343/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2107 - val_loss: 0.8976\n",
      "Epoch 344/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1968 - val_loss: 0.5592\n",
      "Epoch 345/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2008 - val_loss: 0.5600\n",
      "Epoch 346/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2147 - val_loss: 0.6515\n",
      "Epoch 347/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2054 - val_loss: 0.5912\n",
      "Epoch 348/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2003 - val_loss: 0.7258\n",
      "Epoch 349/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1969 - val_loss: 0.7261\n",
      "Epoch 350/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1955 - val_loss: 0.6833\n",
      "Epoch 351/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2106 - val_loss: 0.7678\n",
      "Epoch 352/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2008 - val_loss: 0.8691\n",
      "Epoch 353/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2139 - val_loss: 0.7491\n",
      "Epoch 354/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2155 - val_loss: 0.8150\n",
      "Epoch 355/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2144 - val_loss: 0.7445\n",
      "Epoch 356/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2205 - val_loss: 0.7930\n",
      "Epoch 357/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2194 - val_loss: 0.9592\n",
      "Epoch 358/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2116 - val_loss: 0.7728\n",
      "Epoch 359/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1985 - val_loss: 0.7445\n",
      "Epoch 360/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2121 - val_loss: 0.8675\n",
      "Epoch 361/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2090 - val_loss: 0.8855\n",
      "Epoch 362/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2038 - val_loss: 0.8133\n",
      "Epoch 363/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1926 - val_loss: 0.7213\n",
      "Epoch 364/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1817 - val_loss: 0.7535\n",
      "Epoch 365/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1933 - val_loss: 0.8910\n",
      "Epoch 366/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1854 - val_loss: 0.9427\n",
      "Epoch 367/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2110 - val_loss: 0.7472\n",
      "Epoch 368/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1879 - val_loss: 0.7806\n",
      "Epoch 369/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1830 - val_loss: 0.8654\n",
      "Epoch 370/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2063 - val_loss: 0.8378\n",
      "Epoch 371/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2063 - val_loss: 0.8411\n",
      "Epoch 372/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1902 - val_loss: 0.7535\n",
      "Epoch 373/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2036 - val_loss: 0.6410\n",
      "Epoch 374/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1954 - val_loss: 0.7432\n",
      "Epoch 375/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1933 - val_loss: 0.7219\n",
      "Epoch 376/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1847 - val_loss: 0.6802\n",
      "Epoch 377/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1848 - val_loss: 0.7918\n",
      "Epoch 378/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1974 - val_loss: 0.8744\n",
      "Epoch 379/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1969 - val_loss: 0.7103\n",
      "Epoch 380/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1844 - val_loss: 0.8571\n",
      "Epoch 381/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1914 - val_loss: 0.8514\n",
      "Epoch 382/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1821 - val_loss: 0.6979\n",
      "Epoch 383/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1919 - val_loss: 0.8040\n",
      "Epoch 384/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2016 - val_loss: 0.8877\n",
      "Epoch 385/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1971 - val_loss: 0.8045\n",
      "Epoch 386/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1959 - val_loss: 0.9634\n",
      "Epoch 387/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1736 - val_loss: 0.9691\n",
      "Epoch 388/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1936 - val_loss: 0.8241\n",
      "Epoch 389/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1800 - val_loss: 0.7399\n",
      "Epoch 390/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2096 - val_loss: 0.6976\n",
      "Epoch 391/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1876 - val_loss: 0.7882\n",
      "Epoch 392/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1842 - val_loss: 0.8936\n",
      "Epoch 393/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1775 - val_loss: 0.7921\n",
      "Epoch 394/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1846 - val_loss: 0.8722\n",
      "Epoch 395/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1780 - val_loss: 0.7285\n",
      "Epoch 396/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1748 - val_loss: 0.8601\n",
      "Epoch 397/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1819 - val_loss: 0.7247\n",
      "Epoch 398/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1908 - val_loss: 0.7683\n",
      "Epoch 399/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1723 - val_loss: 0.9836\n",
      "Epoch 400/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1799 - val_loss: 0.8323\n",
      "Epoch 401/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1907 - val_loss: 0.9433\n",
      "Epoch 402/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1911 - val_loss: 0.9021\n",
      "Epoch 403/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1961 - val_loss: 0.8249\n",
      "Epoch 404/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1771 - val_loss: 0.9669\n",
      "Epoch 405/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1767 - val_loss: 1.0457\n",
      "Epoch 406/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1761 - val_loss: 0.8863\n",
      "Epoch 407/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1701 - val_loss: 0.9041\n",
      "Epoch 408/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1772 - val_loss: 0.8449\n",
      "Epoch 409/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1758 - val_loss: 0.7580\n",
      "Epoch 410/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1697 - val_loss: 0.6358\n",
      "Epoch 411/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1854 - val_loss: 0.6156\n",
      "Epoch 412/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1907 - val_loss: 0.6763\n",
      "Epoch 413/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1883 - val_loss: 0.6706\n",
      "Epoch 414/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1771 - val_loss: 0.8350\n",
      "Epoch 415/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1893 - val_loss: 0.6237\n",
      "Epoch 416/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1740 - val_loss: 0.6435\n",
      "Epoch 417/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1772 - val_loss: 0.8615\n",
      "Epoch 418/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1744 - val_loss: 0.7393\n",
      "Epoch 419/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1833 - val_loss: 0.7336\n",
      "Epoch 420/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1810 - val_loss: 0.7174\n",
      "Epoch 421/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1830 - val_loss: 0.7573\n",
      "Epoch 422/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1805 - val_loss: 0.8012\n",
      "Epoch 423/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1801 - val_loss: 0.7426\n",
      "Epoch 424/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1911 - val_loss: 0.6768\n",
      "Epoch 425/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1810 - val_loss: 0.7285\n",
      "Epoch 426/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1724 - val_loss: 0.6610\n",
      "Epoch 427/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1905 - val_loss: 0.6210\n",
      "Epoch 428/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1797 - val_loss: 0.7449\n",
      "Epoch 429/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1861 - val_loss: 0.7964\n",
      "Epoch 430/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1689 - val_loss: 0.9129\n",
      "Epoch 431/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1760 - val_loss: 0.9197\n",
      "Epoch 432/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1765 - val_loss: 0.7617\n",
      "Epoch 433/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1536 - val_loss: 0.8138\n",
      "Epoch 434/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1831 - val_loss: 0.9080\n",
      "Epoch 435/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1833 - val_loss: 0.9265\n",
      "Epoch 436/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1855 - val_loss: 0.9100\n",
      "Epoch 437/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1779 - val_loss: 0.8412\n",
      "Epoch 438/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1578 - val_loss: 0.9331\n",
      "Epoch 439/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1728 - val_loss: 0.8362\n",
      "Epoch 440/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1681 - val_loss: 0.8893\n",
      "Epoch 441/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1855 - val_loss: 0.8746\n",
      "Epoch 442/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1741 - val_loss: 0.8059\n",
      "Epoch 443/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1684 - val_loss: 0.8093\n",
      "Epoch 444/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1657 - val_loss: 0.9181\n",
      "Epoch 445/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1707 - val_loss: 0.9296\n",
      "Epoch 446/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1677 - val_loss: 0.8487\n",
      "Epoch 447/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1617 - val_loss: 0.9752\n",
      "Epoch 448/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1791 - val_loss: 0.9212\n",
      "Epoch 449/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1822 - val_loss: 0.7409\n",
      "Epoch 450/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1770 - val_loss: 0.7204\n",
      "Epoch 451/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1675 - val_loss: 0.8059\n",
      "Epoch 452/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1633 - val_loss: 0.5977\n",
      "Epoch 453/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1609 - val_loss: 0.6810\n",
      "Epoch 454/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1788 - val_loss: 0.8237\n",
      "Epoch 455/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1701 - val_loss: 0.8043\n",
      "Epoch 456/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1682 - val_loss: 0.7050\n",
      "Epoch 457/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1613 - val_loss: 0.6479\n",
      "Epoch 458/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1772 - val_loss: 0.7286\n",
      "Epoch 459/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1609 - val_loss: 0.8121\n",
      "Epoch 460/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1627 - val_loss: 0.7239\n",
      "Epoch 461/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1659 - val_loss: 0.7991\n",
      "Epoch 462/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1604 - val_loss: 0.8670\n",
      "Epoch 463/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1715 - val_loss: 0.8237\n",
      "Epoch 464/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1628 - val_loss: 0.9288\n",
      "Epoch 465/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1739 - val_loss: 0.6857\n",
      "Epoch 466/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1553 - val_loss: 0.7372\n",
      "Epoch 467/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1677 - val_loss: 0.7162\n",
      "Epoch 468/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1641 - val_loss: 0.7472\n",
      "Epoch 469/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1573 - val_loss: 0.7900\n",
      "Epoch 470/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1614 - val_loss: 0.6938\n",
      "Epoch 471/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1616 - val_loss: 0.6737\n",
      "Epoch 472/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1557 - val_loss: 0.7781\n",
      "Epoch 473/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1739 - val_loss: 0.6690\n",
      "Epoch 474/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1503 - val_loss: 0.6647\n",
      "Epoch 475/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1609 - val_loss: 0.6831\n",
      "Epoch 476/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1650 - val_loss: 0.6968\n",
      "Epoch 477/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1592 - val_loss: 0.7133\n",
      "Epoch 478/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1533 - val_loss: 0.8060\n",
      "Epoch 479/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1493 - val_loss: 0.7923\n",
      "Epoch 480/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1564 - val_loss: 0.6788\n",
      "Epoch 481/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1663 - val_loss: 0.5951\n",
      "Epoch 482/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1599 - val_loss: 0.6232\n",
      "Epoch 483/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1683 - val_loss: 0.7004\n",
      "Epoch 484/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1470 - val_loss: 0.6412\n",
      "Epoch 485/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1470 - val_loss: 0.6692\n",
      "Epoch 486/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1543 - val_loss: 0.6300\n",
      "Epoch 487/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1416 - val_loss: 0.7480\n",
      "Epoch 488/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1733 - val_loss: 0.6729\n",
      "Epoch 489/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1563 - val_loss: 0.6640\n",
      "Epoch 490/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1636 - val_loss: 0.6632\n",
      "Epoch 491/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1659 - val_loss: 0.7110\n",
      "Epoch 492/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1527 - val_loss: 0.6821\n",
      "Epoch 493/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1608 - val_loss: 0.6923\n",
      "Epoch 494/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1420 - val_loss: 0.5948\n",
      "Epoch 495/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1579 - val_loss: 0.6602\n",
      "Epoch 496/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1531 - val_loss: 0.6534\n",
      "Epoch 497/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1549 - val_loss: 0.7013\n",
      "Epoch 498/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1562 - val_loss: 0.7607\n",
      "Epoch 499/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1514 - val_loss: 0.7654\n",
      "Epoch 500/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1546 - val_loss: 0.7851\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 24, 15)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 24, 32)       3072        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 24, 15)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "seq_self_attention_2 (SeqSelfAt (None, 24, 32)       1025        bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 24, 64)       9216        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 24, 32)       0           seq_self_attention_2[0][0]       \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "seq_self_attention_1 (SeqSelfAt (None, 24, 64)       4097        bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_2 (LayerNor (None, 24, 32)       64          add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 24, 64)       0           seq_self_attention_1[0][0]       \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "seq_self_attention_3 (SeqSelfAt (None, 24, 32)       1025        layer_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 24, 64)       128         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_3 (LayerNor (None, 24, 32)       64          seq_self_attention_3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 1536)         0           layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 768)          0           layer_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           24592       flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           12304       flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 16)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 16)           0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1552)         0           dropout_1[0][0]                  \n",
      "                                                                 flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 784)          0           dropout_2[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            1553        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            785         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 1)            0           dense_2[0][0]                    \n",
      "                                                                 dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            2           add_3[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 57,927\n",
      "Trainable params: 57,927\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "opt = optimizers.Adam(lr=0.0009)\n",
    "print('Train...')\n",
    "model.compile(optimizer = opt , loss=\"mse\")\n",
    "#model.compile(optimizer = \"adam\" , loss=\"mse\")\n",
    "history = model.fit([x_train,x_train], y_train, epochs = 500, batch_size=24, validation_split=0.1, shuffle=True)\n",
    "# history = model.fit(x_train, y_train, epochs = 500, batch_size=6, validation_split=0.1, shuffle=True)\n",
    "model.summary()\n",
    "#Save Model\n",
    "model.save('GRU_Single_Attention_model_region.h5')  # creates a HDF5 file \n",
    "del model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6b8cefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict time:  0.5099737644195557\n",
      "RMSE2:  9.142096633568197\n",
      "MAE2:  6.426921561003139\n",
      "R-square2:  -0.579800350232373\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'mse score')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "custom_ob = {'LayerNormalization': LayerNormalization , 'SeqSelfAttention':SeqSelfAttention}\n",
    "model = load_model('GRU_Single_Attention_model_region.h5', custom_objects=custom_ob)\n",
    "t1 = time.time()\n",
    "y_pred2 = model.predict([x_test,x_test])\n",
    "#y_pred2 = model.predict(x_test)\n",
    "#y_pred = model.predict(x_train)\n",
    "t2 = time.time()\n",
    "print('Predict time: ',t2-t1)\n",
    "y_pred = scaler.inverse_transform(y_pred2)#Undo scaling\n",
    "rmse_lstm2 = np.sqrt(mean_squared_error(y_test, y_pred2))\n",
    "#rmse_lstm = np.sqrt(mean_squared_error(y_train, y_pred))\n",
    "#print('RMSE: ',rmse_lstm)\n",
    "print('RMSE2: ',rmse_lstm2)\n",
    "mae2 = mean_absolute_error(y_test, y_pred2)\n",
    "#mae = mean_absolute_error(y_train, y_pred)\n",
    "#print('MAE: ',mae)\n",
    "print('MAE2: ',mae2)\n",
    "r22 =  r2_score(y_test, y_pred2)\n",
    "# r2 =  r2_score(y_train, y_pred)\n",
    "# print('R-square: ',r2)\n",
    "print('R-square2: ',r22)\n",
    "\n",
    "# n = len(y_test)\n",
    "# p = 12\n",
    "# Adj_r2 = 1-(1-r2)*(n-1)/(n-p-1)\n",
    "# Adj_r22 = 1-(1-r22)*(n-1)/(n-p-1)\n",
    "# print('Adj R-square: ',Adj_r2)\n",
    "# print('Adj R-square2: ',Adj_r22)\n",
    "\n",
    "plt.plot(history.history[\"loss\"],label=\"loss\")\n",
    "plt.plot(history.history[\"val_loss\"],label=\"val_loss\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"No. Of Epochs\")\n",
    "plt.ylabel(\"mse score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2191101a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
