{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f619a9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from tcn import TCN\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error, r2_score,mean_absolute_error\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential , load_model , Model\n",
    "from keras.layers import Dense, Dropout , LSTM , Bidirectional ,GRU ,Flatten,Add,BatchNormalization\n",
    "from keras_layer_normalization import LayerNormalization\n",
    "from keras.initializers import  glorot_normal, RandomUniform\n",
    "from keras import optimizers,Input\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c3cfed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160, 13) (144, 13) (40, 13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:32: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7624c26da774bc0be8ce2c0a3d8dd70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:44: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae8ce09c4dae4c40ab10e6448e358fce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:\n",
      "(120, 24, 12) (120,)\n",
      "Test size:\n",
      "(16, 24, 12) (16,)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"station_bike _Pitfield.csv\")\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "df = df.set_index(\"timestamp\")\n",
    "#df.head()\n",
    "\n",
    "df[\"hour\"] = df.index.hour\n",
    "df[\"day_of_month\"] = df.index.day\n",
    "df[\"day_of_week\"]  = df.index.dayofweek\n",
    "df[\"month\"] = df.index.month\n",
    "\n",
    "training_data_len = math.ceil(len(df) * 0.9) # taking 90% of data to train and 10% of data to test\n",
    "testing_data_len = len(df) - training_data_len\n",
    "\n",
    "time_steps = 24\n",
    "train, test = df.iloc[0:training_data_len], df.iloc[(training_data_len-time_steps):len(df)]\n",
    "print(df.shape, train.shape, test.shape)\n",
    "train_trans = train[['t1','t2', 'hum', 'wind_speed']].to_numpy()\n",
    "test_trans = test[['t1','t2', 'hum', 'wind_speed']].to_numpy()\n",
    "\n",
    "scaler = RobustScaler() # Handles outliers\n",
    "#scaler = MinMaxScaler(feature_range=(0, 1)) # scale to (0,1)\n",
    "train.loc[:, ['t1','t2','hum', 'wind_speed']]=scaler.fit_transform(train_trans)\n",
    "test.loc[:, ['t1','t2', 'hum', 'wind_speed']]=scaler.fit_transform(test_trans)\n",
    "\n",
    "train['cnt'] = scaler.fit_transform(train[['cnt']])\n",
    "test['cnt'] = scaler.fit_transform(test[['cnt']])\n",
    "\n",
    "#Split the data into x_train and y_train data sets\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in tqdm(range(len(train) - time_steps)):\n",
    "    x_train.append(train.drop(columns='cnt').iloc[i:i + time_steps].to_numpy())\n",
    "    y_train.append(train.loc[:,'cnt'].iloc[i + time_steps])\n",
    "\n",
    "#Convert x_train and y_train to numpy arrays\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "#Create the x_test and y_test data sets\n",
    "x_test = []\n",
    "y_test = df.loc[:,'cnt'].iloc[training_data_len:len(df)]\n",
    "\n",
    "for i in tqdm(range(len(test) - time_steps)):\n",
    "    x_test.append(test.drop(columns='cnt').iloc[i:i + time_steps].to_numpy())\n",
    "    # y_test.append(test.loc[:,'cnt'].iloc[i + time_steps])\n",
    "\n",
    "#Convert x_test and y_test to numpy arrays\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# All 12 columns of the data\n",
    "print('Train size:')\n",
    "print(x_train.shape, y_train.shape)\n",
    "print('Test size:')\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "169cfd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 24, 12)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 24, 24)       1800        input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "seq_self_attention_3 (SeqSelfAt (None, 24, 24)       577         bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 24, 24)       0           seq_self_attention_3[0][0]       \n",
      "                                                                 bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_3 (LayerNor (None, 24, 24)       48          add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 576)          0           layer_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 12)           6924        flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 12)           0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 588)          0           dropout_3[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            589         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 1)            2           dense_8[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 9,940\n",
      "Trainable params: 9,940\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Concatenate\n",
    "init = glorot_normal(seed=None) # 給 GRU\n",
    "init_d = RandomUniform(minval=-0.05, maxval=0.05) # 給 Dense layer\n",
    "\n",
    "def Encoder(layer):\n",
    "    shortcut = layer\n",
    "    layer = SeqSelfAttention(\n",
    "                     attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n",
    "                     kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     bias_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     attention_regularizer_weight=0.0001)(layer)\n",
    "    layer = Add()([layer,shortcut])\n",
    "    layer = LayerNormalization()(layer)\n",
    "    layer = Flatten()(layer)\n",
    "    \n",
    "    shortcut2 = layer\n",
    "    layer = Dense(12,kernel_initializer=init_d)(layer)\n",
    "    layer = Dropout(0.2)(layer)\n",
    "    layer = Concatenate()([layer,shortcut2])\n",
    "    output = Dense(1,kernel_initializer=init_d)(layer)\n",
    "    return output\n",
    "\n",
    "def Decoder(layer):\n",
    "    shortcut = layer\n",
    "    layer = SeqSelfAttention(\n",
    "                     attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n",
    "                     kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     bias_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     attention_regularizer_weight=0.0001)(layer)\n",
    "    layer = Add()([layer,shortcut])\n",
    "    layer = LayerNormalization()(layer)\n",
    "    layer = SeqSelfAttention(\n",
    "                     attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n",
    "                     kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     bias_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     attention_regularizer_weight=0.0001)(layer)\n",
    "    layer = LayerNormalization()(layer)\n",
    "    \n",
    "    layer = Flatten()(layer)\n",
    "    shortcut2 = layer\n",
    "    layer = Dense(10,kernel_initializer=init_d)(layer)\n",
    "    #layer = Dropout(0.2)(layer)\n",
    "    layer = Concatenate()([layer,shortcut2])\n",
    "    output = Dense(1,kernel_initializer=init_d)(layer)\n",
    "    return output\n",
    "\n",
    "def Bi_GRU(layer,unit):\n",
    "    output = Bidirectional(GRU(unit, dropout=0.1, recurrent_dropout=0.1, return_sequences=True,\n",
    "                            kernel_initializer=init))(layer)\n",
    "    return output\n",
    "\n",
    "#start = Input(shape = (x_train.shape[1],x_train.shape[2]))\n",
    "start = Input(shape = (x_train.shape[1:]))\n",
    "start2 = Input(shape = (x_train.shape[1:]))\n",
    "x = Bi_GRU(start,12)\n",
    "x = Encoder(x)\n",
    "\n",
    "# y = Bi_GRU(start2,8)\n",
    "# y = Decoder(y)\n",
    "\n",
    "#Merge = Add()([x,x])\n",
    "Last = Dense(1)(x)\n",
    "model = Model([start,start2] , Last)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c86e9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 108 samples, validate on 12 samples\n",
      "Epoch 1/500\n",
      "108/108 [==============================] - 1s 14ms/step - loss: 1.7868 - val_loss: 1.2663\n",
      "Epoch 2/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.2648 - val_loss: 1.0038\n",
      "Epoch 3/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.4823 - val_loss: 0.6257\n",
      "Epoch 4/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.1912 - val_loss: 0.7033\n",
      "Epoch 5/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.6301 - val_loss: 0.8733\n",
      "Epoch 6/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.2266 - val_loss: 0.7568\n",
      "Epoch 7/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.0771 - val_loss: 0.5997\n",
      "Epoch 8/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.0308 - val_loss: 0.5188\n",
      "Epoch 9/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.9216 - val_loss: 0.4948\n",
      "Epoch 10/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.8328 - val_loss: 0.5707\n",
      "Epoch 11/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.9072 - val_loss: 0.5201\n",
      "Epoch 12/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.8079 - val_loss: 0.6356\n",
      "Epoch 13/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.7282 - val_loss: 0.4836\n",
      "Epoch 14/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.7922 - val_loss: 0.5509\n",
      "Epoch 15/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.7417 - val_loss: 0.5587\n",
      "Epoch 16/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.7696 - val_loss: 0.5060\n",
      "Epoch 17/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5886 - val_loss: 0.7865\n",
      "Epoch 18/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6282 - val_loss: 0.4888\n",
      "Epoch 19/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6712 - val_loss: 0.4777\n",
      "Epoch 20/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.7385 - val_loss: 0.4381\n",
      "Epoch 21/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5844 - val_loss: 0.4921\n",
      "Epoch 22/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5099 - val_loss: 0.4758\n",
      "Epoch 23/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4568 - val_loss: 0.4716\n",
      "Epoch 24/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5494 - val_loss: 0.4816\n",
      "Epoch 25/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5235 - val_loss: 0.5018\n",
      "Epoch 26/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4911 - val_loss: 0.5035\n",
      "Epoch 27/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5115 - val_loss: 0.4500\n",
      "Epoch 28/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4071 - val_loss: 0.6574\n",
      "Epoch 29/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4598 - val_loss: 0.5074\n",
      "Epoch 30/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4626 - val_loss: 0.4941\n",
      "Epoch 31/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4897 - val_loss: 0.4780\n",
      "Epoch 32/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5747 - val_loss: 0.4769\n",
      "Epoch 33/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4705 - val_loss: 0.4706\n",
      "Epoch 34/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4071 - val_loss: 0.4646\n",
      "Epoch 35/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3297 - val_loss: 0.4985\n",
      "Epoch 36/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3544 - val_loss: 0.5173\n",
      "Epoch 37/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3365 - val_loss: 0.5292\n",
      "Epoch 38/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3643 - val_loss: 0.4475\n",
      "Epoch 39/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4021 - val_loss: 0.5599\n",
      "Epoch 40/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4462 - val_loss: 0.6334\n",
      "Epoch 41/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2842 - val_loss: 0.5356\n",
      "Epoch 42/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2488 - val_loss: 0.5474\n",
      "Epoch 43/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2623 - val_loss: 0.5448\n",
      "Epoch 44/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3526 - val_loss: 0.5940\n",
      "Epoch 45/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3595 - val_loss: 0.4617\n",
      "Epoch 46/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3625 - val_loss: 0.4828\n",
      "Epoch 47/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3406 - val_loss: 0.4239\n",
      "Epoch 48/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3127 - val_loss: 0.5824\n",
      "Epoch 49/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2921 - val_loss: 0.6187\n",
      "Epoch 50/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2082 - val_loss: 0.5878\n",
      "Epoch 51/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3119 - val_loss: 0.5408\n",
      "Epoch 52/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2578 - val_loss: 0.6613\n",
      "Epoch 53/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3107 - val_loss: 0.5530\n",
      "Epoch 54/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3673 - val_loss: 0.5141\n",
      "Epoch 55/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2393 - val_loss: 0.5537\n",
      "Epoch 56/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2642 - val_loss: 0.5497\n",
      "Epoch 57/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2567 - val_loss: 0.4758\n",
      "Epoch 58/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2948 - val_loss: 0.5133\n",
      "Epoch 59/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2734 - val_loss: 0.5133\n",
      "Epoch 60/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3011 - val_loss: 0.5378\n",
      "Epoch 61/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2362 - val_loss: 0.6103\n",
      "Epoch 62/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3781 - val_loss: 0.6600\n",
      "Epoch 63/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3260 - val_loss: 0.4882\n",
      "Epoch 64/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2456 - val_loss: 0.4958\n",
      "Epoch 65/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2610 - val_loss: 0.5366\n",
      "Epoch 66/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2480 - val_loss: 0.5706\n",
      "Epoch 67/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3130 - val_loss: 0.4526\n",
      "Epoch 68/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3078 - val_loss: 0.5053\n",
      "Epoch 69/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2409 - val_loss: 0.5319\n",
      "Epoch 70/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2522 - val_loss: 0.5201\n",
      "Epoch 71/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2118 - val_loss: 0.5586\n",
      "Epoch 72/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1861 - val_loss: 0.5245\n",
      "Epoch 73/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2306 - val_loss: 0.4531\n",
      "Epoch 74/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2212 - val_loss: 0.5319\n",
      "Epoch 75/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2795 - val_loss: 0.5716\n",
      "Epoch 76/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2118 - val_loss: 0.5407\n",
      "Epoch 77/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2638 - val_loss: 0.5043\n",
      "Epoch 78/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2462 - val_loss: 0.5036\n",
      "Epoch 79/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2554 - val_loss: 0.5340\n",
      "Epoch 80/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2509 - val_loss: 0.4966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2422 - val_loss: 0.4579\n",
      "Epoch 82/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1993 - val_loss: 0.5620\n",
      "Epoch 83/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2204 - val_loss: 0.5519\n",
      "Epoch 84/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2060 - val_loss: 0.5296\n",
      "Epoch 85/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1941 - val_loss: 0.5230\n",
      "Epoch 86/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1633 - val_loss: 0.6177\n",
      "Epoch 87/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1896 - val_loss: 0.5593\n",
      "Epoch 88/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1881 - val_loss: 0.6896\n",
      "Epoch 89/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1645 - val_loss: 0.5975\n",
      "Epoch 90/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2032 - val_loss: 0.5578\n",
      "Epoch 91/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2222 - val_loss: 0.5958\n",
      "Epoch 92/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1633 - val_loss: 0.5562\n",
      "Epoch 93/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1883 - val_loss: 0.5375\n",
      "Epoch 94/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1616 - val_loss: 0.5886\n",
      "Epoch 95/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1763 - val_loss: 0.5939\n",
      "Epoch 96/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2952 - val_loss: 0.6261\n",
      "Epoch 97/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2110 - val_loss: 0.5746\n",
      "Epoch 98/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2061 - val_loss: 0.5569\n",
      "Epoch 99/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1883 - val_loss: 0.4724\n",
      "Epoch 100/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2281 - val_loss: 0.5967\n",
      "Epoch 101/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1742 - val_loss: 0.5522\n",
      "Epoch 102/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2376 - val_loss: 0.6013\n",
      "Epoch 103/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2072 - val_loss: 0.6920\n",
      "Epoch 104/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1632 - val_loss: 0.6918\n",
      "Epoch 105/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1203 - val_loss: 0.7481\n",
      "Epoch 106/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1736 - val_loss: 0.6665\n",
      "Epoch 107/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1537 - val_loss: 0.6542\n",
      "Epoch 108/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1289 - val_loss: 0.5469\n",
      "Epoch 109/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1557 - val_loss: 0.4637\n",
      "Epoch 110/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2318 - val_loss: 0.5756\n",
      "Epoch 111/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1578 - val_loss: 0.6539\n",
      "Epoch 112/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2390 - val_loss: 0.6580\n",
      "Epoch 113/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2059 - val_loss: 0.7925\n",
      "Epoch 114/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1868 - val_loss: 0.6187\n",
      "Epoch 115/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2172 - val_loss: 0.6218\n",
      "Epoch 116/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1856 - val_loss: 0.5955\n",
      "Epoch 117/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1685 - val_loss: 0.6078\n",
      "Epoch 118/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1879 - val_loss: 0.6904\n",
      "Epoch 119/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1285 - val_loss: 0.6925\n",
      "Epoch 120/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1434 - val_loss: 0.6082\n",
      "Epoch 121/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1999 - val_loss: 0.5629\n",
      "Epoch 122/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2069 - val_loss: 0.6689\n",
      "Epoch 123/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1710 - val_loss: 0.5875\n",
      "Epoch 124/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1668 - val_loss: 0.5399\n",
      "Epoch 125/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1592 - val_loss: 0.6123\n",
      "Epoch 126/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1534 - val_loss: 0.6217\n",
      "Epoch 127/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1266 - val_loss: 0.7034\n",
      "Epoch 128/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1560 - val_loss: 0.6881\n",
      "Epoch 129/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1763 - val_loss: 0.4939\n",
      "Epoch 130/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1330 - val_loss: 0.6092\n",
      "Epoch 131/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1591 - val_loss: 0.5634\n",
      "Epoch 132/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1412 - val_loss: 0.5244\n",
      "Epoch 133/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1364 - val_loss: 0.5767\n",
      "Epoch 134/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1449 - val_loss: 0.5304\n",
      "Epoch 135/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1718 - val_loss: 0.5829\n",
      "Epoch 136/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1608 - val_loss: 0.6270\n",
      "Epoch 137/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1562 - val_loss: 0.5930\n",
      "Epoch 138/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1502 - val_loss: 0.5706\n",
      "Epoch 139/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1432 - val_loss: 0.6005\n",
      "Epoch 140/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1309 - val_loss: 0.5380\n",
      "Epoch 141/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1428 - val_loss: 0.6687\n",
      "Epoch 142/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2032 - val_loss: 0.7577\n",
      "Epoch 143/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1532 - val_loss: 0.5502\n",
      "Epoch 144/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1186 - val_loss: 0.6857\n",
      "Epoch 145/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1675 - val_loss: 0.6050\n",
      "Epoch 146/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1610 - val_loss: 0.5166\n",
      "Epoch 147/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1305 - val_loss: 0.6298\n",
      "Epoch 148/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1480 - val_loss: 0.5915\n",
      "Epoch 149/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1593 - val_loss: 0.6236\n",
      "Epoch 150/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1498 - val_loss: 0.7153\n",
      "Epoch 151/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1897 - val_loss: 0.6842\n",
      "Epoch 152/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2160 - val_loss: 0.4792\n",
      "Epoch 153/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1430 - val_loss: 0.5270\n",
      "Epoch 154/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1060 - val_loss: 0.6173\n",
      "Epoch 155/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1021 - val_loss: 0.6009\n",
      "Epoch 156/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1178 - val_loss: 0.6516\n",
      "Epoch 157/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1179 - val_loss: 0.6198\n",
      "Epoch 158/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1171 - val_loss: 0.5954\n",
      "Epoch 159/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1127 - val_loss: 0.6529\n",
      "Epoch 160/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1837 - val_loss: 0.6348\n",
      "Epoch 161/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1315 - val_loss: 0.5689\n",
      "Epoch 162/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1892 - val_loss: 0.5327\n",
      "Epoch 163/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1276 - val_loss: 0.5060\n",
      "Epoch 164/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1252 - val_loss: 0.5486\n",
      "Epoch 165/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1274 - val_loss: 0.6083\n",
      "Epoch 166/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1615 - val_loss: 0.5153\n",
      "Epoch 167/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1552 - val_loss: 0.6721\n",
      "Epoch 168/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1555 - val_loss: 0.5225\n",
      "Epoch 169/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1150 - val_loss: 0.5080\n",
      "Epoch 170/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0898 - val_loss: 0.5382\n",
      "Epoch 171/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1173 - val_loss: 0.5879\n",
      "Epoch 172/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1258 - val_loss: 0.5050\n",
      "Epoch 173/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0932 - val_loss: 0.5059\n",
      "Epoch 174/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1486 - val_loss: 0.5716\n",
      "Epoch 175/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1290 - val_loss: 0.5896\n",
      "Epoch 176/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1223 - val_loss: 0.5813\n",
      "Epoch 177/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0754 - val_loss: 0.5200\n",
      "Epoch 178/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0795 - val_loss: 0.4995\n",
      "Epoch 179/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1055 - val_loss: 0.5835\n",
      "Epoch 180/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1678 - val_loss: 0.9883\n",
      "Epoch 181/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1208 - val_loss: 0.6190\n",
      "Epoch 182/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1035 - val_loss: 0.4864\n",
      "Epoch 183/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0836 - val_loss: 0.6542\n",
      "Epoch 184/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1045 - val_loss: 0.5715\n",
      "Epoch 185/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1081 - val_loss: 0.6166\n",
      "Epoch 186/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1086 - val_loss: 0.5951\n",
      "Epoch 187/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1426 - val_loss: 0.5277\n",
      "Epoch 188/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1573 - val_loss: 0.6026\n",
      "Epoch 189/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1719 - val_loss: 0.6668\n",
      "Epoch 190/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1240 - val_loss: 0.5574\n",
      "Epoch 191/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0913 - val_loss: 0.6880\n",
      "Epoch 192/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0951 - val_loss: 0.5511\n",
      "Epoch 193/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1098 - val_loss: 0.5840\n",
      "Epoch 194/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0940 - val_loss: 0.5804\n",
      "Epoch 195/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1262 - val_loss: 0.4763\n",
      "Epoch 196/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1774 - val_loss: 0.5893\n",
      "Epoch 197/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1588 - val_loss: 0.5335\n",
      "Epoch 198/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1087 - val_loss: 0.5246\n",
      "Epoch 199/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0872 - val_loss: 0.5980\n",
      "Epoch 200/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1385 - val_loss: 0.5735\n",
      "Epoch 201/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1234 - val_loss: 0.5981\n",
      "Epoch 202/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1920 - val_loss: 0.5807\n",
      "Epoch 203/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0941 - val_loss: 0.5809\n",
      "Epoch 204/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1029 - val_loss: 0.5104\n",
      "Epoch 205/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1142 - val_loss: 0.5216\n",
      "Epoch 206/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0866 - val_loss: 0.5533\n",
      "Epoch 207/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0774 - val_loss: 0.5468\n",
      "Epoch 208/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1512 - val_loss: 0.5594\n",
      "Epoch 209/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1104 - val_loss: 0.5120\n",
      "Epoch 210/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1208 - val_loss: 0.5695\n",
      "Epoch 211/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1172 - val_loss: 0.4667\n",
      "Epoch 212/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0997 - val_loss: 0.4676\n",
      "Epoch 213/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1080 - val_loss: 0.5714\n",
      "Epoch 214/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0891 - val_loss: 0.4989\n",
      "Epoch 215/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0871 - val_loss: 0.5041\n",
      "Epoch 216/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0725 - val_loss: 0.6168\n",
      "Epoch 217/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1003 - val_loss: 0.4737\n",
      "Epoch 218/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1152 - val_loss: 0.4820\n",
      "Epoch 219/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0794 - val_loss: 0.5414\n",
      "Epoch 220/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0878 - val_loss: 0.6227\n",
      "Epoch 221/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0957 - val_loss: 0.5137\n",
      "Epoch 222/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0961 - val_loss: 0.4891\n",
      "Epoch 223/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0932 - val_loss: 0.4998\n",
      "Epoch 224/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1000 - val_loss: 0.5247\n",
      "Epoch 225/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0820 - val_loss: 0.5582\n",
      "Epoch 226/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1342 - val_loss: 0.5513\n",
      "Epoch 227/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0883 - val_loss: 0.5224\n",
      "Epoch 228/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0991 - val_loss: 0.6220\n",
      "Epoch 229/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0867 - val_loss: 0.5778\n",
      "Epoch 230/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0958 - val_loss: 0.5665\n",
      "Epoch 231/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0999 - val_loss: 0.5129\n",
      "Epoch 232/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0750 - val_loss: 0.5141\n",
      "Epoch 233/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2342 - val_loss: 0.5594\n",
      "Epoch 234/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0928 - val_loss: 0.5407\n",
      "Epoch 235/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0900 - val_loss: 0.5215\n",
      "Epoch 236/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1516 - val_loss: 0.5963\n",
      "Epoch 237/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0747 - val_loss: 0.5529\n",
      "Epoch 238/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0960 - val_loss: 0.5205\n",
      "Epoch 239/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0919 - val_loss: 0.5659\n",
      "Epoch 240/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0905 - val_loss: 0.5753\n",
      "Epoch 241/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0883 - val_loss: 0.5480\n",
      "Epoch 242/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0715 - val_loss: 0.5836\n",
      "Epoch 243/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0881 - val_loss: 0.5743\n",
      "Epoch 244/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0672 - val_loss: 0.6039\n",
      "Epoch 245/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0783 - val_loss: 0.5154\n",
      "Epoch 246/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1121 - val_loss: 0.6025\n",
      "Epoch 247/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0562 - val_loss: 0.6315\n",
      "Epoch 248/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0738 - val_loss: 0.6225\n",
      "Epoch 249/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0610 - val_loss: 0.5933\n",
      "Epoch 250/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0784 - val_loss: 0.5696\n",
      "Epoch 251/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1075 - val_loss: 0.5066\n",
      "Epoch 252/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0890 - val_loss: 0.5240\n",
      "Epoch 253/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1106 - val_loss: 0.7356\n",
      "Epoch 254/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0924 - val_loss: 0.5790\n",
      "Epoch 255/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0942 - val_loss: 0.4958\n",
      "Epoch 256/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0772 - val_loss: 0.5447\n",
      "Epoch 257/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0597 - val_loss: 0.5561\n",
      "Epoch 258/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0985 - val_loss: 0.5909\n",
      "Epoch 259/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0794 - val_loss: 0.5483\n",
      "Epoch 260/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0944 - val_loss: 0.4993\n",
      "Epoch 261/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0550 - val_loss: 0.5537\n",
      "Epoch 262/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0784 - val_loss: 0.5307\n",
      "Epoch 263/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0895 - val_loss: 0.4970\n",
      "Epoch 264/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0716 - val_loss: 0.5905\n",
      "Epoch 265/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0558 - val_loss: 0.5192\n",
      "Epoch 266/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0890 - val_loss: 0.5132\n",
      "Epoch 267/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0795 - val_loss: 0.5899\n",
      "Epoch 268/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0709 - val_loss: 0.6514\n",
      "Epoch 269/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0883 - val_loss: 0.5285\n",
      "Epoch 270/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0440 - val_loss: 0.5455\n",
      "Epoch 271/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0654 - val_loss: 0.5983\n",
      "Epoch 272/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0617 - val_loss: 0.6168\n",
      "Epoch 273/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0864 - val_loss: 0.5999\n",
      "Epoch 274/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0984 - val_loss: 0.7026\n",
      "Epoch 275/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0898 - val_loss: 0.5834\n",
      "Epoch 276/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0878 - val_loss: 0.5636\n",
      "Epoch 277/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0718 - val_loss: 0.6239\n",
      "Epoch 278/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0792 - val_loss: 0.5910\n",
      "Epoch 279/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0891 - val_loss: 0.5873\n",
      "Epoch 280/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0889 - val_loss: 0.5568\n",
      "Epoch 281/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0650 - val_loss: 0.6814\n",
      "Epoch 282/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0779 - val_loss: 0.5269\n",
      "Epoch 283/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0777 - val_loss: 0.5041\n",
      "Epoch 284/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0708 - val_loss: 0.5458\n",
      "Epoch 285/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0843 - val_loss: 0.5659\n",
      "Epoch 286/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0822 - val_loss: 0.5025\n",
      "Epoch 287/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0703 - val_loss: 0.5448\n",
      "Epoch 288/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0743 - val_loss: 0.5544\n",
      "Epoch 289/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0695 - val_loss: 0.5007\n",
      "Epoch 290/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0758 - val_loss: 0.5284\n",
      "Epoch 291/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0613 - val_loss: 0.4931\n",
      "Epoch 292/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0851 - val_loss: 0.6368\n",
      "Epoch 293/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0754 - val_loss: 0.5964\n",
      "Epoch 294/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0545 - val_loss: 0.6999\n",
      "Epoch 295/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0479 - val_loss: 0.7443\n",
      "Epoch 296/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0738 - val_loss: 0.6105\n",
      "Epoch 297/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0664 - val_loss: 0.5553\n",
      "Epoch 298/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0680 - val_loss: 0.5913\n",
      "Epoch 299/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0864 - val_loss: 0.5727\n",
      "Epoch 300/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0617 - val_loss: 0.5614\n",
      "Epoch 301/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0598 - val_loss: 0.5593\n",
      "Epoch 302/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0620 - val_loss: 0.6949\n",
      "Epoch 303/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1481 - val_loss: 0.6818\n",
      "Epoch 304/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0657 - val_loss: 0.6007\n",
      "Epoch 305/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0862 - val_loss: 0.6501\n",
      "Epoch 306/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0438 - val_loss: 0.5497\n",
      "Epoch 307/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0532 - val_loss: 0.5736\n",
      "Epoch 308/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0498 - val_loss: 0.6582\n",
      "Epoch 309/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0671 - val_loss: 0.5970\n",
      "Epoch 310/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0674 - val_loss: 0.5694\n",
      "Epoch 311/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0695 - val_loss: 0.5850\n",
      "Epoch 312/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0833 - val_loss: 0.6454\n",
      "Epoch 313/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0790 - val_loss: 0.6963\n",
      "Epoch 314/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0690 - val_loss: 0.6389\n",
      "Epoch 315/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0732 - val_loss: 0.6156\n",
      "Epoch 316/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0629 - val_loss: 0.6226\n",
      "Epoch 317/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0576 - val_loss: 0.6726\n",
      "Epoch 318/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1011 - val_loss: 0.6023\n",
      "Epoch 319/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0614 - val_loss: 0.5816\n",
      "Epoch 320/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0643 - val_loss: 0.5055\n",
      "Epoch 321/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0682 - val_loss: 0.5081\n",
      "Epoch 322/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0483 - val_loss: 0.6338\n",
      "Epoch 323/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0750 - val_loss: 0.5894\n",
      "Epoch 324/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0672 - val_loss: 0.5889\n",
      "Epoch 325/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0652 - val_loss: 0.4868\n",
      "Epoch 326/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0535 - val_loss: 0.5792\n",
      "Epoch 327/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0543 - val_loss: 0.5591\n",
      "Epoch 328/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0495 - val_loss: 0.5543\n",
      "Epoch 329/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0593 - val_loss: 0.5648\n",
      "Epoch 330/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0554 - val_loss: 0.5412\n",
      "Epoch 331/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0681 - val_loss: 0.6573\n",
      "Epoch 332/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0741 - val_loss: 0.5414\n",
      "Epoch 333/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0631 - val_loss: 0.5328\n",
      "Epoch 334/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0755 - val_loss: 0.4732\n",
      "Epoch 335/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0773 - val_loss: 0.5961\n",
      "Epoch 336/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0615 - val_loss: 0.6573\n",
      "Epoch 337/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0617 - val_loss: 0.6382\n",
      "Epoch 338/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0682 - val_loss: 0.6185\n",
      "Epoch 339/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0631 - val_loss: 0.5895\n",
      "Epoch 340/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0660 - val_loss: 0.4935\n",
      "Epoch 341/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0852 - val_loss: 0.4666\n",
      "Epoch 342/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0612 - val_loss: 0.4902\n",
      "Epoch 343/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0782 - val_loss: 0.4907\n",
      "Epoch 344/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0860 - val_loss: 0.5082\n",
      "Epoch 345/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0739 - val_loss: 0.5158\n",
      "Epoch 346/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0661 - val_loss: 0.5854\n",
      "Epoch 347/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0421 - val_loss: 0.5457\n",
      "Epoch 348/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0565 - val_loss: 0.6063\n",
      "Epoch 349/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0519 - val_loss: 0.5695\n",
      "Epoch 350/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0587 - val_loss: 0.6122\n",
      "Epoch 351/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0621 - val_loss: 0.5307\n",
      "Epoch 352/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0590 - val_loss: 0.6619\n",
      "Epoch 353/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0712 - val_loss: 0.5717\n",
      "Epoch 354/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0532 - val_loss: 0.5241\n",
      "Epoch 355/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0496 - val_loss: 0.5648\n",
      "Epoch 356/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0552 - val_loss: 0.5825\n",
      "Epoch 357/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0566 - val_loss: 0.5908\n",
      "Epoch 358/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0431 - val_loss: 0.5360\n",
      "Epoch 359/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0622 - val_loss: 0.5222\n",
      "Epoch 360/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0442 - val_loss: 0.5522\n",
      "Epoch 361/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0647 - val_loss: 0.6578\n",
      "Epoch 362/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0461 - val_loss: 0.5137\n",
      "Epoch 363/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0483 - val_loss: 0.5302\n",
      "Epoch 364/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0844 - val_loss: 0.5680\n",
      "Epoch 365/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0483 - val_loss: 0.6462\n",
      "Epoch 366/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1052 - val_loss: 0.5809\n",
      "Epoch 367/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0711 - val_loss: 0.5589\n",
      "Epoch 368/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0914 - val_loss: 0.5621\n",
      "Epoch 369/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0644 - val_loss: 0.5147\n",
      "Epoch 370/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0601 - val_loss: 0.6402\n",
      "Epoch 371/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0569 - val_loss: 0.5842\n",
      "Epoch 372/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0434 - val_loss: 0.5585\n",
      "Epoch 373/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0762 - val_loss: 0.6558\n",
      "Epoch 374/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0472 - val_loss: 0.5961\n",
      "Epoch 375/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0657 - val_loss: 0.5243\n",
      "Epoch 376/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0505 - val_loss: 0.5413\n",
      "Epoch 377/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0561 - val_loss: 0.5019\n",
      "Epoch 378/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0589 - val_loss: 0.4843\n",
      "Epoch 379/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0524 - val_loss: 0.5857\n",
      "Epoch 380/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0535 - val_loss: 0.5421\n",
      "Epoch 381/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0588 - val_loss: 0.5443\n",
      "Epoch 382/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0519 - val_loss: 0.5604\n",
      "Epoch 383/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0526 - val_loss: 0.5528\n",
      "Epoch 384/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0608 - val_loss: 0.5593\n",
      "Epoch 385/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0346 - val_loss: 0.6020\n",
      "Epoch 386/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0487 - val_loss: 0.5359\n",
      "Epoch 387/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1339 - val_loss: 0.6242\n",
      "Epoch 388/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0769 - val_loss: 0.5401\n",
      "Epoch 389/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0759 - val_loss: 0.6287\n",
      "Epoch 390/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0532 - val_loss: 0.5135\n",
      "Epoch 391/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0487 - val_loss: 0.6138\n",
      "Epoch 392/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0502 - val_loss: 0.5679\n",
      "Epoch 393/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0438 - val_loss: 0.5858\n",
      "Epoch 394/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0361 - val_loss: 0.5997\n",
      "Epoch 395/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0455 - val_loss: 0.4785\n",
      "Epoch 396/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0725 - val_loss: 0.7141\n",
      "Epoch 397/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0511 - val_loss: 0.5743\n",
      "Epoch 398/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0557 - val_loss: 0.5999\n",
      "Epoch 399/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0787 - val_loss: 0.5916\n",
      "Epoch 400/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0727 - val_loss: 0.6015\n",
      "Epoch 401/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0584 - val_loss: 0.5938\n",
      "Epoch 402/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0651 - val_loss: 0.6847\n",
      "Epoch 403/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0789 - val_loss: 0.5500\n",
      "Epoch 404/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0860 - val_loss: 0.5701\n",
      "Epoch 405/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0631 - val_loss: 0.6633\n",
      "Epoch 406/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0602 - val_loss: 0.5398\n",
      "Epoch 407/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0503 - val_loss: 0.6311\n",
      "Epoch 408/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0471 - val_loss: 0.5517\n",
      "Epoch 409/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0487 - val_loss: 0.5455\n",
      "Epoch 410/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0450 - val_loss: 0.6243\n",
      "Epoch 411/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0444 - val_loss: 0.5904\n",
      "Epoch 412/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0518 - val_loss: 0.5346\n",
      "Epoch 413/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0563 - val_loss: 0.6145\n",
      "Epoch 414/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0639 - val_loss: 0.4743\n",
      "Epoch 415/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1381 - val_loss: 0.5556\n",
      "Epoch 416/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0585 - val_loss: 0.6383\n",
      "Epoch 417/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0446 - val_loss: 0.6100\n",
      "Epoch 418/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0814 - val_loss: 0.5767\n",
      "Epoch 419/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0723 - val_loss: 0.5771\n",
      "Epoch 420/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0685 - val_loss: 0.6474\n",
      "Epoch 421/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0488 - val_loss: 0.5475\n",
      "Epoch 422/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0550 - val_loss: 0.5125\n",
      "Epoch 423/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0813 - val_loss: 0.6435\n",
      "Epoch 424/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0681 - val_loss: 0.5697\n",
      "Epoch 425/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0585 - val_loss: 0.5312\n",
      "Epoch 426/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0643 - val_loss: 0.5272\n",
      "Epoch 427/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0465 - val_loss: 0.6520\n",
      "Epoch 428/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0660 - val_loss: 0.4857\n",
      "Epoch 429/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0654 - val_loss: 0.5931\n",
      "Epoch 430/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0503 - val_loss: 0.6170\n",
      "Epoch 431/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0408 - val_loss: 0.6288\n",
      "Epoch 432/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0452 - val_loss: 0.5995\n",
      "Epoch 433/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0527 - val_loss: 0.5873\n",
      "Epoch 434/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0525 - val_loss: 0.5986\n",
      "Epoch 435/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0469 - val_loss: 0.6619\n",
      "Epoch 436/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0416 - val_loss: 0.6425\n",
      "Epoch 437/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0867 - val_loss: 0.6044\n",
      "Epoch 438/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0461 - val_loss: 0.6143\n",
      "Epoch 439/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0456 - val_loss: 0.6757\n",
      "Epoch 440/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1154 - val_loss: 0.6035\n",
      "Epoch 441/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0489 - val_loss: 0.5770\n",
      "Epoch 442/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0367 - val_loss: 0.5717\n",
      "Epoch 443/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0390 - val_loss: 0.5624\n",
      "Epoch 444/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0505 - val_loss: 0.6428\n",
      "Epoch 445/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0488 - val_loss: 0.6611\n",
      "Epoch 446/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0548 - val_loss: 0.5353\n",
      "Epoch 447/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0440 - val_loss: 0.6011\n",
      "Epoch 448/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0790 - val_loss: 0.6490\n",
      "Epoch 449/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0473 - val_loss: 0.5953\n",
      "Epoch 450/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0521 - val_loss: 0.6316\n",
      "Epoch 451/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0937 - val_loss: 0.5419\n",
      "Epoch 452/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0466 - val_loss: 0.5957\n",
      "Epoch 453/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0673 - val_loss: 0.5671\n",
      "Epoch 454/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0496 - val_loss: 0.5339\n",
      "Epoch 455/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0489 - val_loss: 0.5291\n",
      "Epoch 456/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0476 - val_loss: 0.6658\n",
      "Epoch 457/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0501 - val_loss: 0.5716\n",
      "Epoch 458/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0716 - val_loss: 0.5080\n",
      "Epoch 459/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0601 - val_loss: 0.6440\n",
      "Epoch 460/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0483 - val_loss: 0.5098\n",
      "Epoch 461/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0609 - val_loss: 0.5515\n",
      "Epoch 462/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0422 - val_loss: 0.6403\n",
      "Epoch 463/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0372 - val_loss: 0.5154\n",
      "Epoch 464/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0337 - val_loss: 0.5725\n",
      "Epoch 465/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0423 - val_loss: 0.6178\n",
      "Epoch 466/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0452 - val_loss: 0.5001\n",
      "Epoch 467/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1086 - val_loss: 0.4803\n",
      "Epoch 468/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0639 - val_loss: 0.5552\n",
      "Epoch 469/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0489 - val_loss: 0.5635\n",
      "Epoch 470/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0488 - val_loss: 0.6174\n",
      "Epoch 471/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0402 - val_loss: 0.5660\n",
      "Epoch 472/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0298 - val_loss: 0.5554\n",
      "Epoch 473/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0367 - val_loss: 0.5823\n",
      "Epoch 474/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0483 - val_loss: 0.5467\n",
      "Epoch 475/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0442 - val_loss: 0.5946\n",
      "Epoch 476/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0472 - val_loss: 0.6018\n",
      "Epoch 477/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0447 - val_loss: 0.5003\n",
      "Epoch 478/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0482 - val_loss: 0.5930\n",
      "Epoch 479/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0600 - val_loss: 0.5347\n",
      "Epoch 480/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0617 - val_loss: 0.5520\n",
      "Epoch 481/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0513 - val_loss: 0.5820\n",
      "Epoch 482/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0510 - val_loss: 0.5490\n",
      "Epoch 483/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0558 - val_loss: 0.6097\n",
      "Epoch 484/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0642 - val_loss: 0.5307\n",
      "Epoch 485/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0423 - val_loss: 0.5566\n",
      "Epoch 486/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0424 - val_loss: 0.5075\n",
      "Epoch 487/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0549 - val_loss: 0.5966\n",
      "Epoch 488/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0522 - val_loss: 0.6394\n",
      "Epoch 489/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0397 - val_loss: 0.5471\n",
      "Epoch 490/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0723 - val_loss: 0.5321\n",
      "Epoch 491/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0726 - val_loss: 0.5920\n",
      "Epoch 492/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0523 - val_loss: 0.5237\n",
      "Epoch 493/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0922 - val_loss: 0.5375\n",
      "Epoch 494/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0521 - val_loss: 0.5462\n",
      "Epoch 495/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0491 - val_loss: 0.5466\n",
      "Epoch 496/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0510 - val_loss: 0.5614\n",
      "Epoch 497/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0397 - val_loss: 0.5719\n",
      "Epoch 498/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0771 - val_loss: 0.5633\n",
      "Epoch 499/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0438 - val_loss: 0.5212\n",
      "Epoch 500/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0448 - val_loss: 0.5084\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 24, 12)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 24, 24)       1800        input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "seq_self_attention_3 (SeqSelfAt (None, 24, 24)       577         bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 24, 24)       0           seq_self_attention_3[0][0]       \n",
      "                                                                 bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_3 (LayerNor (None, 24, 24)       48          add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 576)          0           layer_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 12)           6924        flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 12)           0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 588)          0           dropout_3[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            589         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 1)            2           dense_8[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 9,940\n",
      "Trainable params: 9,940\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Predict time:  0.30518341064453125\n",
      "RMSE:  8.424069168798404\n",
      "RMSE2:  6.738826888786874\n",
      "MAE:  6.809965016841889\n",
      "MAE2:  6.809965016841889\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'mse score')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "opt = optimizers.Adam(lr=0.001,decay = 0.0001)\n",
    "print('Train...')\n",
    "# model.compile(optimizer = opt , loss=\"mse\")\n",
    "model.compile(optimizer = \"adam\" , loss=\"mse\")\n",
    "history = model.fit([x_train,x_train], y_train, epochs = 500, batch_size=8, validation_split=0.1, shuffle=True)\n",
    "# history = model.fit(x_train, y_train, epochs = 500, batch_size=6, validation_split=0.1, shuffle=True)\n",
    "model.summary()\n",
    "#Save Model\n",
    "model.save('GRU_Single_Attention_model_Pitfield.h5')  # creates a HDF5 file \n",
    "del model\n",
    "\n",
    "custom_ob = {'LayerNormalization': LayerNormalization , 'SeqSelfAttention':SeqSelfAttention}\n",
    "model = load_model('GRU_Single_Attention_model_Pitfield.h5', custom_objects=custom_ob)\n",
    "t1 = time.time()\n",
    "# y_pred = model.predict([x_test,x_test])\n",
    "y_pred2 = model.predict(x_test)\n",
    "y_pred = model.predict(x_train)\n",
    "t2 = time.time()\n",
    "print('Predict time: ',t2-t1)\n",
    "y_pred = scaler.inverse_transform(y_pred)#Undo scaling\n",
    "rmse_lstm2 = np.sqrt(mean_squared_error(y_test, y_pred2))\n",
    "rmse_lstm = np.sqrt(mean_squared_error(y_train, y_pred))\n",
    "print('RMSE: ',rmse_lstm)\n",
    "print('RMSE2: ',rmse_lstm2)\n",
    "mae = mean_absolute_error(y_test, y_pred2)\n",
    "mae = mean_absolute_error(y_train, y_pred)\n",
    "print('MAE: ',mae)\n",
    "print('MAE2: ',mae)\n",
    "# r22 =  r2_score(y_test, y_pred2)\n",
    "# r2 =  r2_score(y_train, y_pred)\n",
    "# print('R-square: ',r2)\n",
    "# print('R-square2: ',r22)\n",
    "\n",
    "# n = len(y_test)\n",
    "# p = 12\n",
    "# Adj_r2 = 1-(1-r2)*(n-1)/(n-p-1)\n",
    "# Adj_r22 = 1-(1-r22)*(n-1)/(n-p-1)\n",
    "# print('Adj R-square: ',Adj_r2)\n",
    "# print('Adj R-square2: ',Adj_r22)\n",
    "\n",
    "plt.plot(history.history[\"loss\"],label=\"loss\")\n",
    "plt.plot(history.history[\"val_loss\"],label=\"val_loss\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"No. Of Epochs\")\n",
    "plt.ylabel(\"mse score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3099842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 108 samples, validate on 12 samples\n",
      "Epoch 1/200\n",
      "108/108 [==============================] - 3s 29ms/step - loss: 1.9354 - val_loss: 1.5108\n",
      "Epoch 2/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.8679 - val_loss: 1.4483\n",
      "Epoch 3/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.8455 - val_loss: 1.3850\n",
      "Epoch 4/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.7982 - val_loss: 1.3318\n",
      "Epoch 5/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.7438 - val_loss: 1.2814\n",
      "Epoch 6/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.7209 - val_loss: 1.2380\n",
      "Epoch 7/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.6817 - val_loss: 1.1930\n",
      "Epoch 8/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.6267 - val_loss: 1.1509\n",
      "Epoch 9/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.6092 - val_loss: 1.1138\n",
      "Epoch 10/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.5885 - val_loss: 1.0826\n",
      "Epoch 11/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.5293 - val_loss: 1.0527\n",
      "Epoch 12/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.5278 - val_loss: 1.0289\n",
      "Epoch 13/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.4636 - val_loss: 1.0010\n",
      "Epoch 14/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.4557 - val_loss: 0.9694\n",
      "Epoch 15/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.4213 - val_loss: 0.9379\n",
      "Epoch 16/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.3775 - val_loss: 0.9111\n",
      "Epoch 17/200\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 1.3284 - val_loss: 0.8877\n",
      "Epoch 18/200\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 1.3251 - val_loss: 0.8648\n",
      "Epoch 19/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.2832 - val_loss: 0.8446\n",
      "Epoch 20/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.2510 - val_loss: 0.8268\n",
      "Epoch 21/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.2223 - val_loss: 0.8126\n",
      "Epoch 22/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.1958 - val_loss: 0.7986\n",
      "Epoch 23/200\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 1.1698 - val_loss: 0.7881\n",
      "Epoch 24/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.1660 - val_loss: 0.7733\n",
      "Epoch 25/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.1200 - val_loss: 0.7602\n",
      "Epoch 26/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.1793 - val_loss: 0.7440\n",
      "Epoch 27/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.1133 - val_loss: 0.7296\n",
      "Epoch 28/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.0855 - val_loss: 0.7177\n",
      "Epoch 29/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.1083 - val_loss: 0.7042\n",
      "Epoch 30/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.0815 - val_loss: 0.7014\n",
      "Epoch 31/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.0525 - val_loss: 0.6847\n",
      "Epoch 32/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.0490 - val_loss: 0.6837\n",
      "Epoch 33/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.0362 - val_loss: 0.6814\n",
      "Epoch 34/200\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.9956 - val_loss: 0.6750\n",
      "Epoch 35/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.9791 - val_loss: 0.6622\n",
      "Epoch 36/200\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.9894 - val_loss: 0.6358\n",
      "Epoch 37/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.9742 - val_loss: 0.6246\n",
      "Epoch 38/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.0021 - val_loss: 0.6239\n",
      "Epoch 39/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.9985 - val_loss: 0.6181\n",
      "Epoch 40/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.0070 - val_loss: 0.6249\n",
      "Epoch 41/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.9615 - val_loss: 0.6142\n",
      "Epoch 42/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.9623 - val_loss: 0.6181\n",
      "Epoch 43/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.9188 - val_loss: 0.6093\n",
      "Epoch 44/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.9603 - val_loss: 0.5941\n",
      "Epoch 45/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.9411 - val_loss: 0.5789\n",
      "Epoch 46/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.9658 - val_loss: 0.5716\n",
      "Epoch 47/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.8997 - val_loss: 0.5805\n",
      "Epoch 48/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8982 - val_loss: 0.5752\n",
      "Epoch 49/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8966 - val_loss: 0.5645\n",
      "Epoch 50/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8852 - val_loss: 0.5724\n",
      "Epoch 51/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8594 - val_loss: 0.5607\n",
      "Epoch 52/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8763 - val_loss: 0.5555\n",
      "Epoch 53/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.9110 - val_loss: 0.5350\n",
      "Epoch 54/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8503 - val_loss: 0.5455\n",
      "Epoch 55/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8850 - val_loss: 0.5485\n",
      "Epoch 56/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8974 - val_loss: 0.5299\n",
      "Epoch 57/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.8855 - val_loss: 0.5257\n",
      "Epoch 58/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8380 - val_loss: 0.5326\n",
      "Epoch 59/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8464 - val_loss: 0.5278\n",
      "Epoch 60/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8528 - val_loss: 0.5504\n",
      "Epoch 61/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.9041 - val_loss: 0.5115\n",
      "Epoch 62/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8081 - val_loss: 0.5128\n",
      "Epoch 63/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.8544 - val_loss: 0.5158\n",
      "Epoch 64/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8247 - val_loss: 0.5390\n",
      "Epoch 65/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.8292 - val_loss: 0.5458\n",
      "Epoch 66/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8228 - val_loss: 0.5369\n",
      "Epoch 67/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8180 - val_loss: 0.5233\n",
      "Epoch 68/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8372 - val_loss: 0.5152\n",
      "Epoch 69/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8319 - val_loss: 0.5024\n",
      "Epoch 70/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8218 - val_loss: 0.4990\n",
      "Epoch 71/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8259 - val_loss: 0.5349\n",
      "Epoch 72/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8149 - val_loss: 0.5137\n",
      "Epoch 73/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8087 - val_loss: 0.5231\n",
      "Epoch 74/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8233 - val_loss: 0.5127\n",
      "Epoch 75/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8240 - val_loss: 0.4871\n",
      "Epoch 76/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7982 - val_loss: 0.5160\n",
      "Epoch 77/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7898 - val_loss: 0.4814\n",
      "Epoch 78/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8421 - val_loss: 0.5285\n",
      "Epoch 79/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.8110 - val_loss: 0.4814\n",
      "Epoch 80/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.7958 - val_loss: 0.4651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7428 - val_loss: 0.4866\n",
      "Epoch 82/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7866 - val_loss: 0.5198\n",
      "Epoch 83/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7488 - val_loss: 0.5083\n",
      "Epoch 84/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7459 - val_loss: 0.5006\n",
      "Epoch 85/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7846 - val_loss: 0.4619\n",
      "Epoch 86/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7699 - val_loss: 0.5040\n",
      "Epoch 87/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.8252 - val_loss: 0.4538\n",
      "Epoch 88/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7487 - val_loss: 0.4674\n",
      "Epoch 89/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6858 - val_loss: 0.4315\n",
      "Epoch 90/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6986 - val_loss: 0.4784\n",
      "Epoch 91/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7291 - val_loss: 0.5102\n",
      "Epoch 92/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.8115 - val_loss: 0.4253\n",
      "Epoch 93/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7336 - val_loss: 0.4548\n",
      "Epoch 94/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6704 - val_loss: 0.4084\n",
      "Epoch 95/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6797 - val_loss: 0.4646\n",
      "Epoch 96/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6873 - val_loss: 0.4462\n",
      "Epoch 97/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7098 - val_loss: 0.4324\n",
      "Epoch 98/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6842 - val_loss: 0.3917\n",
      "Epoch 99/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6500 - val_loss: 0.4261\n",
      "Epoch 100/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6209 - val_loss: 0.4592\n",
      "Epoch 101/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6593 - val_loss: 0.4627\n",
      "Epoch 102/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6412 - val_loss: 0.4274\n",
      "Epoch 103/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6433 - val_loss: 0.4133\n",
      "Epoch 104/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6060 - val_loss: 0.4771\n",
      "Epoch 105/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5673 - val_loss: 0.4144\n",
      "Epoch 106/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6047 - val_loss: 0.5005\n",
      "Epoch 107/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6681 - val_loss: 0.3904\n",
      "Epoch 108/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5683 - val_loss: 0.3933\n",
      "Epoch 109/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5557 - val_loss: 0.4952\n",
      "Epoch 110/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5517 - val_loss: 0.4661\n",
      "Epoch 111/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5705 - val_loss: 0.4767\n",
      "Epoch 112/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5282 - val_loss: 0.4426\n",
      "Epoch 113/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5160 - val_loss: 0.4875\n",
      "Epoch 114/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5591 - val_loss: 0.3773\n",
      "Epoch 115/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5387 - val_loss: 0.3810\n",
      "Epoch 116/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4822 - val_loss: 0.6021\n",
      "Epoch 117/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5835 - val_loss: 0.4265\n",
      "Epoch 118/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5069 - val_loss: 0.4983\n",
      "Epoch 119/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5442 - val_loss: 0.4844\n",
      "Epoch 120/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4751 - val_loss: 0.4485\n",
      "Epoch 121/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5009 - val_loss: 0.4676\n",
      "Epoch 122/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4741 - val_loss: 0.3888\n",
      "Epoch 123/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4993 - val_loss: 0.4025\n",
      "Epoch 124/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4993 - val_loss: 0.4702\n",
      "Epoch 125/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4799 - val_loss: 0.4565\n",
      "Epoch 126/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4858 - val_loss: 0.5038\n",
      "Epoch 127/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5070 - val_loss: 0.4371\n",
      "Epoch 128/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4660 - val_loss: 0.4002\n",
      "Epoch 129/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4452 - val_loss: 0.3850\n",
      "Epoch 130/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5091 - val_loss: 0.4112\n",
      "Epoch 131/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5067 - val_loss: 0.4035\n",
      "Epoch 132/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5023 - val_loss: 0.4023\n",
      "Epoch 133/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4250 - val_loss: 0.3882\n",
      "Epoch 134/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4143 - val_loss: 0.3694\n",
      "Epoch 135/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4896 - val_loss: 0.3985\n",
      "Epoch 136/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4731 - val_loss: 0.4429\n",
      "Epoch 137/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4928 - val_loss: 0.4260\n",
      "Epoch 138/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5246 - val_loss: 0.4583\n",
      "Epoch 139/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4226 - val_loss: 0.4376\n",
      "Epoch 140/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4239 - val_loss: 0.4485\n",
      "Epoch 141/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4548 - val_loss: 0.4544\n",
      "Epoch 142/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3960 - val_loss: 0.3893\n",
      "Epoch 143/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4451 - val_loss: 0.4488\n",
      "Epoch 144/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4745 - val_loss: 0.4919\n",
      "Epoch 145/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4218 - val_loss: 0.4128\n",
      "Epoch 146/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4427 - val_loss: 0.3920\n",
      "Epoch 147/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4324 - val_loss: 0.4994\n",
      "Epoch 148/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5032 - val_loss: 0.6290\n",
      "Epoch 149/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4650 - val_loss: 0.4846\n",
      "Epoch 150/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4456 - val_loss: 0.4316\n",
      "Epoch 151/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4615 - val_loss: 0.4434\n",
      "Epoch 152/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4250 - val_loss: 0.4436\n",
      "Epoch 153/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4444 - val_loss: 0.4575\n",
      "Epoch 154/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4223 - val_loss: 0.4450\n",
      "Epoch 155/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4151 - val_loss: 0.4613\n",
      "Epoch 156/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3795 - val_loss: 0.3831\n",
      "Epoch 157/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3662 - val_loss: 0.4500\n",
      "Epoch 158/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4363 - val_loss: 0.4378\n",
      "Epoch 159/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.4329 - val_loss: 0.5234\n",
      "Epoch 160/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4488 - val_loss: 0.4032\n",
      "Epoch 161/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4392 - val_loss: 0.4435\n",
      "Epoch 162/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4104 - val_loss: 0.4273\n",
      "Epoch 163/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.4636 - val_loss: 0.5130\n",
      "Epoch 164/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4132 - val_loss: 0.4303\n",
      "Epoch 165/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3779 - val_loss: 0.4153\n",
      "Epoch 166/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4163 - val_loss: 0.3769\n",
      "Epoch 167/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4503 - val_loss: 0.4144\n",
      "Epoch 168/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4076 - val_loss: 0.3774\n",
      "Epoch 169/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4065 - val_loss: 0.3755\n",
      "Epoch 170/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5035 - val_loss: 0.3438\n",
      "Epoch 171/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.4264 - val_loss: 0.3373\n",
      "Epoch 172/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4793 - val_loss: 0.3844\n",
      "Epoch 173/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4402 - val_loss: 0.3812\n",
      "Epoch 174/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3759 - val_loss: 0.4248\n",
      "Epoch 175/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4129 - val_loss: 0.3622\n",
      "Epoch 176/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4776 - val_loss: 0.3974\n",
      "Epoch 177/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3821 - val_loss: 0.4321\n",
      "Epoch 178/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3495 - val_loss: 0.4158\n",
      "Epoch 179/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4471 - val_loss: 0.4713\n",
      "Epoch 180/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4447 - val_loss: 0.3590\n",
      "Epoch 181/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3951 - val_loss: 0.3521\n",
      "Epoch 182/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.3696 - val_loss: 0.4497\n",
      "Epoch 183/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.4014 - val_loss: 0.3610\n",
      "Epoch 184/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.3708 - val_loss: 0.4150\n",
      "Epoch 185/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.4575 - val_loss: 0.4485\n",
      "Epoch 186/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4255 - val_loss: 0.3803\n",
      "Epoch 187/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.3877 - val_loss: 0.4063\n",
      "Epoch 188/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.3980 - val_loss: 0.3426\n",
      "Epoch 189/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.4424 - val_loss: 0.3405\n",
      "Epoch 190/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3741 - val_loss: 0.3712\n",
      "Epoch 191/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.3735 - val_loss: 0.4516\n",
      "Epoch 192/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.4131 - val_loss: 0.3485\n",
      "Epoch 193/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.4284 - val_loss: 0.4784\n",
      "Epoch 194/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.4130 - val_loss: 0.4447\n",
      "Epoch 195/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.4123 - val_loss: 0.3591\n",
      "Epoch 196/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.4134 - val_loss: 0.4719\n",
      "Epoch 197/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.3940 - val_loss: 0.4028\n",
      "Epoch 198/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.3608 - val_loss: 0.4180\n",
      "Epoch 199/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.3815 - val_loss: 0.4007\n",
      "Epoch 200/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.4224 - val_loss: 0.3743\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_6 (Bidirection (None, 24, 12)            684       \n",
      "_________________________________________________________________\n",
      "layer_normalization_5 (Layer (None, 24, 12)            24        \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 12)                684       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 1,405\n",
      "Trainable params: 1,405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Saved\n",
      "Predict time:  0.3510603904724121\n",
      "RMSE:  8.439720939305182\n",
      "RMSE2:  6.501888008115648\n",
      "MAE:  7.296073053677877\n",
      "MAE2:  7.296073053677877\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'mse score')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEICAYAAABS0fM3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAABT80lEQVR4nO3dd3hUVfrA8e876b33BEKHkFBDF0Sl2cDeC9a1rGUtq+uuq+u6xXV1qz/r2isqKjZQFEE6AQKhEwKkkh6SENJmzu+PMylAgAAzmZCcz/PkmcydO/e+uUnuO6eLUgrDMAyj+7K4OgDDMAzDtUwiMAzD6OZMIjAMw+jmTCIwDMPo5kwiMAzD6OZMIjAMw+jmnJYIRCRBRBaJyBYR2Swi97Wxj4jIv0UkU0Q2isgIZ8VjGIZhtM3dicduBB5USq0TkQBgrYh8r5Ta0mqfc4F+9q8xwIv2x6MKDw9XiYmJTgrZMAyja1q7dm2JUiqirdeclgiUUgVAgf37KhHZCsQBrRPBLOBtpUe1rRSRYBGJsb+3TYmJiaSlpTkrbMMwjC5JRPYe7bUOaSMQkURgOLDqsJfigJxWz3Pt2w5//+0ikiYiacXFxU6L0zAMoztyeiIQEX/gU+B+pVTlyRxDKfWKUipVKZUaEdFmycYwDMM4SU5NBCLigU4C7yml5raxSx6Q0Op5vH2bYRiG0UGc1kYgIgL8D9iqlHr+KLvNA34pIh+iG4n3H6t9wDCM7quhoYHc3Fxqa2tdHUqn5u3tTXx8PB4eHu1+jzN7DU0ArgcyRCTdvu0xoAeAUuol4BvgPCATqAFucmI8hmGcxnJzcwkICCAxMRH9OdM4nFKK0tJScnNz6dWrV7vf58xeQ0uBY/627L2F7nZWDIZhdB21tbUmCRyHiBAWFsaJdqoxI4sNwzhtmCRwfCdzjbpNIti+r4q/fLOV6rpGV4diGIbRqXSbRJBTVsPLS7LYvu+kerAahmHg7+/v6hCcotskgkGxgQBsKahycSSGYRidS7dJBLFB3gR6u7O1wJQIDMM4NUopHn74YZKTk0lJSeGjjz4CoKCggEmTJjFs2DCSk5P5+eefsVqtzJ49u3nff/zjHy6O/kjO7D7aqYgIg2ICTSIwjC7gD19uZku+Y/+Xk2IDeeLCwe3ad+7cuaSnp7NhwwZKSkoYNWoUkyZN4v3332f69On89re/xWq1UlNTQ3p6Onl5eWzatAmAiooKh8btCN2mRAAwKCaQ7fuqsNmUq0MxDOM0tnTpUq6++mrc3NyIiorizDPPZM2aNYwaNYo33niDJ598koyMDAICAujduzdZWVncc889zJ8/n8DAQFeHf4RuUyIASIoJpKbeyt6yGnqF+7k6HMMwTlJ7P7l3tEmTJrFkyRK+/vprZs+ezQMPPMANN9zAhg0bWLBgAS+99BJz5szh9ddfd3Woh+h2JQLAVA8ZhnFKJk6cyEcffYTVaqW4uJglS5YwevRo9u7dS1RUFLfddhu33nor69ato6SkBJvNxqWXXsrTTz/NunXrXB3+EbpViaBflD8W0YngvJQYV4djGMZp6uKLL2bFihUMHToUEeFvf/sb0dHRvPXWWzz77LN4eHjg7+/P22+/TV5eHjfddBM2mw2Av/zlLy6O/kiiZ3k4faSmpqpTWZhm2j8W4+flztw7x5tRioZxGtm6dSuDBg1ydRinhbaulYisVUqltrV/t6oaArh+XCLrsytYuLXI1aEYhmF0Ct0uEVw1KoHeEX785dutNFhtrg7HMAzD5bpdIvBws/DwtAFkFR9g6c4SV4djGIbhct0uEQCcNTAST3cLyzJNIjAMw+iWicDbw42RPUJYtqvU1aEYhmG4nNMSgYi8LiJFIrLpKK8HiciXIrJBRDaLSIeuTjahbxhbCyopra7ryNMahmF0Os4sEbwJzDjG63cDW5RSQ4HJwHMi4unEeA4xvm84ACuyTKnAMIzuzWmJQCm1BCg71i5AgH2Re3/7vh22asyQuCACvNxZbqqHDMNwgmOtXbBnzx6Sk5M7MJpjc2UbwX+BQUA+kAHcp5Rqsz+niNwuImkiknaia3EejbubhVG9QllpSgSGYXRzrpxiYjqQDpwN9AG+F5GflVJHTASklHoFeAX0yGJHBTCyZwg/biui/EA9IX4dVitlGMap+vZR2Jfh2GNGp8C5fz3qy48++igJCQncfffdADz55JO4u7uzaNEiysvLaWho4Omnn2bWrFkndNra2lruvPNO0tLScHd35/nnn+ess85i8+bN3HTTTdTX12Oz2fj000+JjY3liiuuIDc3F6vVyuOPP86VV155Sj82uLZEcBMwV2mZwG5gYEcGMLJnCADrc8o78rSGYZyGrrzySubMmdP8fM6cOdx444189tlnrFu3jkWLFvHggw9yotP2vPDCC4gIGRkZfPDBB9x4443U1tby0ksvcd9995Genk5aWhrx8fHMnz+f2NhYNmzYwKZNm5gx41jNsO3nyhJBNnAO8LOIRAEDgKyODGBofDDuFiFtTzlnD4zqyFMbhnEqjvHJ3VmGDx9OUVER+fn5FBcXExISQnR0NL/61a9YsmQJFouFvLw8CgsLiY6Obvdxly5dyj333APAwIED6dmzJzt27GDcuHH86U9/Ijc3l0suuYR+/fqRkpLCgw8+yCOPPMIFF1zAxIkTHfKzObP76AfACmCAiOSKyC0icoeI3GHf5Y/AeBHJAH4AHlFKdegILx9PNwbHBrJ2rykRGIZxfJdffjmffPIJH330EVdeeSXvvfcexcXFrF27lvT0dKKioqitrXXIua655hrmzZuHj48P5513Hj/++CP9+/dn3bp1pKSk8Lvf/Y6nnnrKIedyWolAKXX1cV7PB6Y56/ztNaJnCB+szqbBasPDrVuOrzMMo52uvPJKbrvtNkpKSli8eDFz5swhMjISDw8PFi1axN69e0/4mBMnTuS9997j7LPPZseOHWRnZzNgwACysrLo3bs39957L9nZ2WzcuJGBAwcSGhrKddddR3BwMK+99ppDfq5utR5BW0b2DOGNZXvYmLu/uc3AMAyjLYMHD6aqqoq4uDhiYmK49tprufDCC0lJSSE1NZWBA0+8mfOuu+7izjvvJCUlBXd3d9588028vLyYM2cO77zzDh4eHkRHR/PYY4+xZs0aHn74YSwWCx4eHrz44osO+bm63XoEh6uoqeeMZxYxrk8Yr97Q5lTdhmF0AmY9gvYz6xGcoGBfT+44szffbykkbc+xxr8ZhmF0Td0+EQDcfEYvIgK8eP77Ha4OxTCMLiQjI4Nhw4Yd8jVmzBhXh3WEbt9GAODr6c41o3vw7x93UlxVR0SAl6tDMgyjDUqp02qJ2ZSUFNLT0zv0nCdT3W9KBHbTB0ejFCzcWujqUAzDaIO3tzelpaUndaPrLpRSlJaW4u3tfULvMyUCu0ExASSE+rBg8z6uHt3D1eEYhnGY+Ph4cnNzcdR8Y12Vt7c38fHxJ/QekwjsRITpSdG8vWIvVbUNBHh7uDokwzBa8fDwoFevXq4Oo0syVUOtTE+Opt5q42ezlrFhGN2ISQStDEsIxtvDQtoeM+WEYRjdh0kErXi4WRgSH8zabJMIDMPoPkwiOMzIniFszttPbYPV1aEYhmF0CJMIDjOiRwiNNkVG3n5Xh2IYhtEhTCI4zIgewQBmamrDMLoNkwgOE+bvRa9wP9aZRGAYRjdhEkEbxvQK5eedJewuOeDqUAzDMJzOmSuUvS4iRSKy6Rj7TBaRdBHZLCKLnRXLibpvSj883S3c/1E6DVabq8MxDMNwKmeWCN4EjrqysogEA/8HzFRKDQYud2IsJyQmyIc/X5zChpwKXlnSocsoG4ZhdDinJQKl1BLgWBP8XwPMVUpl2/cvclYsJ+P8ITFMS4rivz9mkldx0NXhGIZhOI0r2wj6AyEi8pOIrBWRG462o4jcLiJpIpLWkRNOPX5BEgrFn7/e2mHnNAzD6GiuTATuwEjgfGA68LiI9G9rR6XUK0qpVKVUakRERIcFmBDqy/VjezJ/8z4O1psBZoZhdE2uTAS5wAKl1AGlVAmwBBjqwnjaNLpXGFabYnO+GWBmGEbX5MpE8AVwhoi4i4gvMAbodHUwQ+ODANiQaxKBYRhdk9PWIxCRD4DJQLiI5AJPAB4ASqmXlFJbRWQ+sBGwAa8ppY7a1dRVIgO9iQnyZkNOhatDMQzDcAqnJQKl1NXt2OdZ4FlnxeAoQ+OD2Zhb4eowDMMwnMKMLG6HIQlB7CmtoaKm3tWhGIZhOJxJBO0wLD4YgO+2FFJT3+jaYAzDMBzMJIJ2SIkPwtPdwq8/2ciZz/5Eo5l2wjCMLqR7JYKGg6DUCb8twNuD+fdN5BeTelNcVUeWmYzOMIwupPskgk1z4c9xULH3pN7eO8KfS0bEA5gxBYZhdCndJxGEJIKyQsGGkz5Enwg/vNwtbMqrdFxchmEYLtZ9EkFkEljcTykRuLtZGBgTaEoEhmF0Kd0nEXh4Q8QgyE8/pcMMjg1kc34l6iTaGgzDMDqj7pMIAGKH6hLBKdzEk2ODqKptJKfMTE1tGEbX0L0SQcwwqCmByvyTPsTg2EDANBgbhtF1dLNEYJ/ctCD9pA8xIDoAL3cL/1y4kwwzEZ1hGF1A90oEUckgllNqMPb2cOPF60ZQXlPP5S8vp6S6zoEBGoZhdLzulQg8fSF8wCklAoCzB0bxzi1jqG2w8dWGk69mMgzD6Ay6VyIAXT10ij2HQFcRJcUE8lm6SQSGYZzeumciqN4HVftO+VAXD49jQ04FWcXVDgjMMAzDNZyWCETkdREpEpFjLjYjIqNEpFFELnNWLIeIHaYfT7F6CGDmsFhE4HNTKjAM4zTmzBLBm8CMY+0gIm7AM8B3TozjUNEp+tEBiSAq0JvUniEs2lZ0yscyDMNwFaclAqXUEqDsOLvdA3wKdNyd1CsAwvo6JBEAnNk/goy8/ab3kGEYpy2XtRGISBxwMfBiO/a9XUTSRCStuLj41E/uoAZjgDP7RwLw804HxGUYhuECrmws/ifwiFLquKu8KKVeUUqlKqVSIyIiTv3MMcOgMhcOlJzyoQbHBhLu78lP200iMAzj9OS0xevbIRX4UEQAwoHzRKRRKfW508/c1GCctxb6Tz+lQ1kswqR+ESzaXoTVpnCzyKnHZxiG0YFcViJQSvVSSiUqpRKBT4C7OiQJAMSl6imps1c45HCT+kdQXtPA1gKzToFhGKcfp5UIROQDYDIQLiK5wBOAB4BS6iVnnbddPH119dBexySCcX3CAFiZVUpyXJBDjmkYhtFRnJYIlFJXn8C+s50Vx1H1HAerXtbrGHv4nNKhogK96R3ux4pdpdw6sbeDAjQMw+gY3W9kcZMe48Far9sJHGBM7zBW7y7DajML1hiGcXrpxolgrH50YPVQVV2jWafAMIzTTvdNBL6heunK7OUOOdzYXqEAfJGez4G6Rocc0zAMoyO0KxGISE8RmWL/3kdEApwbVgfpOR5yVoP11G/ckYHejOkVyv+W7mbCMz9SsN8sZWkYxunhuIlARG5Dd+982b4pHvjciTF1nJ7job4aCjMccri3bxnNy9ePpKKmgQWbTn12U8MwjI7QnhLB3cAEoBJAKbUTiHRmUB2mxzj96KB2Ai93N6YPjqZ3hB8/mpHGhmGcJtqTCOqUUvVNT0TEHegaXWOC4iC4h8PaCZqcMzCSlbtKTVuBYRinhfYkgsUi8hjgIyJTgY+BL50bVgfqMV6XCJTjcttZAyOpt9pYlnnqcxkZhmE4W3sSwSNAMZAB/AL4BvidM4PqUD3HQU0JlGY67JCjEkMJ8HJn4dZChx3TMAzDWY6ZCOwLx2xVSr2qlLpcKXWZ/fuuUTUEkDhRP2b95LBDerhZmJoUxbcZ+zhYb3XYcQ3DMJzhmIlAKWUFtotIjw6Kp+OF9oaQRMhc6NDDXp6aQFVdI/M3Fzj0uIZhGI7WnqqhEGCziPwgIvOavpwdWIcRgb5TYfcSaKh12GHH9g6lZ5gvc9bkOuyYhmEYztCeSeced3oUrtZvKqx5Vfce6nO2Qw4pIlw+Mp6/f7eDPSUHSAz3c8hxDcMwHO24JQKl1GJgGxBg/9pq39Z1JJ4Bbp6w0/HVQx5uwpvL9zj0uIZhGI7UnpHFVwCrgcuBK4BVInKZswPrUJ5+0HMCZH7v0MNGBXpz4dBY5qTlsL+mwaHHNgzDcJT2tBH8FhillLpRKXUDMJquWF3UbyqU7IDyvQ497K1n9Kam3sr7q7MdelzDMAxHaU8isCililo9L23P+0TkdREpEpFNR3n9WhHZKCIZIrJcRIa2M2bn6DtVPzq491BSbCAT+obx5vLd1DfaHHpswzAMR2hPIpgvIgtEZLaIzAa+Br5tx/veBGYc4/XdwJlKqRTgj8Ar7Tim84T309NNODgRANw6sTeFlXV8tTHf4cc2DMM4Ve1pLH4YPfPoEPvXK0qpX7fjfUuAsmO8vlwpVW5/uhI9q6nriEDfKZC1GBrrHHroyf0j6Bfpz2s/76YrjcUzDKNraE8VTy/gG6XUA0qpB9AlhEQHx3ELxyhliMjtIpImImnFxU6c1bPvVGg4ANkrHXpYEeGWM3qxpaCSZZmlDj22YRjGqWpP1dDHQOvKbat9m0OIyFnoRPDI0fZRSr2ilEpVSqVGREQ46tRH6jVJdyN1cO8hgIuGxxEZ4MWLix03p5FhGIYjtCcRuLeehtr+vacjTi4iQ4DXgFlKKdd/VPby12sUOHg8AYC3hxu3TezNssxS1meXN2+vrmtkyvOLzUylhmG4THsSQbGIzGx6IiKzgFO+a9nnL5oLXK+U2nGqx3OYvlOgeCvsd/zUENeM6UGQjwePfprBB6uzsdkU6dkVZBZVs2hb0fEPYBiG4QTtSQR3AI+JSLaI5KCrcH5xvDeJyAfACmCAiOSKyC0icoeI3GHf5fdAGPB/IpIuImkn+TM4Vj/ndCMF8PNy588Xp1DbaOU3czP4ZF0uG3IrANhSUOnw8xmGYbTHcecaUkrtAsaKiL/9eXV7DqyUuvo4r98K3NqeY3WoiIEQGA87v4eRsx1++POHxHBeSjTj//oji7cX02DVzS9bCipRSiEiDj+nYRjGsbSn19B9IhIIHAD+KSLrRGSa80NzERHoPw12LYKGg046hTChbzjLdpWwIbcCDzehoqaBgv2Om/3UMAyjvdpTNXSzUqoSmIauyrke+KtTo3K1QTN1N9Kdju891GRiv3AqahoorKxj+uBoALbkm+ohwzA6XnsSQVNdxXnA20qpza22dU2JE8E3DLZ87rRTjO8T3vz9laMSEIHNJhEYhuEC7UkEa0XkO3QiWCAiARw6rqDrcXOHQRfC9vlOqx6KCPBiYHQAbhZhVGIoiWF+bCnY75RzGYZhHEt7EsEtwKPoGUhr0GMIbnJqVJ1B0kW6emjHAqed4uYJvbhuTA+8PdxIiglkU55uMG602iiqMu0FhmF0jPbMNWRTSq1TSlXYn5cqpTY6PTJXS5wIAbGw7m2nneKKUQn8YVYyAGf2jyCv4iArdpXyx6+2cNazP1Fa7dg5jwzDMNrSnhJB9+TmDiNugF0/Qtlup59u5rBYwvw8+cu323h3VTYH6q28v8qsYWAYhvOZRHAsI27Q3UnXveX0U3l7uHHd2J5k5O3Hy93CiB7BvL1yL3WNVqef2zCM7q1diUBEzhCRm+zfR9hnJO36guKg/7mw/l1orD/+/qfo+nE9CfBy5+6z+nL/lP4UV9Xx1YYCp5/XMIzurT0Dyp5ATyvxG/smD+BdZwbVqaTeBAeKYdtXTj9VuL8XKx87h7sm92Fiv3D6R/nz2lKzhoFhGM7VnhLBxcBM9MhilFL5QIAzg+pU+pytVy5b+0aHnM7Pyx0RaV7DYGtBJSuyXD8xq2EYXVd7EkG90h9JFYCI+Dk3pE7G4qbnHNq9BEo6di2BWcPiCPPz5OXFWdQ2mLYCwzCcoz2JYI6IvAwEi8htwELgVeeG1ckMvx4sHrDivx16Wm8PN26akMjiHcUMe+o7XlhkFrUxDMPx2jP76N9FZCpQCQwAfq+Uct4kPJ2RfySMvBHWvgkT7oPQjmsrv2tyXwbHBvH+6myeXbAdiwh3Tu7TYec3DKPra09jsR/wo30R+1cBHxHxcHpknc2kh3Wp4KeOnW/PYhHOGhjJS9eN5MKhsTwzfxu7its1E7hhGEa7tKdqaAngJSJxwHz07KNvOjOoTikgGkbfChlzOmSA2eHcLMKj5w4EYOGWwg4/v2EYXVe7Zh+1zzF0CfCiUupyYPBx3yTyuogUicimo7wuIvJvEckUkY0iMuLEQneBsXeBWGDVyy45fVywD0kxgSzcahKBYRiO065EICLjgGuBr+3b3NrxvjeBGcd4/Vygn/3rduDFdhzTtQJjIflSWP8O1LpmptApSVGs3Vve5jxESimsNoXNpnh35V4+Wev4dZcNw+h62pMI7kcPJvtMKbVZRHoDi473JqXUEqDsGLvMQq9voJRSK9G9kmLaEY9rjb0L6qth3TsuOf3UQVHYFCzaXnzEa7e9ncawP3zH9H8u4Xefb+IP8zY3L4VpGIZxNO2ZfXSxUmqmUuoZ+/MspdS9Djh3HJDT6nmufdsRROR2EUkTkbTi4iNvgB0qdhjEj9Y9iFww4jc5LpDYIG/+t3Q3tQ1WluwoJj2ngq0FlSzcWkS/KH+8Pdy4ZEQcVXWNbMip6PAYDcM4vRy3+6iIpAKPAYmt91dKDXFeWIdSSr0CvAKQmprq+vkWRt4IX9wN2Suh57gOPbWI8NSsZG59O41LX1zO5vxK/DzdGN4jBB8PN16fPYpgX0/21zTw+fo8ft5ZQmpiaIfGaBjG6aU9VUPvoev7LwUubPV1qvKAhFbP4+3bOr/BF4NngFPXKjiWKUlR3DaxF5vzK7lkeBw+nu4szSzhspHxBPt6AhDk68GQ+GB+3uniEpRhGJ1eexJBsVJqnlJqt1Jqb9OXA849D7jB3ntoLLBfKXV6TLXp6Qcpl8Hmz1zWaPybcwfx7X0Tee6Kobx8/UhG9wrl9km9D9lnUr9wNuTuJ6u4mgN1jUccI6u4mg9XmzUPDKO7a08ieEJEXhORq0Xkkqav471JRD4AVgADRCRXRG4RkTtE5A77Lt8AWUAmeqDaXSf7Q7jEiBug8SBkfOKS01sswqCYQESEkT1DmPOLcSSE+h6yz8T+EVhtirOfW8zM/y5FKcUna3O5+c011DfaePTTDB6dm8H+mgaX/AyGYXQOx20jQK9PPBA9/XRTFxQFzD3Wm5RSVx/ndQXc3Y7zd06xwyEqRS9aM+oWV0fTptSeIfztsiGs2V3Gx2tz2VV8gLdX7GFj7n7ueHctq/foTl1bCioZ1yfMxdEahuEq7UkEo5RSA5weyelGRDcaf/MQ5Kfr3kSdjIhwRWoC4/uE8fHaXD5Oy2Fj7n4CvNz5cVsRIb4elNc0mERgGN1ce6qGlotIktMjOR2lXAbu3pD2uqsjOab4EF/6RvrzxrI9ALx43UgGRgfw5MzBRAZ4sSW/0rUBGobhUu1JBGOBdBHZbp8KIkNENjo7sNOCTwgMvRo2fAjVRa6O5pjO7B9BvdVGbJA3E/qGMf/+ScwaFsegmEC2FJhEYBjdWXsSwQz0NBDT0N1GL8Ax3Ue7hvH3gLXeZfMPtdfkAREAnD0oEhFp3p4UG0hmURX1jWYEsmF0V+0ZWby3ra+OCO60ENYHBl0Aa16F2s77yXpMrzCuTE3ghnGJh2xPigmkwarYWVR1zPe/sCiTD0xXU8PoktpTIjCOZ+KDejzB8v+4OpKj8nS38MxlQ+gfdehy00mxgQDMS89nU97+NruSVtTU88+FO3jT3sbQ5FjLZ85Zk8Otb6WdeuCGYTidSQSOEDscki6CFS90+raCwyWG+RER4MXLS7K44D9LGfrUd9z7wXrqGltu8t9k7GsuNdTU64FpH67OJvmJBUddPvOTdbks3Fpo1lo2jNNAe7qPGu1x9uOw9UtY/Dc4/++ujqbd3CzCoocms7v4AHkVNazdW86rP++mvKae12ePwsPNwufpebhZBKtNsSW/kuyyGh6dm0G4vxfPLtgOwN1n9W0+Zl2jlXT7ZHe55TX0jQxo69SGYXQSpkTgKOF99WjjtW9AWZarozkh/l7upMQHMSM5ht+en8QfL0rm550lfLkhn9zyGlbvLuPaMT0ASM+p4K/fbmNkzxB+/vVZnDMwkpcX78Jqa5kLcGPu/ubG55yygy75mQzDaD+TCBzpzEf0usaL/uzqSE7JtaN7MDA6gP/7aRdPztuCh5tw28TeRAd688ayPRRV1XHzhF74eLoxc1gslbWNbM5vmXNp9e6WZShyymtc8SMYhnECTCJwpMAYGHsnZHwMeWtdHc1Js1iEOyf3IbOomoVbC3n03EEkhPqSEh9EXsVB/L3cOWdQJEDziORlmaXN71+1u4x+kf54ulvIKTOJwDA6O5MIHO2MX4F/NHz1ANhO34bSC4bEkhwXyHkp0dw8IRGAIXFBAMxIjsbbQ69WGhngTf8of5bvKgEgr+Ig6/aWM6Z3KPEhPuSUHWR5Zgm//mQDNpvrl5IwDONIJhE4mncgzPgzFKTDmv+5OpqT5mYRPr9rAi9cM6J5ANroXnqBm8tGxh+y7/g+4azZU8Zfvt3KWc/+RH2jjZlD4+gR6ktOeQ1vrdjDnLRcljhhbYSa+kbyKkw7hGGcCpMInGHwJdDnbFj4JJTucnU0J83dzXLIKOQxvcNY/ujZjO196AR14/uEUdtg4+XFWVw4NJafHp7M6F6hJIT4kl1Ww4pdutro3ZVtj0N8feluXvxpF2UH6k84xmcXbGfGP5dQ3cZ6C4ZhtI9JBM4gAjP/C27u8NkdYO06N6nYYJ8jtk3sF8HVo3vwxk2jeO6Koc37JIT6UFXbSGVtI/2j/PlhWxEvLMrkyXmbybSPZF6yo5invtrCM/O3MeGvP/JFeh61DVZ2Flah2lgT2npY9dKyzBKqahv5ckP+Cf0cB+oazRgHw7CTtv7ZOrPU1FSVlnaajFjN+AQ+vQVSb4Hzn9MJohv5NqOAO99bB8BX95zBrBeWYbUpPNz0mISpSVFsyqvEy93Cv68ezlNfbWH17jJ8PNw42GDl8pHx/PmSFNxE+O3nGXy6No9Gm41Xb0jlnEFRlB+oZ/gfvwdgSHwQ8355Rrtju+KlFcQEe/Ovq4Y75Wc3jM5GRNYqpVLbes2pA8pEZAbwL8ANeE0p9dfDXu8BvAUE2/d5VCn1jTNj6lApl8G+jbDsXxCcoBuSu5GmFdMGxwaSHBfEnF+Mxd/Lg4gAL179OYs5a3Ior6nnw9vHkRwXxLu3jOG/izIpra7Dy92N15ftZldxNf2jAvhwTQ4XD49jzZ4y/rFwB2cPjGTt3nIAzk+J4euMAjbl7SfZ3qB9LAfqGknbW0ZClW+bryul+CI9n+mDo/HxdHPcBTGMTsppiUBE3IAXgKlALrBGROYppba02u13wByl1Iv2NQ++ARKdFZNLnPMk7M/V7QVBCTo5dBMJob6IwBl9wwEY2TO0+bVHZgzkV1P6U1RVS3yIviF7ult4YGr/5n1S4gP567fbWJddwRWp8Txz6RA+Tsvl159uZPGOYtbsKcPTzcITM5NYsqOY38zN4OM7xjX3aDqaDbkV2BRkl9VQ22A9Yv8tBZXc/1E6f70khatG93DU5TCMTsuZbQSjgUylVJZSqh74EJh12D4KCLR/HwScWEXv6cBigYtehJ4T4PM7Ycs8V0fUYYJ8PHj75tHcNblvm697uluak0BbLh4ez+KHz+Ktm0fzp4tTEBEuGh5HbJA3f/12G4u2F5ESH0RkgDfPXzmMTfn7eWxuxnHjWp9dAYBSkFV84IjXM4uqAdhpfzSMrs6ZiSAOyGn1PNe+rbUngetEJBddGrinrQOJyO0ikiYiacXFju+C6HTuXnDVexA9BOZcryen6yYm9osgyNfjpN/v7eHGmf0j8HDTf6qe7haempVMTlkNOwqrGZWoSxlTk6K4bWJv5q7Po7iqjqraBpZnlhxyrIP1Vmw2xbq95Xh76ONlFh95s99dopPDrjZeM4yuyNW9hq4G3lRKxQPnAe+IyBExKaVeUUqlKqVSIyIiOjxIh/AJgdlfwaCZsOAx2DTX1RGdtqYkRTH//kncMK4n17Squpk+OBqAtXvLeXlxFte8tqr5pq6U4pIXl3PVKytZl13O9MHRWAQyC49ch6GplJBpSgRGN+HMRJAHJLR6Hm/f1totwBwApdQKwBsId2JMruXhA5e+BgljdTVRzhpXR3TaSgj15alZyfQIa6laSo4LxMvdQtqeMhZt19OBf7upAND1/lsLKlm9p4zymgbG9g6jZ5jfMUsEeRUHOVhvupgaXZ8zE8EaoJ+I9BIRT+Aq4PAK8mzgHAARGYROBKdh3c8JaKomCoiB9y6FfZtcHVGX4eXuxtD4YL7bUsjmfL1a3LcZ+wBYsGkfFoFbz+iFRWBs7zD6RPizs/DQRKCUIqu4msgAL92GUGJKBUbX57REoJRqBH4JLAC2onsHbRaRp0Rkpn23B4HbRGQD8AEwW51uAxtOhl843PAFePrDu5eedovZdGapiSFk2ye6mzk0loy8/eSU1TB/8z5GJYbyuwuSWPf4VHqF+9E30p89pQdotLas11xcVceBeitTkqIAUz1kdA9ObSNQSn2jlOqvlOqjlPqTfdvvlVLz7N9vUUpNUEoNVUoNU0p958x4OpWQnnDNHKit0IPOTuMJ6jqTpsbjcH8vHpymu6I+/MkGdhRWMyNZtyEE+3oC0C/SnwarYk9pywypu+ztA+cMjMQiLc8NoytzdWNx9xadrEcc714C3//e1dF0CSN6hCACZ/aPoGeYH7ee0YvN+Xr0clMiaDIgWq+ctm1fZfO2pvaBAdEB9Aj1ZZcpERjdgFmq0tWGXwcFG2DFfyEkEUbf5uqITmtBvh68eO1IkuP08JTfXZDEY+cN4kB9IwHeh3Zj7R8VgIebsCmvkguGxAKQVVyNl7uF2CAf+kb6m6oho1swiaAzmP4XqMiGbx4GN08YeaOrIzqtHf7J32KRI5IA6DEJA6ID2JTXsrra+pwK+kb6Y7EIA6ID+Gl7cZujjw2jKzFVQ52Bmztc/ib0PQe+vBfS3nB1RN1GcmwQm/L3o5Ri+74q1u4tZ9aw2ObXGm2KHW2MNTCMrsQkgs7Cwweueh/6TYOvfgUb55gG5A6QHBdERU0DeRUHeX/VXjzdLFw2MqH5NYBNeZWHvOfz9Xlc/cpK6hrN78foGkwi6EzcveDytyBhNMy9DZ7pBenvuzqqLq3pZr8yq4y56/M4NyWaUD/dqyg+xIdAb3c25bdUHVXVNvDUV1tYkVXK3HWHj480jNOTSQSdjacvXP8ZXPQSRA6EL++Dgo2ujqrLGhgdgJtF+O1nGRyoa2T2+MTm10SE5LigQ9oQXlqsV1KLD/Hh/37KPGQMgmGcrkwi6Iw8/WDY1XDVB+AbBu9eogeeffMwbPgIGutcHWGX4e3hRr9If+qtNv5++VCG9wg55PXkuCC2FVRRVdvA899t55UlWcwaFssTFw4mp+wgX27sehPmGt2P6TXUmfmF6ekofnoGqvdB9kpY/Qr8+LQef9B/mqsj7BIevyCJmnorU+2jiVsbHBtIvdXG9H8sIX9/LRcNi+XJmYMJ8vGgd7gfH67O4eLh8S6I2jAcx5QIOru4kXDtHPjFEng0B66bC14B8OE1sH2+q6PrEib0DW8zCQCk2NsQahttvHnTKP551XCCfT0RES4eHseq3WXkVRxs97kqaxv41UfpFFeZUp3ReZhEcDqxWHQX05u/1aOS51wPJZmujqpL6x3hz6s3pPLtfROZPCDykNcuGq6X1/h8vW40zimr4emvtlDbcPTeRMt2lvDZ+jyWHbZWgmG4kkkEpyPvILj6IxALLP+Xq6Pp8qYmRREV6H3E9oRQX0YlhvDZ+jyUUryxbA+vLd3Nswu2H/VYTT2QcstrjrqPYXQ0kwhOVwFRenqK9A+g0jRYusolI+LJLKpm7d5yFmzeh4eb8Pqy3fz2swx+M3fjEesZNI1JyC1vf3WSYTibSQSns/H3gLLBoj/rBXiNDjdrWCyB3u787vNN5FUc5PELkugT4c9Ha3L4YHUO323Z17yvUorNzSUCkwiMzsMkgtNZSCKMvRPWvwOf3wXbvobyPfq1go2Qu9aV0XULvp7uXJGawLZ9VbhZhAuHxDL/vols/eMMogK9+CajoHnfoqo6SqrrEeGEGpi7jO3z4b0rzIeWTsip3UdFZAbwL8ANeE0p9dc29rkCvYi9AjYopa5xZkxdzrSn9fQUS56FDe/rdoPY4ZC3FsQNpv0R3L3B4q6rkixm8jRHu35cT/63bDfjeocRYh+VDHoN5TlpOdTUN+Lr6d48MG1kjxA25u7HZlNYLOKqsDverh9h5wKoqwLvQFdHY7TitEQgIm7AC8BUIBdYIyLzlFJbWu3TD/gNMEEpVS4ikW0fzTgqETj7d5B6M1QXwpZ5sPVLmPgQFG6CBY+17LtxDlz+Bviby+xIPcP8eObSIQyKPvTmdm5yDG+v2MtP24s5LyWGTXmViOjG57S95RRX17XZCN1lVRfqxwPFJhF0Ms4sEYwGMpVSWQAi8iEwC9jSap/bgBeUUuUASimzZuPJCozVX7HDYcoTepvNCju/g5BekL8OvnpAjz+Y/bWe18hwmCtSE47YNrpXKGF+nnyTUcB5KTFk5FXQK8yP/lF6QZzc8pr2J4LKfNj+LYy6xZFhd6zmRFACYX1cG4txCGe2EcQBOa2e59q3tdYf6C8iy0Rkpb0q6QgicruIpIlIWnFx117b3qEsbjDgXD1n0bBr4JKXIXeNTgimntbp3CzCtMFRLNpWREVNPcsySxnfN4z4EB/gBBuMN86Brx+AA6VOirYDtC4RGJ2KqxuL3YF+wGTgauBVEQk+fCel1CtKqVSlVGpERETHRtiVJM2CMx+B9Hdh1cuujqZbODc5hgP1Vp76agsHG6yclxJD3Mkkgjrd7XRfYR5VtQ3OCNX5qkwi6KycWTWUB7QuL8fbt7WWC6xSSjUAu0VkBzoxrHFiXN3bmY/CPnvbgacvRKfAgt+CXwSMvQt6jHF1hF3KuD5hBPl4MHddHmF+nozpFYabRQjz8zyxRFCrE8Hv3l9K/BA3npw52EkRO0ldNTTo9aA5YEZVdzbOLBGsAfqJSC8R8QSuAuYdts/n6NIAIhKOrirKcmJMhsWiq4gSRsO8e+CVyVCaCVk/wevT4Ofn9LiEFyeY6a8dwMPNwpRBeh6j6cnRuNl7CcWF+Bwyuri2wcpTX24hq/goayTX6VXSrDVlh0yLfdpoqhYCUyLohJyWCJRSjcAvgQXAVmCOUmqziDwlIjPtuy0ASkVkC7AIeFgpdRpXgp4mvAJg9jdw0Ysw+hdw10p4YAskXwo/PAWLn9FrKL91ge77bTNz7p+KpqUvZw2Nbd42KDqQlVml/LBV3yD//cNOXl+2m7dX7G3ep7bBSk6ZPVnYE0Ew1WQWV6NOtzYekwg6NaeOI1BKfQN8c9i237f6XgEP2L+MjmSx6AbkYa2GbVzyGvQcr3sZhfeDty+CD66EiIFwwxcQEH3Uwx2iqlBXNVlc3QTVOUzqH8GyR88mLtinedtj5w9i675K7nx3HRcMjeGL9HwsAou2F/GESkJE+M+PO3l1yW6+uW8ife1tBMFSTUVNA6UH6gn3P416fjUlAq8gkwg6IfOfarSwWGDUrXqG0+AecNcKuPhlKMuC7x6HygKYdy/8ZyR8ckvbPY/2bYJ/DIZ1bx35mlKw4ztY+CQcrHD2T9OptE4CAEE+Hrxz8xjOHxLDgk37iAzw4sFpA9hbWkNWia5LX7iliHqrjd9/sQnVnAj0a5lF1WzIqWgpMXR2TQ3FUYNNG0EnZBamMY7O3QuGXgWlu2DJ3/TI0PoD+p950yfQ5yw9Wrm1hU+ArQE2fQqpN7Vst1nhw2thx7f6ecYncMXbEDfiyPMqBUVbIDJJD5jrooJ8PfjHlcOobbBitSnKDtTz7ILtLNpWhI+HG9sLq0iKCWT5rlJqwirwA4LQbQhbCyr5x/c7GNcnjJevT3XtD9Ie1YV6dHtEf9j6laujMQ5jSgTG8Z3xK11d5O4Nt34Pt3wPPcbD/Mcg/X09vcVzg+DNCyBzIQT1gL3LDu3znvGJTgKTfwM3zQcE3rkYirYdeb7t38CL43Xf+eMp3wM7FzrqJ3UJbw83/LzcSQj1pV+kPz9sLeKn7br65B9XDiMq0AtbrW4gjvOqxc/TjXdW7KWytpEtBZWuDL39qgvBLxL8o6CmVH8wMDoNkwiM4/P0hdt/gl+u1t1NLRa46AW9lObnd+qlM8P7QvF2CO0Dl76mZ0Vt+vTfWA+LnoboITDp19BzHMz+Upc43rkI9iw99HxNCeD738PO7+HtWbpXU1t+eEq3Y9Qd1tum/oAuwZxmjarnD4lhRVYpz3+/g7hgH/pH+TOyZwieVl0lFOleQ59I/+bqo5yyg1TXNXZYfHe/v44/f7P1xN9YXainTvcNBxTUlJ1cAGv+Bz/88eTeaxyVSQRG+/gEg6dfy/PQ3nDPOrjtR7h9Mdz4JTywVbcrJIyGoARY/Sp8/wS8ca7uhTTlyZYG5JBEuP4znQzePF8nE6V075gd83WJo3ofvHcZ7F4C716qSx8AS/+hq5lsNshaDLZG2Lv80Hh//JMucXz7yIn1eqqrhoPlp3ChTs0vz+rLBUNiKKmu46yBEYgII+P88EIPIguxHKBvpD8AkQG6sXj7vqqTPl+D1YbN1r5kmVVczdcbC1i+6yTq+KsKdWnAL1w/P9kG4w0ftvwdtFa8Q1dHGifFJALj5InoNZVjh+nnbu76xi4CKZdDQTos/w8oK0x9Cvqcfej7owbDncth2HW6eunHp2HzZ9BYq+dLmvRr3QZx/yboMQ6+fhDK98LPz8O2r2DzXKix35R2L245rrUBNn4EvmGw+mV4/wr9PoDGOtg09+glhU9vgfcud+RVOiHubhb+eeUw/jhrML88qx8AI6NbmvICVXVzIrj7rL4AbNtXic2mjn5DVwq2fcNHK3by5LzNzZtrG6yM/fMPDH3qO67/3yo+WpNNfePRk+actFwA8k5mLYXqQup9Ith90Fc/P9lEULZLly4Or1pa/i+Ye7v+3R/HjsIq9u2vPbnzd1GmsdhwjrMfh3G/1CWJY0197ekHM/8DAvz8d70tMB7iR0OPsS37nf8cvDAG3r2keboF5v9GP4b10yWDJpkLdYK46gPYn6t7Kb00Ee5ZC1s+h28e0p9OEyccGktFNuxYoJOZzeb47q9fP6Qb0i9stbzo7p/1z+nm0bzJ3c3C9eMSm58PCtWPVcoHH2slF8dXMyH2bQYm/4e/L3Bna0ElV726kogAL164ZgR//XYbX23Mx9/LneeuGMpg6w748GoybLfzbv1k7jqrD5EB3mzfV0XpgXom9Y8gu/QAj3yaQVFlHfec0++I0BusNj5dl4sIlNc0NE+t3S7WRjhQzNpST/6wJpv5npxcIjhYodsXQPc8Cohqea00S5cMK7KPO6HdbW+nkRwXxAvXtNFRoZsyJQLDOSwW3YbQnvUPLBa48D9wzcd6mosZfznyJhwxAJJm6lHQMcOg7xQ4UKTbJIZeCYUZLd0S09/T4xj6TYUxt+v2iLr9uhSxY77e5/B2CYD17wFKl0iqnLD8Z+ZCfeNvUrJTD9pb8cKR++5Z2tzF1svePpCrwvFsqCQmdwFDy+bjNecaUqI8mJeez+rdZXy9sYDvNu/jlSW7iAzwoqS6joc+3oh11yIABqjdACzapif5bVo/+U8XJbPoocn0j/JnQ25Fcwg5ZTX88ast1NQ38tXGfIqr6rhgiB4Ud8xSwfL/wJwbW55X7wMU22sCKLTpmVdPqgtp2a6W7w///TS9VrLzmIdosNrIKas5peq0rsgkAqNzsFig/zSdBJJmtr3PxIfA4qGX6Bxyld7WezL0mqy/37VId3Xd/i0MubLlU3bsCN2msXFOy41472GJwGaF9e+CT4h+XroLh2is15+IrQ3602plXku1VJl9NpU1r+nzN7VlVObrHlhrXtPP7aOKc1UEgoKcleDhB7lp3Gt7m8raRiIDvPD2sPDL99fj4Wbh5etTefqiZLYWVLJhyRcAnB1cRFywDwu36kSwOb+SQG934kN8EBH6RQaQWdTS6P7VxgL+t3Q3v/5kI3/6eitD44O4bkwPHcuxVlhLe10n3aZqmv16irGNlf5U4I8VC1QVHP39R1PaavaZqpYlQKmrahmwVpp5zEMUVNRiU7Cn5AANVhs7C6vYW3rgiP1qG6wd0whfU6bH57iYSQTG6SNmCDycCSmXwcDzod90GHatXoMhpJcew/DVr8DdB8bf2/I+ERh4AWQvB2sdRA6GnDX6Jt0keyVU5sIZ9kHux7mhtNsHV8K8X+okoKy6tNHUY6YiWz/uz9HVVc/01G0kmT8ACirs7Rr2Cef8o+xVHtmroNckGHIFwysW4kkDd5zZhytTE6i32rhmTA8iAryYEVHOZSkhJNu2YcNCbN0upgwM5+edxdQ2WNmct5/kuCDEPlajT4Qf2WU11Dbo+vedRToBfbWxgPKaBv58SQo9wnQd/1FLBKW7dIKzNbYsm7pfz0a/6UAA/l6ebLcl0JhzEsuoHlIiaHXzLGuVII7ze8uxz+/UaFPsLa3hjnfXcu+H6Ufs99hnGcx+ffWhG7N+0r3RHOnbX8Pr0w/t0LD5M6jt2PmkTCIwTi8+wfrR0xeunQPxI3Uj9eVv6uqG3Yth0kOH1h8DDLKXMjz9YeID0HgQlv0TXj1b37CyftLLfI64Xo+XaH1zAT1iet3bx46tfA/sXdHy3NoAe5bp9ovWJYxK+yS8+3PAzVO3iaS9rts+Vr+qu72CLhlAc4lg3Eh7nXbDAYgcBMmX4dVYxd+GFXNj3pM87PkpM4fGctfkvpC/Hl4cx7N1T+FJI5ZBFyD11ZyXUE9tg41F24rYuq+KwbEtK4X1ifTXn5ZLW0Yvj+sdxiXD4/jNuQMZHBtEZIA37hY56prLjdu/a/7+YMHWQ37efBXGTRMSWW0bAHlpYG2guq6RRmsbDdT1B6DhsAbd0l0QEAPIoZ+i7de20cP/+Img1UjslVml7Co+wIacCgr2H/rzrN5dxpaCypY5ncqydDfmtnosnYryPTrhN5VQ9+fBx7P130MHMonA6Bpih8GsF/Qn/7F3Hvl63Eg90K3fVOh9lt626E96bef17+kEEjtCVw2F9tY3l6pCKLQvqLf0eT1b67avjzy2UvD+lfCvofDGDCjYoLeX7NAlkKp8XZ3TpCkRVORAYByc9zdd3TXpYT0Qb6f9ZmqvUmluHA/u0XKMqMG6WswnhIvynsNt6+f4717Av68eTkSAl/65AMleoUf0jr4dgBGeucQF+/D4F5upb7SRHBfUfMim3kiZRdXYbIrMomoGxgTw/JXDuHVibwDcfn6We/2+by4RqMY6MlYuxGa/mZdt+Jp8pVu3C3fZZ6/dn0u9uz/V+HJ5agK5AUNxtx5kz+aVnPHMjzz99VZ9DZf8XVffHayAl86AL+469DqX7YLw/nqp1UNKBDoRfF83GFVy7ESQW36QpmWiP1rTsm7Wd5tbJsXbX9NAbvlBauqtlB2wlxqzV+nHplLaUby0eBdLd55A+0fT1BsbPrSfXPfMonBz2/s7iUkERtcx5HK46r22l+G0WPSo6Av/pRuxY4frxBA7XP8T5qbpGyvYE0EmfHa7HuNgs+qV3QC+vO/IVcLy1+tG6JGz9U0342O9vSkhgO6yKvaG86Z/9v05EJygq7mmPQ3Dr9fb66v1KNzKwxJBSM+W40UOAndPXdKpKgBEx9zUrXLfJj3B2+hf6LjiU0EseBRv5vELBlFSXQdwaIkgwh8RnQjy9+sbYb/IgJZzHiiBxc9wd8ObWEq2wsaPqX1+KCnzL2X9Dx9Bw0GCC1fxs/s49qkQDuTbk+j+PMrdIwnw0u0R5593EQDvfzyHipoGPlufh/WHP8GPf4S5t8HrM/Qn8Lx1+v2rX9UDB0t3QVgfrP7RZO3e1dLgW5pFsYSSYe2JVBccMrhw7d5y7n5/HQ32RJVTXkNMkA9xwT5k5O3H091CYpgvCza3tDlsLmiplslpqgLL1dVExXlZLQsD5a3V42TspQarTbHr+1f54Isvjz077HePw7eP6vc1tW1s+UKXgipNIjAM5wqIBm/7J+BrP9WD30bOhv32+vveZ+rXwvrqT5lZP8HBMj26uSJbt0ccrICPb9TjEZps+BDcvPRYib5T9E3fZtOJwN2+JnH5bv0p3uLeUuVTkXPop/yQntBzAiB6jqe6St0+UFelG8n97bO/Wtx1l1mA1JshKgXOuF+XPpo+sRZuguhkXdo4/znw8NGfpvdlMH1wNBP7hRPg7U6vcP/m03t7uBEf4kNmUTU77Y3G/aL84fO79BoVGz8CWyMNFi8eK/0tzL2V/MYgrEo4uGc1BVuW40k9gUlT2eeRgFfFLj5Oy2HP7h3sbQyhf3QAIsLwlGTKPaI5W9awNPSPrLZdg9vSZ/nOcwpbA8ZB8VadjCv26uqhVS/pdTJqK1Chvdla7UddWS5f/99DlP7nbBoKMthljWK3itE/SKu2hBd/yuTrjQWk7dGDBHPKakgI9aGPvfQzLD6Y81JiWLW7jHL7p/8t+S3TdmTbq5Ia9uoSwZ7dO5n2jyV6zMbGObp60T4AcV/ubv7q9hLn7v+AjMPWjPjXwp28tXyPftLUe612v/6dDThPJ//MhS1/GyU7Dv0bczKTCIzuyS8MvPz1J2qLu75hx4/Wr4X10VNkWOy9jpY8qx9HzoaL/g/2/Axf3K23NdbrCfgGnq+TTPJl+pN89gq9sE/0EN3FFXSCCYjVrzfW6W6VQT0OCYspf4Dpf4aYofp5Zb5OBF4BLT2awvrq0gDoKrE7l0J/+3LfJTt1EircAlHJhx47OgXy1yFK8cK1I5h75/jmhXKa9I3wJ7OomsxCeyLwq9P14ouf0TfjuFR+7nU/kZRRNehKZlQ9xi4Vi0/pZvK36HrupFFnUx/cl8i6bJ6ct4mAun1k1gYxILqldBE0cBJjLVuJa8zhA5nOX9Rs7qy8kZnFd7LtvE9h8mP6d7AvA0p30RCUCMD9ixrZUOFDT8/9XOf+I2Gla/EoymC3LZoC93h9cHs7QWl1XfOcTT9t1z2lcssPkhDiS98InQhSE0M4e2AkVptizR7diL85v5JgXw8s2AjY9C6UZeFeots7ennup2B/LdsLq1oGKdrbk+rWf4ibKAZJNnPSWqqdahusvLR4F++vyta/9/I9+m/A3vPJ2s/+uyve3pIIbI06GXQQkwiM7s03VI+CTpoFHvZP70037sEX6x5GeWk6KUQPgSFX6G6sGR/rG0DmQj3Iaai9O+uAc8HDV48N2LdR39CbbuphfSAoTtf9N1UPBSccGk/CKBh3FwTaF7GpzNWlAu9A3SjuFairhQ4X3l8/luzQpY+GA7pE0Fq/aboqInc1gd4e9IsKOOIwfe3zGK3LLifc34vgfcsApUsjNaUw/DrKB1zD2XV/57aK2TTiTlngQGJrMyFvHflE0COhBz6xgwiUGqKsRYRJFRIcz3nJMc3nsSRfCqG9kRvnsWnwr3m5bhrnDoknJMCP36T50Bhm/3m2fA4o0vo9wOS65ygIHUNEbE98G/cTaStis01Xl+VKDCOHj+Sg8sS65nWwNvDVxgIabYqEUB8WbS+itsFKUVUd8SG+ze0hoxJDSYoNRATqMj6HTXPZnL+fET1CmOm7ibN2/gneOA9RNjbbehJqLUWwsTKrtKXXV1kWKEXozk8A6GUpZEH67ubeV2l7yjnYYGVXcTX1xZk6wVnraczXVYeP/FiJLSBWV31V5unSJbS0T9nNXZdLZpFzxj84NRGIyAwR2S4imSLy6DH2u1RElIicBvPpGl3OxS/BJa+0PI8drksKkx5qaTeIGdqSKJqm3t4+HzZ8oAevNU2f4eUPZz4C27/Wxf2YoS1TcIT20Y3DlbnNXSoJOiwRNAmM04/781pKBKBLC+PvOXJ/31A9oVvJDl0tBEeWCPrP0DeZzZ8d9VJMTYrGZlN8u2kf/SL9dVdWnxC46Rs9UnzIFSTFBZGlYlm5u5yLh8fh13M4MVJKr6o0igL0ojrx/XTy+80A3ah79ZTxnNEvvOVEA2bAveshbiSzxydy1oAI/jgrmV/PGMj67Aou/qgQhTTHuq4hnny3ON6/fSzTxg63H0T4b8yf+V3DTeyIuZAx/eP4bcPNuO1dCvN/w9x1uQyL9uThQRXsKKxm9W79iT8h1IcZydH84szejO8bhq+nO73DfBm/8zlsPzzFruIDDI4N5FKP5dTj0dww/a1tDBbVyJCQelbuKmmphivbDQUbCK7exTKVggUbMXV7WLxDl0aW7NSPjTZFYdam5kuwZtn3AGyo8GZbfSS1hTuoLc2hKmIYys1Tlyo/ng07v6eytoFff7KRT9Yevuy7YzgtEYiIG/ACcC6QBFwtIklt7BcA3AesclYshnFCPH3hynf0aOamRBA/quX10F4QMQg2fqjrelMuP2SKCCbcByPtazHEjYTEibr6KXaYLhFU5rd8mjy8RNAkMBbdTbKpasjeqDvien3MtoT311VD+zbprrCHlxy8A3WvqS1f6OqjuirdO+fzu5tH+o7uFcrbN48mwNudUYkhuitr78m6NDP9T+DpR3JcEBuemMaOp8/l+SuGEdZHf34LlWrd8woI6TkEgCkNP+lzB8Ud9XInxwXxxk2jCfHz5NIRcbx03UiKai0UWiL0J2SvIFaV+dM3wh93N4u9CykQn8qsiam8a51K/z69Gd0rlM/UJNZFXwFrXqU4dxePB81n5tqb6Cn7eP57XdWSEOpLqJ8nvzl3EF7uugH/nPBywqxFWMp342erZkiEhTH1q/jG/RwY90u2+I+j0Fv3nDo7ppHtu7N1ogddItj2NTYsfBQwG4CR3nmsT1sOlfks2VFMbJD+ELE/p+VTvnfhegBuO28s6QdCqdm3g7J9e1iQ50WhVyKsfUMnwk9vYdX6jTTaFFOTIo96HU+FM0sEo4FMpVSWUqoe+BCY1cZ+fwSeAcwsUEbnk3iGHryVfMmh2wecqxuDrfUt1UJNROD85+HOFRCVpBffeTRH35gD4/R78tfrm3XgUW6Qbh56PqTKXD09hteR1ThHCO+n65l3L9HtCB4+R+4z+GL9CXfvMlj7pq6D3/ABPNsX/j4A1vyP8X3DWfu7qdybbG/H6HPOEYcJ8vHA013fPqL7j27eHp00Xn8TGANJFyE59nEVQfHHjx8QEWYkR3PX5L5sbbDf8KMGs72wioFNbQxNiWDAuUxNiuJXU/pz9egeBPt6Mq53GM/v143+093WMGS/HpNxW9hG0nMq8HK30CfC//DTcpb7xubvh3pkc4Z1NZ6qjg9qx2Kd+jR/CHii+Xc1OrSWgFpdl68QsndtonzHUnZZEmmIHAqe/lwQtIe7su6m5ouH2LavimvH9sTT3YK1eIfuzQUMlj0oNy+umJDMpLFjCJVqYqScgIgeLK2ydwyYcD/YrCT+/AChvh4MSwhp13U8Uc5MBHFATqvnufZtzURkBJCglGqjc/Yh+90uImkiklZcbNY7NTqQp6+eYjth9KHbB5ynHyOTdNvB4SwWnQRaHwdabvzbvtY3tNYlicM1tSe0LhEcS8QA3cspZyWM+UXb+/SfobumfnkvLP+vTnJ3Loczf6176nz9IGz+HE93C+5bvwBEL116DBb/MErdIrAhRA0Y0/LCtKf1KO/WP3c7nZcSQ5bS7SS14UkUVtYxMMaeCKIGwwX/hFG34WYR7pvSj/gQfX1nDYtlaXkImfTgTs/5eFRkgbhxnf86tj41g7WPTyXUz/OI8w2qXtU8/uHCiCJ8tn5KtU8sqxv7sq+yltzyg3iH6dLbIP9K4kXfh2rCh+BfvQevfetZ1dCXHhH+EJnEiIoFBEoNlbtWIwLTkqIYEBWAX1UWKmYI1fjiKY1IQBSIEN83BQBBMWnUUD7xupj/+N2LOucJrOc8Sb+adH4Rv/eIxn1HcVljsYhYgOeBB4+3r1LqFaVUqlIqNSIiwvnBGcbxNFX5jL/nxJbTjEpqGU/QeinPtgTaexjVVravRBA1WD9OuE+vPd0WL39d7VWRoz/tn/EARA6Esx6D6+fqhPfZL3SPp7Vv6pJPU8P1MXj3nkB9+GCkqXsu6Gqv6U/DgPPbHttxDBEBXjqxAXmeuvF+QLQ9GYroa+d9ZHKcMTgGTzcLXzWmEqmKdalrwr2wbyM+VXvw9zpsxtQNH8F3jxNUnMZX1nHkqnAmWzZA1k+U9Z6JwsKekgPsq6wlODwa3DwJbihmfJiuFlqukgmVanw5yGprfxLD/CBqMIKiEQvRFPPyJb3oFxXAwCh/Iutz2O/Xixybvb3E3z4CvqmDAuAT2oNZ06bwXOlY0nP3kxZ6PgUqlMsOfnxC1/BEODMR5AGtK0Dj7duaBADJwE8isgcYC8wzDcbGacFigdlfwbBrTux9ob3hsXx4cLseSXwsgfG68fdgmW6QPp5eZ+qV5Kb84dj79RirV5Ebe3dLGwjoqqQr3tGPb12op/K2j0g+Hr9L/4P3TfOOfGHUrXD1yU3LEDvkHPJVKC/s0VVBA6OPnwyDfD2YPCCC+VZ7Ca7nhJak+OktsPSfLYPubDaY/ygs/zdirSfddzxbVS8iS1aCsuI9/EoAvt1UgNWmSAj116W4ynzOia6lQvkxt6ClAXytrZ9OBPbOASXDfgnAtFA9aGx4uJVADvB9YQC56rBEEJKInosdCIzlgiExeHtY+HRdLu+mFfKm7XzCilfpgY9O4MxEsAboJyK9RMQTuApo/ktRSu1XSoUrpRKVUonASmCmUso5P6lhdBYe3u0rRaRcrmdZnfkf/Sn/eER0j6f2HHvwRTDjz0fuGxAFM56B2grd+Nw6URyLd6Aem+FAZ50xgfti3mPuHk+CfT2aV2Q7nvun9OfCqVNQo26DiQ/q9olJD0N9jZ6YMPMHvWNhhk6yF/wT7l5Dv1HT8Eqw90iKSiay7whG9Ahmzhrd1Tc+xMfe6yufGFVEqXs0e5W+kVe4R5BHOInhvjD0arhlIdHTfqWPZR9hPsZXtyt8le+PNcBeVdaUCDy8W9pRguIJ8PZg+uBoPk7L5csN+fhPuBW8g9u3jvdJcNrCNEqpRhH5JbAAcANeV0ptFpGngDSlVBsfHwzDaBY/EuJf7vjzDrlCN1InjD2xai8H8/Zw43+zR3Hda6uIDvRuniX1eJJiA0mKDQT+3rLx7N/pFe/+kQTr3tJTnjctZtR/BgTG8KupwI6p8P4rOgkDl6cmsC47A0C3QwTFwd7liLs3YQn9mB45DlaDd+9x/K3/UGKC7G0iCfZeZkE9mhNBn/wvsXkG8PittxC/6z1Y9GVLIgBdWjxQ3Dxw8NIR8XyRnk98iA+3nZMCqT/ofZzAqSuUKaW+Ab45bNvvj7LvZGfGYhhGO4noT9KdQKC3B3PvHN/uJHBM7p66h9fKF/Vkb7sXQ/gA3cOpSe/JOmnY228uGBLDH77UE/TFBHvrqib7XFLBA87lvukjwPNXePedyhWJbXQFjhmiBxbWlMHmz7GMuJ6+8VFQkahf92/VHbT/dF2ysv+sE/qGc/nIeC4dGY+3hxuE9z31a3AUZqlKwzA6NXc3B9ZgD79Br6C25G+wd3nL4MDmk3ke0nYT4O3BxcPjWJ9dgYebRSeI0F565PjA8/VOU548+vlihuq5hRY+qecVahpf0jQSPLRXy77j7tZfdm4W4dnLh578z3oC5Jiz5HVCqampKi3NNCMYhnGSPr21ZYbYK9+DQRccc/f6RhsNVht+h/c4ao/sVfDmeXruoIQxcEvLeg0UbdM9ozqo+k1E1iql2uyMY0oEhmF0L5e8qhvhsxbp2WKPw9Pd0jx47oT1GAOP7NW9v1rPNAu6224nYRKBYRjdiwj0m6K/OoKXvx5d3omZ2UcNwzC6OZMIDMMwujmTCAzDMLo5kwgMwzC6OZMIDMMwujmTCAzDMLo5kwgMwzC6OZMIDMMwurnTbooJESkG9p7k28OBEgeG40idNTYT14nprHFB543NxHViTjaunkqpNhe2OO0SwakQkbSjzbXhap01NhPXiemscUHnjc3EdWKcEZepGjIMw+jmTCIwDMPo5rpbInjF1QEcQ2eNzcR1YjprXNB5YzNxnRiHx9Wt2ggMwzCMI3W3EoFhGIZxGJMIDMMwurlukwhEZIaIbBeRTBF51IVxJIjIIhHZIiKbReQ++/YnRSRPRNLtX+e5ILY9IpJhP3+afVuoiHwvIjvtjyEuiGtAq+uSLiKVInK/K66ZiLwuIkUisqnVtjavkWj/tv/NbRQRp61OcpS4nhWRbfZzfyYiwfbtiSJysNV1e6mD4zrq701EfmO/XttFZLqz4jpGbB+1imuPiKTbt3fkNTvaPcJ5f2dKqS7/BbgBu4DegCewAUhyUSwxwAj79wHADiAJeBJ4yMXXaQ8Qfti2vwGP2r9/FHimE/wu9wE9XXHNgEnACGDT8a4RcB7wLSDAWGBVB8c1DXC3f/9Mq7gSW+/nguvV5u/N/n+wAfACetn/Z906MrbDXn8O+L0LrtnR7hFO+zvrLiWC0UCmUipLKVUPfAjMckUgSqkCpdQ6+/dVwFYgzhWxtNMs4C37928BF7kuFADOAXYppU52dPkpUUotAcoO23y0azQLeFtpK4FgEYnpqLiUUt8ppRrtT1cC8c4494nGdQyzgA+VUnVKqd1AJvp/t8NjExEBrgA+cNb5j+YY9win/Z11l0QQB+S0ep5LJ7j5ikgiMBxYZd/0S3vR7nVXVMEACvhORNaKyO32bVFKqQL79/uAKBfE1dpVHPrP6eprBke/Rp3p7+5m9KfGJr1EZL2ILBaRiS6Ip63fW2e6XhOBQqXUzlbbOvyaHXaPcNrfWXdJBJ2OiPgDnwL3K6UqgReBPsAwoABdLO1oZyilRgDnAneLyKTWLypdDnVZf2MR8QRmAh/bN3WGa3YIV1+jtojIb4FG4D37pgKgh1JqOPAA8L6IBHZgSJ3u99aGqzn0A0eHX7M27hHNHP131l0SQR6Q0Op5vH2bS4iIB/oX/J5Sai6AUqpQKWVVStmAV3FikfholFJ59sci4DN7DIVNxUz7Y1FHx9XKucA6pVQhdI5rZne0a+TyvzsRmQ1cAFxrv3lgr3optX+/Fl0X37+jYjrG783l1wtARNyBS4CPmrZ19DVr6x6BE//OuksiWAP0E5Fe9k+VVwHzXBGIve7xf8BWpdTzrba3rtO7GNh0+HudHJefiAQ0fY9uaNyEvk432ne7EfiiI+M6zCGf0lx9zVo52jWaB9xg79UxFtjfqmjvdCIyA/g1MFMpVdNqe4SIuNm/7w30A7I6MK6j/d7mAVeJiJeI9LLHtbqj4mplCrBNKZXbtKEjr9nR7hE48++sI1rBO8MXumV9BzqT/9aFcZyBLtJtBNLtX+cB7wAZ9u3zgJgOjqs3usfGBmBz0zUCwoAfgJ3AQiDURdfNDygFglpt6/Brhk5EBUADui72lqNdI3Qvjhfsf3MZQGoHx5WJrjtu+jt7yb7vpfbfcTqwDriwg+M66u8N+K39em0Hzu3o36V9+5vAHYft25HX7Gj3CKf9nZkpJgzDMLq57lI1ZBiGYRyFSQSGYRjdnEkEhmEY3ZxJBIZhGN2cSQSGYRjdnEkERpcgIkpEnmv1/CERefIUjneGiKwWPXvntlZTbjT1KV9ln25g4mHv+8k+c2bTLJWfnGwMR4lrj4iEO/KYhuHu6gAMw0HqgEtE5C9KqZJTOZCIRAPvAxcppdbZb7wLRCRPKfU1euK7DKXUrUc5xLVKqbRTicEwOpIpERhdRSN6LddfHf6CfS75H+2TnP0gIj2Oc6y7gTdVywyQJegRuo+KyDD0dMCz7J/4fdoTnIi8KSIviUiaiOwQkQvs271F5A3R60CsF5Gz7NvdROTvIrLJHvc9rQ53j4iss79noH3/M1uVQtY3jRI3jPYwicDoSl4ArhWRoMO2/wd4Syk1BD3x2r+Pc5zBwNrDtqUBg5VS6cDvgY+UUsOUUgfbeP97rW7Kz7banoieV+d84CUR8UYnHaWUSkFPofGWffvt9v2HtYq7SYnSkwO+CDxk3/YQcLdSahh65sy24jKMNplEYHQZSs/Q+DZw72EvjUNX9YCe3uAMJ4dyrT1JDFNKPdxq+xyllE3pqY2zgIH2WN4FUEptA/aiJzObArys7OsJKKVaz5vfNAnZWnSyAFgGPC8i9wLBqmUdAsM4LpMIjK7mn+j5bPxO4RhbgJGHbRuJnmvmVBw+n8vJzu9SZ3+0Ym/nU0r9FbgV8AGWNVUZGUZ7mERgdCn2T85z0MmgyXL0jLMA1wI/H+cwLwCz7e0BiEgYeqnHv51ieJeLiEVE+qAn+dtuj+Va+3n6Az3s278HfmGfEhkRCT3WgUWkj1IqQyn1DHq2XZMIjHYzicDoip4DWnexvAe4SUQ2AtcDTYuB3yEidxz+ZqWn8L0OeFVEtqETyetKqS/bef7WbQQLW23PRk+r/C16dsta4P8Ai4hkoOe/n62UqgNes++/UUQ2ANcc55z3NzUso2fT/PY4+xtGMzP7qGF0ABF5E/hKKeXQcQWG4QimRGAYhtHNmRKBYRhGN2dKBIZhGN2cSQSGYRjdnEkEhmEY3ZxJBIZhGN2cSQSGYRjd3P8Dy7wRaoD9oAoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "init = glorot_normal(seed=None) # 給 LSTM\n",
    "init_d = RandomUniform(minval=-0.05, maxval=0.05) # 給 Dense layer\n",
    "nadam = optimizers.Nadam(lr=0.0015,clipvalue=0.5)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(GRU(6, kernel_initializer=init ,return_sequences = True,kernel_regularizer=regularizers.l2(0.01)\n",
    "                             ,recurrent_regularizer = regularizers.l2(0.01) ,input_shape=(x_train.shape[1],x_train.shape[2]))))\n",
    "model.add(LayerNormalization())\n",
    "model.add(Bidirectional(GRU(6,kernel_initializer=init,kernel_regularizer=regularizers.l2(0.01),recurrent_regularizer = regularizers.l2(0.01))))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(units=1, kernel_initializer=init_d))\n",
    "model.compile(optimizer = nadam , loss=\"mse\")\n",
    "history = model.fit(x_train, y_train, epochs=200, batch_size=24, validation_split=0.1, shuffle=True)\n",
    "#model summary\n",
    "model.summary()\n",
    "#Save Model\n",
    "model.save('GRU_model_pit.h5')  # creates a HDF5 file \n",
    "print('Model Saved')\n",
    "del model  # deletes the existing model\n",
    "\n",
    "custom_ob = {'LayerNormalization': LayerNormalization , 'SeqSelfAttention':SeqSelfAttention}\n",
    "model = load_model('GRU_model_pit.h5', custom_objects=custom_ob)\n",
    "t1 = time.time()\n",
    "# y_pred = model.predict([x_test,x_test])\n",
    "y_pred2 = model.predict(x_test)\n",
    "y_pred = model.predict(x_train)\n",
    "t2 = time.time()\n",
    "print('Predict time: ',t2-t1)\n",
    "y_pred = scaler.inverse_transform(y_pred)#Undo scaling\n",
    "rmse_lstm2 = np.sqrt(mean_squared_error(y_test, y_pred2))\n",
    "rmse_lstm = np.sqrt(mean_squared_error(y_train, y_pred))\n",
    "print('RMSE: ',rmse_lstm)\n",
    "print('RMSE2: ',rmse_lstm2)\n",
    "mae = mean_absolute_error(y_test, y_pred2)\n",
    "mae = mean_absolute_error(y_train, y_pred)\n",
    "print('MAE: ',mae)\n",
    "print('MAE2: ',mae)\n",
    "# r22 =  r2_score(y_test, y_pred2)\n",
    "# r2 =  r2_score(y_train, y_pred)\n",
    "# print('R-square: ',r2)\n",
    "# print('R-square2: ',r22)\n",
    "\n",
    "# n = len(y_test)\n",
    "# p = 12\n",
    "# Adj_r2 = 1-(1-r2)*(n-1)/(n-p-1)\n",
    "# Adj_r22 = 1-(1-r22)*(n-1)/(n-p-1)\n",
    "# print('Adj R-square: ',Adj_r2)\n",
    "# print('Adj R-square2: ',Adj_r22)\n",
    "\n",
    "plt.plot(history.history[\"loss\"],label=\"loss\")\n",
    "plt.plot(history.history[\"val_loss\"],label=\"val_loss\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"No. Of Epochs\")\n",
    "plt.ylabel(\"mse score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5067e1a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2856d381",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
