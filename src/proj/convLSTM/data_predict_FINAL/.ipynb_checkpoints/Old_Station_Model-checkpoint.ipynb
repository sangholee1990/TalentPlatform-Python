{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfe73e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from tcn import TCN\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error, r2_score,mean_absolute_error\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential , load_model , Model\n",
    "from keras.layers import Dense, Dropout , LSTM , Bidirectional ,GRU ,Flatten,Add,BatchNormalization\n",
    "from keras_layer_normalization import LayerNormalization\n",
    "from keras.initializers import  glorot_normal, RandomUniform\n",
    "from keras import optimizers,Input\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3db94502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160, 13) (144, 13) (40, 13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:32: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f98a896b6e34eda85e19dea5e3444e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:44: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8116d29e830d4e7bb53b6125fbfde247",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:\n",
      "(120, 24, 12) (120,)\n",
      "Test size:\n",
      "(16, 24, 12) (16,)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"station_bike _Old.csv\")\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "df = df.set_index(\"timestamp\")\n",
    "#df.head()\n",
    "\n",
    "df[\"hour\"] = df.index.hour\n",
    "df[\"day_of_month\"] = df.index.day\n",
    "df[\"day_of_week\"]  = df.index.dayofweek\n",
    "df[\"month\"] = df.index.month\n",
    "\n",
    "training_data_len = math.ceil(len(df) * 0.9) # taking 90% of data to train and 10% of data to test\n",
    "testing_data_len = len(df) - training_data_len\n",
    "\n",
    "time_steps = 24\n",
    "train, test = df.iloc[0:training_data_len], df.iloc[(training_data_len-time_steps):len(df)]\n",
    "print(df.shape, train.shape, test.shape)\n",
    "train_trans = train[['t1','t2', 'hum', 'wind_speed']].to_numpy()\n",
    "test_trans = test[['t1','t2', 'hum', 'wind_speed']].to_numpy()\n",
    "\n",
    "scaler = RobustScaler() # Handles outliers\n",
    "#scaler = MinMaxScaler(feature_range=(0, 1)) # scale to (0,1)\n",
    "train.loc[:, ['t1','t2','hum', 'wind_speed']]=scaler.fit_transform(train_trans)\n",
    "test.loc[:, ['t1','t2', 'hum', 'wind_speed']]=scaler.fit_transform(test_trans)\n",
    "\n",
    "train['cnt'] = scaler.fit_transform(train[['cnt']])\n",
    "test['cnt'] = scaler.fit_transform(test[['cnt']])\n",
    "\n",
    "#Split the data into x_train and y_train data sets\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in tqdm(range(len(train) - time_steps)):\n",
    "    x_train.append(train.drop(columns='cnt').iloc[i:i + time_steps].to_numpy())\n",
    "    y_train.append(train.loc[:,'cnt'].iloc[i + time_steps])\n",
    "\n",
    "#Convert x_train and y_train to numpy arrays\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "#Create the x_test and y_test data sets\n",
    "x_test = []\n",
    "y_test = df.loc[:,'cnt'].iloc[training_data_len:len(df)]\n",
    "\n",
    "for i in tqdm(range(len(test) - time_steps)):\n",
    "    x_test.append(test.drop(columns='cnt').iloc[i:i + time_steps].to_numpy())\n",
    "    # y_test.append(test.loc[:,'cnt'].iloc[i + time_steps])\n",
    "\n",
    "#Convert x_test and y_test to numpy arrays\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# All 12 columns of the data\n",
    "print('Train size:')\n",
    "print(x_train.shape, y_train.shape)\n",
    "print('Test size:')\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e4edb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 24, 12)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 24, 24)       1800        input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "seq_self_attention_3 (SeqSelfAt (None, 24, 24)       577         bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 24, 24)       0           seq_self_attention_3[0][0]       \n",
      "                                                                 bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_3 (LayerNor (None, 24, 24)       48          add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 576)          0           layer_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 12)           6924        flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 12)           0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 588)          0           dropout_3[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            589         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 1)            2           dense_8[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 9,940\n",
      "Trainable params: 9,940\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Concatenate\n",
    "init = glorot_normal(seed=None) # 給 GRU\n",
    "init_d = RandomUniform(minval=-0.05, maxval=0.05) # 給 Dense layer\n",
    "\n",
    "def Encoder(layer):\n",
    "    shortcut = layer\n",
    "    layer = SeqSelfAttention(\n",
    "                     attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n",
    "                     kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     bias_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     attention_regularizer_weight=0.0001)(layer)\n",
    "    layer = Add()([layer,shortcut])\n",
    "    layer = LayerNormalization()(layer)\n",
    "    layer = Flatten()(layer)\n",
    "    \n",
    "    shortcut2 = layer\n",
    "    layer = Dense(12,kernel_initializer=init_d)(layer)\n",
    "    layer = Dropout(0.15)(layer)\n",
    "    layer = Concatenate()([layer,shortcut2])\n",
    "    output = Dense(1,kernel_initializer=init_d)(layer)\n",
    "    return output\n",
    "\n",
    "def Decoder(layer):\n",
    "    shortcut = layer\n",
    "    layer = SeqSelfAttention(\n",
    "                     attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n",
    "                     kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     bias_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     attention_regularizer_weight=0.0001)(layer)\n",
    "    layer = Add()([layer,shortcut])\n",
    "    layer = LayerNormalization()(layer)\n",
    "    layer = SeqSelfAttention(\n",
    "                     attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n",
    "                     kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     bias_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     attention_regularizer_weight=0.0001)(layer)\n",
    "    layer = LayerNormalization()(layer)\n",
    "    \n",
    "    layer = Flatten()(layer)\n",
    "    shortcut2 = layer\n",
    "    layer = Dense(10,kernel_initializer=init_d)(layer)\n",
    "    #layer = Dropout(0.2)(layer)\n",
    "    layer = Concatenate()([layer,shortcut2])\n",
    "    output = Dense(1,kernel_initializer=init_d)(layer)\n",
    "    return output\n",
    "\n",
    "def Bi_GRU(layer,unit):\n",
    "    output = Bidirectional(GRU(unit, dropout=0.1, recurrent_dropout=0.1, return_sequences=True,\n",
    "                            kernel_initializer=init))(layer)\n",
    "    return output\n",
    "\n",
    "#start = Input(shape = (x_train.shape[1],x_train.shape[2]))\n",
    "start = Input(shape = (x_train.shape[1:]))\n",
    "start2 = Input(shape = (x_train.shape[1:]))\n",
    "x = Bi_GRU(start,12)\n",
    "x = Encoder(x)\n",
    "\n",
    "# y = Bi_GRU(start2,8)\n",
    "# y = Decoder(y)\n",
    "\n",
    "#Merge = Add()([x,x])\n",
    "Last = Dense(1)(x)\n",
    "model = Model([start,start2] , Last)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d2d39d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 108 samples, validate on 12 samples\n",
      "Epoch 1/500\n",
      "108/108 [==============================] - 1s 14ms/step - loss: 1.5710 - val_loss: 1.3453\n",
      "Epoch 2/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.9818 - val_loss: 1.0023\n",
      "Epoch 3/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.9847 - val_loss: 0.6396\n",
      "Epoch 4/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.8987 - val_loss: 0.7233\n",
      "Epoch 5/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.8314 - val_loss: 0.9163\n",
      "Epoch 6/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.8458 - val_loss: 0.6156\n",
      "Epoch 7/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.7558 - val_loss: 0.5145\n",
      "Epoch 8/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5850 - val_loss: 1.0636\n",
      "Epoch 9/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.7500 - val_loss: 1.0231\n",
      "Epoch 10/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6433 - val_loss: 0.7370\n",
      "Epoch 11/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5557 - val_loss: 0.6759\n",
      "Epoch 12/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4874 - val_loss: 0.4964\n",
      "Epoch 13/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4529 - val_loss: 0.4328\n",
      "Epoch 14/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5186 - val_loss: 0.4238\n",
      "Epoch 15/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5178 - val_loss: 0.5411\n",
      "Epoch 16/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4748 - val_loss: 0.6923\n",
      "Epoch 17/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5010 - val_loss: 0.5397\n",
      "Epoch 18/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4475 - val_loss: 0.3647\n",
      "Epoch 19/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5298 - val_loss: 0.3153\n",
      "Epoch 20/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3555 - val_loss: 0.4093\n",
      "Epoch 21/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4259 - val_loss: 0.3996\n",
      "Epoch 22/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3424 - val_loss: 0.3609\n",
      "Epoch 23/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3303 - val_loss: 0.2612\n",
      "Epoch 24/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3870 - val_loss: 0.4335\n",
      "Epoch 25/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3534 - val_loss: 0.2606\n",
      "Epoch 26/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3828 - val_loss: 0.2066\n",
      "Epoch 27/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4215 - val_loss: 0.2323\n",
      "Epoch 28/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2744 - val_loss: 0.4218\n",
      "Epoch 29/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2686 - val_loss: 0.2073\n",
      "Epoch 30/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3286 - val_loss: 0.2109\n",
      "Epoch 31/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3702 - val_loss: 0.1967\n",
      "Epoch 32/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3274 - val_loss: 0.2083\n",
      "Epoch 33/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2960 - val_loss: 0.2392\n",
      "Epoch 34/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3064 - val_loss: 0.2932\n",
      "Epoch 35/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2942 - val_loss: 0.4679\n",
      "Epoch 36/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2812 - val_loss: 0.5453\n",
      "Epoch 37/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2724 - val_loss: 0.3887\n",
      "Epoch 38/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2571 - val_loss: 0.2907\n",
      "Epoch 39/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2162 - val_loss: 0.2702\n",
      "Epoch 40/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2701 - val_loss: 0.2993\n",
      "Epoch 41/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2414 - val_loss: 0.4010\n",
      "Epoch 42/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2828 - val_loss: 0.2498\n",
      "Epoch 43/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2219 - val_loss: 0.2785\n",
      "Epoch 44/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1984 - val_loss: 0.2340\n",
      "Epoch 45/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2155 - val_loss: 0.1910\n",
      "Epoch 46/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2499 - val_loss: 0.1644\n",
      "Epoch 47/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2785 - val_loss: 0.1802\n",
      "Epoch 48/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2795 - val_loss: 0.1875\n",
      "Epoch 49/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2116 - val_loss: 0.1555\n",
      "Epoch 50/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2605 - val_loss: 0.3460\n",
      "Epoch 51/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2268 - val_loss: 0.2484\n",
      "Epoch 52/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2045 - val_loss: 0.1851\n",
      "Epoch 53/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2054 - val_loss: 0.3275\n",
      "Epoch 54/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1939 - val_loss: 0.3084\n",
      "Epoch 55/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2283 - val_loss: 0.3454\n",
      "Epoch 56/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1874 - val_loss: 0.2584\n",
      "Epoch 57/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2163 - val_loss: 0.1554\n",
      "Epoch 58/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1917 - val_loss: 0.1257\n",
      "Epoch 59/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1731 - val_loss: 0.1835\n",
      "Epoch 60/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1905 - val_loss: 0.2439\n",
      "Epoch 61/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1716 - val_loss: 0.1768\n",
      "Epoch 62/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1585 - val_loss: 0.1160\n",
      "Epoch 63/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2199 - val_loss: 0.1265\n",
      "Epoch 64/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2017 - val_loss: 0.1355\n",
      "Epoch 65/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2001 - val_loss: 0.2328\n",
      "Epoch 66/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1424 - val_loss: 0.1923\n",
      "Epoch 67/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1437 - val_loss: 0.1524\n",
      "Epoch 68/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1613 - val_loss: 0.1486\n",
      "Epoch 69/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1586 - val_loss: 0.1828\n",
      "Epoch 70/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1678 - val_loss: 0.2764\n",
      "Epoch 71/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1199 - val_loss: 0.1797\n",
      "Epoch 72/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1764 - val_loss: 0.2025\n",
      "Epoch 73/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1445 - val_loss: 0.1427\n",
      "Epoch 74/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1367 - val_loss: 0.1598\n",
      "Epoch 75/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1387 - val_loss: 0.1168\n",
      "Epoch 76/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1607 - val_loss: 0.1354\n",
      "Epoch 77/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1301 - val_loss: 0.2378\n",
      "Epoch 78/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1781 - val_loss: 0.1174\n",
      "Epoch 79/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1516 - val_loss: 0.1043\n",
      "Epoch 80/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1390 - val_loss: 0.1362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1403 - val_loss: 0.1701\n",
      "Epoch 82/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1304 - val_loss: 0.2529\n",
      "Epoch 83/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1282 - val_loss: 0.2606\n",
      "Epoch 84/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1493 - val_loss: 0.2219\n",
      "Epoch 85/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1243 - val_loss: 0.2686\n",
      "Epoch 86/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1877 - val_loss: 0.1414\n",
      "Epoch 87/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1379 - val_loss: 0.1501\n",
      "Epoch 88/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1323 - val_loss: 0.1834\n",
      "Epoch 89/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1307 - val_loss: 0.1471\n",
      "Epoch 90/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1340 - val_loss: 0.1355\n",
      "Epoch 91/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1127 - val_loss: 0.1432\n",
      "Epoch 92/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1164 - val_loss: 0.1917\n",
      "Epoch 93/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1429 - val_loss: 0.2950\n",
      "Epoch 94/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1360 - val_loss: 0.1479\n",
      "Epoch 95/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1539 - val_loss: 0.3400\n",
      "Epoch 96/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1801 - val_loss: 0.2205\n",
      "Epoch 97/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1821 - val_loss: 0.0957\n",
      "Epoch 98/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1782 - val_loss: 0.0953\n",
      "Epoch 99/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1570 - val_loss: 0.1242\n",
      "Epoch 100/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1412 - val_loss: 0.1908\n",
      "Epoch 101/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1502 - val_loss: 0.1622\n",
      "Epoch 102/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1236 - val_loss: 0.1139\n",
      "Epoch 103/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1313 - val_loss: 0.2500\n",
      "Epoch 104/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1152 - val_loss: 0.2669\n",
      "Epoch 105/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1804 - val_loss: 0.2042\n",
      "Epoch 106/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1501 - val_loss: 0.2536\n",
      "Epoch 107/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1428 - val_loss: 0.2592\n",
      "Epoch 108/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1411 - val_loss: 0.1639\n",
      "Epoch 109/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1164 - val_loss: 0.2408\n",
      "Epoch 110/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1185 - val_loss: 0.1500\n",
      "Epoch 111/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0952 - val_loss: 0.1158\n",
      "Epoch 112/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1263 - val_loss: 0.1160\n",
      "Epoch 113/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1557 - val_loss: 0.1900\n",
      "Epoch 114/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1281 - val_loss: 0.1525\n",
      "Epoch 115/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1380 - val_loss: 0.1594\n",
      "Epoch 116/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1031 - val_loss: 0.1351\n",
      "Epoch 117/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0936 - val_loss: 0.1606\n",
      "Epoch 118/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1121 - val_loss: 0.2753\n",
      "Epoch 119/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1056 - val_loss: 0.2487\n",
      "Epoch 120/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1534 - val_loss: 0.1519\n",
      "Epoch 121/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1189 - val_loss: 0.1301\n",
      "Epoch 122/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1307 - val_loss: 0.1166\n",
      "Epoch 123/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1040 - val_loss: 0.1537\n",
      "Epoch 124/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1238 - val_loss: 0.2451\n",
      "Epoch 125/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0990 - val_loss: 0.3379\n",
      "Epoch 126/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1304 - val_loss: 0.1262\n",
      "Epoch 127/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0947 - val_loss: 0.1044\n",
      "Epoch 128/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0801 - val_loss: 0.1604\n",
      "Epoch 129/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1125 - val_loss: 0.2205\n",
      "Epoch 130/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0948 - val_loss: 0.1534\n",
      "Epoch 131/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0975 - val_loss: 0.1389\n",
      "Epoch 132/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0887 - val_loss: 0.2194\n",
      "Epoch 133/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1028 - val_loss: 0.2596\n",
      "Epoch 134/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1088 - val_loss: 0.1257\n",
      "Epoch 135/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0778 - val_loss: 0.1319\n",
      "Epoch 136/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1012 - val_loss: 0.2329\n",
      "Epoch 137/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0915 - val_loss: 0.2949\n",
      "Epoch 138/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1108 - val_loss: 0.1757\n",
      "Epoch 139/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1170 - val_loss: 0.1739\n",
      "Epoch 140/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0882 - val_loss: 0.1340\n",
      "Epoch 141/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1192 - val_loss: 0.1219\n",
      "Epoch 142/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1185 - val_loss: 0.1917\n",
      "Epoch 143/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1428 - val_loss: 0.1408\n",
      "Epoch 144/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0815 - val_loss: 0.1509\n",
      "Epoch 145/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0891 - val_loss: 0.1709\n",
      "Epoch 146/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0943 - val_loss: 0.1898\n",
      "Epoch 147/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1061 - val_loss: 0.1188\n",
      "Epoch 148/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0983 - val_loss: 0.1027\n",
      "Epoch 149/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0869 - val_loss: 0.2682\n",
      "Epoch 150/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1029 - val_loss: 0.2305\n",
      "Epoch 151/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1031 - val_loss: 0.1210\n",
      "Epoch 152/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0930 - val_loss: 0.1444\n",
      "Epoch 153/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0988 - val_loss: 0.1611\n",
      "Epoch 154/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1052 - val_loss: 0.1280\n",
      "Epoch 155/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0990 - val_loss: 0.0985\n",
      "Epoch 156/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1073 - val_loss: 0.1387\n",
      "Epoch 157/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0990 - val_loss: 0.2619\n",
      "Epoch 158/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0933 - val_loss: 0.2636\n",
      "Epoch 159/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1634 - val_loss: 0.2991\n",
      "Epoch 160/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1136 - val_loss: 0.1762\n",
      "Epoch 161/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1141 - val_loss: 0.1414\n",
      "Epoch 162/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1098 - val_loss: 0.2174\n",
      "Epoch 163/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0790 - val_loss: 0.2380\n",
      "Epoch 164/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0744 - val_loss: 0.2771\n",
      "Epoch 165/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1281 - val_loss: 0.2407\n",
      "Epoch 166/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1386 - val_loss: 0.1680\n",
      "Epoch 167/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0928 - val_loss: 0.0987\n",
      "Epoch 168/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0966 - val_loss: 0.1410\n",
      "Epoch 169/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0832 - val_loss: 0.1594\n",
      "Epoch 170/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0766 - val_loss: 0.1178\n",
      "Epoch 171/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0779 - val_loss: 0.1503\n",
      "Epoch 172/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0853 - val_loss: 0.1629\n",
      "Epoch 173/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1159 - val_loss: 0.2515\n",
      "Epoch 174/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0808 - val_loss: 0.2313\n",
      "Epoch 175/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1088 - val_loss: 0.1538\n",
      "Epoch 176/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0875 - val_loss: 0.1239\n",
      "Epoch 177/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0827 - val_loss: 0.1881\n",
      "Epoch 178/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0864 - val_loss: 0.1100\n",
      "Epoch 179/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0916 - val_loss: 0.1199\n",
      "Epoch 180/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1025 - val_loss: 0.1987\n",
      "Epoch 181/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0858 - val_loss: 0.2102\n",
      "Epoch 182/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0947 - val_loss: 0.1228\n",
      "Epoch 183/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1018 - val_loss: 0.1103\n",
      "Epoch 184/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0721 - val_loss: 0.2122\n",
      "Epoch 185/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0693 - val_loss: 0.1848\n",
      "Epoch 186/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0784 - val_loss: 0.1964\n",
      "Epoch 187/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0827 - val_loss: 0.1510\n",
      "Epoch 188/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0702 - val_loss: 0.1094\n",
      "Epoch 189/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1155 - val_loss: 0.1131\n",
      "Epoch 190/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1124 - val_loss: 0.2359\n",
      "Epoch 191/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0807 - val_loss: 0.1654\n",
      "Epoch 192/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0871 - val_loss: 0.1393\n",
      "Epoch 193/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0877 - val_loss: 0.1703\n",
      "Epoch 194/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0791 - val_loss: 0.1750\n",
      "Epoch 195/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0878 - val_loss: 0.1734\n",
      "Epoch 196/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1027 - val_loss: 0.2704\n",
      "Epoch 197/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0790 - val_loss: 0.3545\n",
      "Epoch 198/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1054 - val_loss: 0.1285\n",
      "Epoch 199/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0758 - val_loss: 0.1344\n",
      "Epoch 200/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0738 - val_loss: 0.2119\n",
      "Epoch 201/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0744 - val_loss: 0.2270\n",
      "Epoch 202/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0760 - val_loss: 0.1810\n",
      "Epoch 203/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0666 - val_loss: 0.2073\n",
      "Epoch 204/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0719 - val_loss: 0.2699\n",
      "Epoch 205/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0733 - val_loss: 0.3352\n",
      "Epoch 206/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0709 - val_loss: 0.1561\n",
      "Epoch 207/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1101 - val_loss: 0.1112\n",
      "Epoch 208/500\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.0731 - val_loss: 0.1119\n",
      "Epoch 209/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0841 - val_loss: 0.1246\n",
      "Epoch 210/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0734 - val_loss: 0.2521\n",
      "Epoch 211/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1138 - val_loss: 0.1482\n",
      "Epoch 212/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0899 - val_loss: 0.1649\n",
      "Epoch 213/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0734 - val_loss: 0.1874\n",
      "Epoch 214/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0653 - val_loss: 0.1372\n",
      "Epoch 215/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1294 - val_loss: 0.1270\n",
      "Epoch 216/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0981 - val_loss: 0.1719\n",
      "Epoch 217/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1070 - val_loss: 0.1433\n",
      "Epoch 218/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0898 - val_loss: 0.1725\n",
      "Epoch 219/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0795 - val_loss: 0.2253\n",
      "Epoch 220/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0713 - val_loss: 0.2566\n",
      "Epoch 221/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0673 - val_loss: 0.1666\n",
      "Epoch 222/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0636 - val_loss: 0.2461\n",
      "Epoch 223/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0669 - val_loss: 0.1925\n",
      "Epoch 224/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0659 - val_loss: 0.1889\n",
      "Epoch 225/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0564 - val_loss: 0.1670\n",
      "Epoch 226/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0813 - val_loss: 0.1869\n",
      "Epoch 227/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0776 - val_loss: 0.1698\n",
      "Epoch 228/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0670 - val_loss: 0.2035\n",
      "Epoch 229/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0888 - val_loss: 0.1545\n",
      "Epoch 230/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0750 - val_loss: 0.1387\n",
      "Epoch 231/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0734 - val_loss: 0.1273\n",
      "Epoch 232/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0623 - val_loss: 0.1674\n",
      "Epoch 233/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0677 - val_loss: 0.2133\n",
      "Epoch 234/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0671 - val_loss: 0.2669\n",
      "Epoch 235/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0970 - val_loss: 0.2915\n",
      "Epoch 236/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0852 - val_loss: 0.1323\n",
      "Epoch 237/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0867 - val_loss: 0.1319\n",
      "Epoch 238/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0766 - val_loss: 0.2013\n",
      "Epoch 239/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1026 - val_loss: 0.1979\n",
      "Epoch 240/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0854 - val_loss: 0.2019\n",
      "Epoch 241/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0801 - val_loss: 0.1966\n",
      "Epoch 242/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0784 - val_loss: 0.1539\n",
      "Epoch 243/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0804 - val_loss: 0.1939\n",
      "Epoch 244/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0880 - val_loss: 0.3073\n",
      "Epoch 245/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0930 - val_loss: 0.1940\n",
      "Epoch 246/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1185 - val_loss: 0.2633\n",
      "Epoch 247/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0984 - val_loss: 0.2502\n",
      "Epoch 248/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0732 - val_loss: 0.1671\n",
      "Epoch 249/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0723 - val_loss: 0.1789\n",
      "Epoch 250/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0865 - val_loss: 0.1494\n",
      "Epoch 251/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0551 - val_loss: 0.1300\n",
      "Epoch 252/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0961 - val_loss: 0.1609\n",
      "Epoch 253/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0972 - val_loss: 0.2178\n",
      "Epoch 254/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0864 - val_loss: 0.1806\n",
      "Epoch 255/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0867 - val_loss: 0.1638\n",
      "Epoch 256/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1036 - val_loss: 0.1499\n",
      "Epoch 257/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0694 - val_loss: 0.1418\n",
      "Epoch 258/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0779 - val_loss: 0.1225\n",
      "Epoch 259/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0588 - val_loss: 0.1328\n",
      "Epoch 260/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0731 - val_loss: 0.1648\n",
      "Epoch 261/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0777 - val_loss: 0.1643\n",
      "Epoch 262/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0688 - val_loss: 0.2749\n",
      "Epoch 263/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0861 - val_loss: 0.2172\n",
      "Epoch 264/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0705 - val_loss: 0.1339\n",
      "Epoch 265/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0678 - val_loss: 0.1493\n",
      "Epoch 266/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0988 - val_loss: 0.1991\n",
      "Epoch 267/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1013 - val_loss: 0.3446\n",
      "Epoch 268/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0787 - val_loss: 0.2023\n",
      "Epoch 269/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0648 - val_loss: 0.2539\n",
      "Epoch 270/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0503 - val_loss: 0.2081\n",
      "Epoch 271/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0676 - val_loss: 0.1571\n",
      "Epoch 272/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0671 - val_loss: 0.1319\n",
      "Epoch 273/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0643 - val_loss: 0.1468\n",
      "Epoch 274/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0599 - val_loss: 0.1630\n",
      "Epoch 275/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0653 - val_loss: 0.1341\n",
      "Epoch 276/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0618 - val_loss: 0.1304\n",
      "Epoch 277/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0591 - val_loss: 0.2911\n",
      "Epoch 278/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0592 - val_loss: 0.2584\n",
      "Epoch 279/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0789 - val_loss: 0.1609\n",
      "Epoch 280/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0610 - val_loss: 0.1504\n",
      "Epoch 281/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0559 - val_loss: 0.1573\n",
      "Epoch 282/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0635 - val_loss: 0.2090\n",
      "Epoch 283/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0600 - val_loss: 0.2005\n",
      "Epoch 284/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0826 - val_loss: 0.1494\n",
      "Epoch 285/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0613 - val_loss: 0.1588\n",
      "Epoch 286/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0537 - val_loss: 0.3867\n",
      "Epoch 287/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0882 - val_loss: 0.4032\n",
      "Epoch 288/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0588 - val_loss: 0.2283\n",
      "Epoch 289/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0528 - val_loss: 0.2158\n",
      "Epoch 290/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1023 - val_loss: 0.1757\n",
      "Epoch 291/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0673 - val_loss: 0.2028\n",
      "Epoch 292/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0788 - val_loss: 0.1686\n",
      "Epoch 293/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0664 - val_loss: 0.1388\n",
      "Epoch 294/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0885 - val_loss: 0.1474\n",
      "Epoch 295/500\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.0770 - val_loss: 0.1659\n",
      "Epoch 296/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0868 - val_loss: 0.1440\n",
      "Epoch 297/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0663 - val_loss: 0.2235\n",
      "Epoch 298/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0673 - val_loss: 0.2425\n",
      "Epoch 299/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0568 - val_loss: 0.2528\n",
      "Epoch 300/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0604 - val_loss: 0.1416\n",
      "Epoch 301/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0537 - val_loss: 0.1751\n",
      "Epoch 302/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0615 - val_loss: 0.1794\n",
      "Epoch 303/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0532 - val_loss: 0.2179\n",
      "Epoch 304/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0615 - val_loss: 0.1792\n",
      "Epoch 305/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0847 - val_loss: 0.1439\n",
      "Epoch 306/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0680 - val_loss: 0.1521\n",
      "Epoch 307/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0598 - val_loss: 0.1820\n",
      "Epoch 308/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0528 - val_loss: 0.1720\n",
      "Epoch 309/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0485 - val_loss: 0.1920\n",
      "Epoch 310/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0473 - val_loss: 0.1685\n",
      "Epoch 311/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0629 - val_loss: 0.2185\n",
      "Epoch 312/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0779 - val_loss: 0.2320\n",
      "Epoch 313/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0683 - val_loss: 0.2107\n",
      "Epoch 314/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0851 - val_loss: 0.1704\n",
      "Epoch 315/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0739 - val_loss: 0.1311\n",
      "Epoch 316/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0573 - val_loss: 0.1258\n",
      "Epoch 317/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0499 - val_loss: 0.1760\n",
      "Epoch 318/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0489 - val_loss: 0.1487\n",
      "Epoch 319/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0607 - val_loss: 0.1673\n",
      "Epoch 320/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0473 - val_loss: 0.2413\n",
      "Epoch 321/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0798 - val_loss: 0.1495\n",
      "Epoch 322/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0473 - val_loss: 0.1597\n",
      "Epoch 323/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0561 - val_loss: 0.1799\n",
      "Epoch 324/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0940 - val_loss: 0.1119\n",
      "Epoch 325/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0734 - val_loss: 0.1477\n",
      "Epoch 326/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0610 - val_loss: 0.1985\n",
      "Epoch 327/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0684 - val_loss: 0.1355\n",
      "Epoch 328/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0541 - val_loss: 0.1610\n",
      "Epoch 329/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0488 - val_loss: 0.2514\n",
      "Epoch 330/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0603 - val_loss: 0.3235\n",
      "Epoch 331/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0665 - val_loss: 0.3629\n",
      "Epoch 332/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0638 - val_loss: 0.2491\n",
      "Epoch 333/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0379 - val_loss: 0.2264\n",
      "Epoch 334/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0553 - val_loss: 0.1846\n",
      "Epoch 335/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0761 - val_loss: 0.1725\n",
      "Epoch 336/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0491 - val_loss: 0.1880\n",
      "Epoch 337/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0443 - val_loss: 0.2159\n",
      "Epoch 338/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0487 - val_loss: 0.1695\n",
      "Epoch 339/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0561 - val_loss: 0.1709\n",
      "Epoch 340/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0496 - val_loss: 0.2283\n",
      "Epoch 341/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0636 - val_loss: 0.2268\n",
      "Epoch 342/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0522 - val_loss: 0.2108\n",
      "Epoch 343/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0617 - val_loss: 0.1549\n",
      "Epoch 344/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1065 - val_loss: 0.1444\n",
      "Epoch 345/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0530 - val_loss: 0.1517\n",
      "Epoch 346/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0444 - val_loss: 0.1466\n",
      "Epoch 347/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0447 - val_loss: 0.2023\n",
      "Epoch 348/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0564 - val_loss: 0.1810\n",
      "Epoch 349/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0517 - val_loss: 0.1308\n",
      "Epoch 350/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0454 - val_loss: 0.2128\n",
      "Epoch 351/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0503 - val_loss: 0.2239\n",
      "Epoch 352/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0486 - val_loss: 0.2125\n",
      "Epoch 353/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0692 - val_loss: 0.1646\n",
      "Epoch 354/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0574 - val_loss: 0.1586\n",
      "Epoch 355/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0457 - val_loss: 0.1462\n",
      "Epoch 356/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0469 - val_loss: 0.1481\n",
      "Epoch 357/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0475 - val_loss: 0.1990\n",
      "Epoch 358/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0567 - val_loss: 0.2173\n",
      "Epoch 359/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0498 - val_loss: 0.1527\n",
      "Epoch 360/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0602 - val_loss: 0.1661\n",
      "Epoch 361/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0530 - val_loss: 0.1897\n",
      "Epoch 362/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0679 - val_loss: 0.1495\n",
      "Epoch 363/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0649 - val_loss: 0.1396\n",
      "Epoch 364/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0558 - val_loss: 0.1740\n",
      "Epoch 365/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0499 - val_loss: 0.1635\n",
      "Epoch 366/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0775 - val_loss: 0.2142\n",
      "Epoch 367/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0973 - val_loss: 0.4037\n",
      "Epoch 368/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0767 - val_loss: 0.2021\n",
      "Epoch 369/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0513 - val_loss: 0.1371\n",
      "Epoch 370/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0580 - val_loss: 0.2496\n",
      "Epoch 371/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0573 - val_loss: 0.2416\n",
      "Epoch 372/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0548 - val_loss: 0.2406\n",
      "Epoch 373/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0593 - val_loss: 0.2012\n",
      "Epoch 374/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0399 - val_loss: 0.1780\n",
      "Epoch 375/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0434 - val_loss: 0.2266\n",
      "Epoch 376/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0402 - val_loss: 0.2286\n",
      "Epoch 377/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0423 - val_loss: 0.1666\n",
      "Epoch 378/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0430 - val_loss: 0.2001\n",
      "Epoch 379/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0829 - val_loss: 0.2306\n",
      "Epoch 380/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0569 - val_loss: 0.1698\n",
      "Epoch 381/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0497 - val_loss: 0.1964\n",
      "Epoch 382/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0626 - val_loss: 0.1295\n",
      "Epoch 383/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0638 - val_loss: 0.1900\n",
      "Epoch 384/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0437 - val_loss: 0.2729\n",
      "Epoch 385/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0533 - val_loss: 0.2321\n",
      "Epoch 386/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0467 - val_loss: 0.1573\n",
      "Epoch 387/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0457 - val_loss: 0.1567\n",
      "Epoch 388/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0546 - val_loss: 0.2000\n",
      "Epoch 389/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0399 - val_loss: 0.2794\n",
      "Epoch 390/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0666 - val_loss: 0.1914\n",
      "Epoch 391/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0573 - val_loss: 0.1950\n",
      "Epoch 392/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0604 - val_loss: 0.1854\n",
      "Epoch 393/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0688 - val_loss: 0.1066\n",
      "Epoch 394/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0558 - val_loss: 0.1502\n",
      "Epoch 395/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0488 - val_loss: 0.1332\n",
      "Epoch 396/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0614 - val_loss: 0.1220\n",
      "Epoch 397/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0466 - val_loss: 0.2277\n",
      "Epoch 398/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0483 - val_loss: 0.1582\n",
      "Epoch 399/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0509 - val_loss: 0.1920\n",
      "Epoch 400/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0374 - val_loss: 0.1521\n",
      "Epoch 401/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0445 - val_loss: 0.2034\n",
      "Epoch 402/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0534 - val_loss: 0.1405\n",
      "Epoch 403/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0422 - val_loss: 0.1215\n",
      "Epoch 404/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0449 - val_loss: 0.1850\n",
      "Epoch 405/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0470 - val_loss: 0.2832\n",
      "Epoch 406/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0453 - val_loss: 0.1760\n",
      "Epoch 407/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.1220 - val_loss: 0.1692\n",
      "Epoch 408/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0469 - val_loss: 0.1616\n",
      "Epoch 409/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0386 - val_loss: 0.1578\n",
      "Epoch 410/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0559 - val_loss: 0.1441\n",
      "Epoch 411/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0421 - val_loss: 0.1591\n",
      "Epoch 412/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0522 - val_loss: 0.1719\n",
      "Epoch 413/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0686 - val_loss: 0.1532\n",
      "Epoch 414/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0626 - val_loss: 0.1860\n",
      "Epoch 415/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0439 - val_loss: 0.1772\n",
      "Epoch 416/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0459 - val_loss: 0.1230\n",
      "Epoch 417/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0544 - val_loss: 0.1893\n",
      "Epoch 418/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0503 - val_loss: 0.1202\n",
      "Epoch 419/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0595 - val_loss: 0.1248\n",
      "Epoch 420/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0393 - val_loss: 0.1690\n",
      "Epoch 421/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0469 - val_loss: 0.1746\n",
      "Epoch 422/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0391 - val_loss: 0.1559\n",
      "Epoch 423/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0444 - val_loss: 0.1320\n",
      "Epoch 424/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0462 - val_loss: 0.1491\n",
      "Epoch 425/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0530 - val_loss: 0.1809\n",
      "Epoch 426/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0484 - val_loss: 0.1895\n",
      "Epoch 427/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0542 - val_loss: 0.2014\n",
      "Epoch 428/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0540 - val_loss: 0.1852\n",
      "Epoch 429/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0400 - val_loss: 0.1495\n",
      "Epoch 430/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0410 - val_loss: 0.1593\n",
      "Epoch 431/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0345 - val_loss: 0.1844\n",
      "Epoch 432/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0459 - val_loss: 0.2477\n",
      "Epoch 433/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0440 - val_loss: 0.2517\n",
      "Epoch 434/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0500 - val_loss: 0.2153\n",
      "Epoch 435/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0403 - val_loss: 0.1506\n",
      "Epoch 436/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0664 - val_loss: 0.2185\n",
      "Epoch 437/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0465 - val_loss: 0.1538\n",
      "Epoch 438/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0401 - val_loss: 0.1510\n",
      "Epoch 439/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0368 - val_loss: 0.1793\n",
      "Epoch 440/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0366 - val_loss: 0.2217\n",
      "Epoch 441/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0425 - val_loss: 0.1664\n",
      "Epoch 442/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0623 - val_loss: 0.1495\n",
      "Epoch 443/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0389 - val_loss: 0.1274\n",
      "Epoch 444/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0658 - val_loss: 0.1886\n",
      "Epoch 445/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0434 - val_loss: 0.2171\n",
      "Epoch 446/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0432 - val_loss: 0.1629\n",
      "Epoch 447/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0358 - val_loss: 0.1383\n",
      "Epoch 448/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0279 - val_loss: 0.2063\n",
      "Epoch 449/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0346 - val_loss: 0.1560\n",
      "Epoch 450/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0447 - val_loss: 0.1628\n",
      "Epoch 451/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0321 - val_loss: 0.2064\n",
      "Epoch 452/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0424 - val_loss: 0.1825\n",
      "Epoch 453/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0396 - val_loss: 0.1566\n",
      "Epoch 454/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0324 - val_loss: 0.1200\n",
      "Epoch 455/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0396 - val_loss: 0.1524\n",
      "Epoch 456/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0267 - val_loss: 0.2136\n",
      "Epoch 457/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0292 - val_loss: 0.2205\n",
      "Epoch 458/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0312 - val_loss: 0.1455\n",
      "Epoch 459/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0428 - val_loss: 0.2266\n",
      "Epoch 460/500\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.0400 - val_loss: 0.1887\n",
      "Epoch 461/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0350 - val_loss: 0.1599\n",
      "Epoch 462/500\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.0690 - val_loss: 0.3180\n",
      "Epoch 463/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0390 - val_loss: 0.1564\n",
      "Epoch 464/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0368 - val_loss: 0.1771\n",
      "Epoch 465/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0364 - val_loss: 0.1462\n",
      "Epoch 466/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0397 - val_loss: 0.1860\n",
      "Epoch 467/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0322 - val_loss: 0.2101\n",
      "Epoch 468/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0360 - val_loss: 0.1436\n",
      "Epoch 469/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0285 - val_loss: 0.1750\n",
      "Epoch 470/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0362 - val_loss: 0.1422\n",
      "Epoch 471/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0395 - val_loss: 0.1578\n",
      "Epoch 472/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0418 - val_loss: 0.1585\n",
      "Epoch 473/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0379 - val_loss: 0.1314\n",
      "Epoch 474/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0356 - val_loss: 0.1518\n",
      "Epoch 475/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0490 - val_loss: 0.1335\n",
      "Epoch 476/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0382 - val_loss: 0.1610\n",
      "Epoch 477/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0426 - val_loss: 0.2261\n",
      "Epoch 478/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0378 - val_loss: 0.2016\n",
      "Epoch 479/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0345 - val_loss: 0.1474\n",
      "Epoch 480/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0343 - val_loss: 0.2453\n",
      "Epoch 481/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0356 - val_loss: 0.1440\n",
      "Epoch 482/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0322 - val_loss: 0.2025\n",
      "Epoch 483/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0381 - val_loss: 0.2061\n",
      "Epoch 484/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0625 - val_loss: 0.1484\n",
      "Epoch 485/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0384 - val_loss: 0.1815\n",
      "Epoch 486/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0377 - val_loss: 0.1441\n",
      "Epoch 487/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0271 - val_loss: 0.1512\n",
      "Epoch 488/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0390 - val_loss: 0.1907\n",
      "Epoch 489/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0339 - val_loss: 0.1499\n",
      "Epoch 490/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0332 - val_loss: 0.1929\n",
      "Epoch 491/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0355 - val_loss: 0.1638\n",
      "Epoch 492/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0292 - val_loss: 0.1579\n",
      "Epoch 493/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0319 - val_loss: 0.2007\n",
      "Epoch 494/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0637 - val_loss: 0.1639\n",
      "Epoch 495/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0535 - val_loss: 0.1553\n",
      "Epoch 496/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0359 - val_loss: 0.1237\n",
      "Epoch 497/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0514 - val_loss: 0.1048\n",
      "Epoch 498/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0400 - val_loss: 0.1098\n",
      "Epoch 499/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0317 - val_loss: 0.1253\n",
      "Epoch 500/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0290 - val_loss: 0.1909\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 24, 12)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 24, 24)       1800        input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "seq_self_attention_3 (SeqSelfAt (None, 24, 24)       577         bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 24, 24)       0           seq_self_attention_3[0][0]       \n",
      "                                                                 bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_3 (LayerNor (None, 24, 24)       48          add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 576)          0           layer_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 12)           6924        flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 12)           0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 588)          0           dropout_3[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            589         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 1)            2           dense_8[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 9,940\n",
      "Trainable params: 9,940\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Predict time:  0.2594587802886963\n",
      "RMSE:  16.229401394100517\n",
      "RMSE2:  10.980393083631286\n",
      "MAE:  13.920356286190176\n",
      "MAE2:  13.920356286190176\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'mse score')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "opt = optimizers.Adam(lr=0.001,decay = 0.0001)\n",
    "print('Train...')\n",
    "# model.compile(optimizer = opt , loss=\"mse\")\n",
    "model.compile(optimizer = \"adam\" , loss=\"mse\")\n",
    "history = model.fit([x_train,x_train], y_train, epochs = 500, batch_size=8, validation_split=0.1, shuffle=True)\n",
    "# history = model.fit(x_train, y_train, epochs = 500, batch_size=6, validation_split=0.1, shuffle=True)\n",
    "model.summary()\n",
    "#Save Model\n",
    "model.save('GRU_Single_Attention_model_Old.h5')  # creates a HDF5 file \n",
    "del model\n",
    "\n",
    "custom_ob = {'LayerNormalization': LayerNormalization , 'SeqSelfAttention':SeqSelfAttention}\n",
    "model = load_model('GRU_Single_Attention_model_Old.h5', custom_objects=custom_ob)\n",
    "t1 = time.time()\n",
    "# y_pred = model.predict([x_test,x_test])\n",
    "y_pred2 = model.predict(x_test)\n",
    "y_pred = model.predict(x_train)\n",
    "t2 = time.time()\n",
    "print('Predict time: ',t2-t1)\n",
    "y_pred = scaler.inverse_transform(y_pred)#Undo scaling\n",
    "rmse_lstm2 = np.sqrt(mean_squared_error(y_test, y_pred2))\n",
    "rmse_lstm = np.sqrt(mean_squared_error(y_train, y_pred))\n",
    "print('RMSE: ',rmse_lstm)\n",
    "print('RMSE2: ',rmse_lstm2)\n",
    "mae = mean_absolute_error(y_test, y_pred2)\n",
    "mae = mean_absolute_error(y_train, y_pred)\n",
    "print('MAE: ',mae)\n",
    "print('MAE2: ',mae)\n",
    "# r22 =  r2_score(y_test, y_pred2)\n",
    "# r2 =  r2_score(y_train, y_pred)\n",
    "# print('R-square: ',r2)\n",
    "# print('R-square2: ',r22)\n",
    "\n",
    "# n = len(y_test)\n",
    "# p = 12\n",
    "# Adj_r2 = 1-(1-r2)*(n-1)/(n-p-1)\n",
    "# Adj_r22 = 1-(1-r22)*(n-1)/(n-p-1)\n",
    "# print('Adj R-square: ',Adj_r2)\n",
    "# print('Adj R-square2: ',Adj_r22)\n",
    "\n",
    "plt.plot(history.history[\"loss\"],label=\"loss\")\n",
    "plt.plot(history.history[\"val_loss\"],label=\"val_loss\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"No. Of Epochs\")\n",
    "plt.ylabel(\"mse score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c8a8fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 108 samples, validate on 12 samples\n",
      "Epoch 1/200\n",
      "108/108 [==============================] - 2s 21ms/step - loss: 1.7031 - val_loss: 1.4332\n",
      "Epoch 2/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.6388 - val_loss: 1.3520\n",
      "Epoch 3/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.5858 - val_loss: 1.2760\n",
      "Epoch 4/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.5006 - val_loss: 1.2117\n",
      "Epoch 5/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.4518 - val_loss: 1.1577\n",
      "Epoch 6/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.3966 - val_loss: 1.1126\n",
      "Epoch 7/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.3727 - val_loss: 1.0765\n",
      "Epoch 8/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.3077 - val_loss: 1.0449\n",
      "Epoch 9/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.2828 - val_loss: 1.0144\n",
      "Epoch 10/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.2262 - val_loss: 0.9887\n",
      "Epoch 11/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.2092 - val_loss: 0.9707\n",
      "Epoch 12/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 1.1625 - val_loss: 0.9505\n",
      "Epoch 13/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.1466 - val_loss: 0.9318\n",
      "Epoch 14/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.0795 - val_loss: 0.9146\n",
      "Epoch 15/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.0781 - val_loss: 0.8889\n",
      "Epoch 16/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.0417 - val_loss: 0.8792\n",
      "Epoch 17/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.0361 - val_loss: 0.8695\n",
      "Epoch 18/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.0231 - val_loss: 0.8463\n",
      "Epoch 19/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.9791 - val_loss: 0.8386\n",
      "Epoch 20/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.9825 - val_loss: 0.8260\n",
      "Epoch 21/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.9613 - val_loss: 0.8045\n",
      "Epoch 22/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.9388 - val_loss: 0.7823\n",
      "Epoch 23/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.9417 - val_loss: 0.7707\n",
      "Epoch 24/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.9150 - val_loss: 0.7674\n",
      "Epoch 25/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.8922 - val_loss: 0.7456\n",
      "Epoch 26/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.8926 - val_loss: 0.7422\n",
      "Epoch 27/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.8509 - val_loss: 0.7306\n",
      "Epoch 28/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.8493 - val_loss: 0.7142\n",
      "Epoch 29/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.8558 - val_loss: 0.6982\n",
      "Epoch 30/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.8402 - val_loss: 0.6833\n",
      "Epoch 31/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.8295 - val_loss: 0.6765\n",
      "Epoch 32/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.8182 - val_loss: 0.6785\n",
      "Epoch 33/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.8017 - val_loss: 0.6775\n",
      "Epoch 34/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.7940 - val_loss: 0.6603\n",
      "Epoch 35/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.7884 - val_loss: 0.6402\n",
      "Epoch 36/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.7883 - val_loss: 0.6268\n",
      "Epoch 37/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.7550 - val_loss: 0.6275\n",
      "Epoch 38/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.7924 - val_loss: 0.6256\n",
      "Epoch 39/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.7423 - val_loss: 0.6181\n",
      "Epoch 40/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.7364 - val_loss: 0.6073\n",
      "Epoch 41/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.7348 - val_loss: 0.5976\n",
      "Epoch 42/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.7072 - val_loss: 0.5799\n",
      "Epoch 43/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.7182 - val_loss: 0.5804\n",
      "Epoch 44/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.7329 - val_loss: 0.5831\n",
      "Epoch 45/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.7078 - val_loss: 0.5893\n",
      "Epoch 46/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.6997 - val_loss: 0.5926\n",
      "Epoch 47/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.7190 - val_loss: 0.5551\n",
      "Epoch 48/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.7035 - val_loss: 0.5606\n",
      "Epoch 49/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6725 - val_loss: 0.5586\n",
      "Epoch 50/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6576 - val_loss: 0.5598\n",
      "Epoch 51/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6580 - val_loss: 0.5419\n",
      "Epoch 52/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6299 - val_loss: 0.5689\n",
      "Epoch 53/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6395 - val_loss: 0.5567\n",
      "Epoch 54/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6433 - val_loss: 0.5436\n",
      "Epoch 55/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6443 - val_loss: 0.5139\n",
      "Epoch 56/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6122 - val_loss: 0.5177\n",
      "Epoch 57/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6171 - val_loss: 0.5166\n",
      "Epoch 58/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6044 - val_loss: 0.5057\n",
      "Epoch 59/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5921 - val_loss: 0.4998\n",
      "Epoch 60/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5929 - val_loss: 0.4885\n",
      "Epoch 61/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5875 - val_loss: 0.4683\n",
      "Epoch 62/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5797 - val_loss: 0.4767\n",
      "Epoch 63/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5602 - val_loss: 0.4401\n",
      "Epoch 64/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5663 - val_loss: 0.4289\n",
      "Epoch 65/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5108 - val_loss: 0.4250\n",
      "Epoch 66/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5216 - val_loss: 0.4448\n",
      "Epoch 67/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5180 - val_loss: 0.4167\n",
      "Epoch 68/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5215 - val_loss: 0.4124\n",
      "Epoch 69/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5144 - val_loss: 0.4192\n",
      "Epoch 70/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5192 - val_loss: 0.4269\n",
      "Epoch 71/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4622 - val_loss: 0.4061\n",
      "Epoch 72/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4972 - val_loss: 0.4473\n",
      "Epoch 73/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4792 - val_loss: 0.4116\n",
      "Epoch 74/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4934 - val_loss: 0.4277\n",
      "Epoch 75/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4368 - val_loss: 0.3556\n",
      "Epoch 76/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4754 - val_loss: 0.4024\n",
      "Epoch 77/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4453 - val_loss: 0.3932\n",
      "Epoch 78/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4665 - val_loss: 0.3839\n",
      "Epoch 79/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4477 - val_loss: 0.3488\n",
      "Epoch 80/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4026 - val_loss: 0.3417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4615 - val_loss: 0.3044\n",
      "Epoch 82/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4072 - val_loss: 0.3330\n",
      "Epoch 83/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4239 - val_loss: 0.3321\n",
      "Epoch 84/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4388 - val_loss: 0.3334\n",
      "Epoch 85/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4319 - val_loss: 0.3474\n",
      "Epoch 86/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4191 - val_loss: 0.3404\n",
      "Epoch 87/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4128 - val_loss: 0.3211\n",
      "Epoch 88/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3890 - val_loss: 0.3346\n",
      "Epoch 89/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3932 - val_loss: 0.3548\n",
      "Epoch 90/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3727 - val_loss: 0.3255\n",
      "Epoch 91/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4129 - val_loss: 0.3750\n",
      "Epoch 92/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3850 - val_loss: 0.3232\n",
      "Epoch 93/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4086 - val_loss: 0.3223\n",
      "Epoch 94/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3804 - val_loss: 0.3237\n",
      "Epoch 95/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3804 - val_loss: 0.3422\n",
      "Epoch 96/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3794 - val_loss: 0.3382\n",
      "Epoch 97/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4068 - val_loss: 0.2974\n",
      "Epoch 98/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3787 - val_loss: 0.3155\n",
      "Epoch 99/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3710 - val_loss: 0.3653\n",
      "Epoch 100/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3763 - val_loss: 0.3693\n",
      "Epoch 101/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3614 - val_loss: 0.3496\n",
      "Epoch 102/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3587 - val_loss: 0.3377\n",
      "Epoch 103/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3631 - val_loss: 0.3106\n",
      "Epoch 104/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3520 - val_loss: 0.3162\n",
      "Epoch 105/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3780 - val_loss: 0.3155\n",
      "Epoch 106/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4086 - val_loss: 0.2807\n",
      "Epoch 107/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3673 - val_loss: 0.3009\n",
      "Epoch 108/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3774 - val_loss: 0.3331\n",
      "Epoch 109/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3895 - val_loss: 0.3005\n",
      "Epoch 110/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3429 - val_loss: 0.3616\n",
      "Epoch 111/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3295 - val_loss: 0.3346\n",
      "Epoch 112/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4008 - val_loss: 0.3750\n",
      "Epoch 113/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3359 - val_loss: 0.2912\n",
      "Epoch 114/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3455 - val_loss: 0.3334\n",
      "Epoch 115/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3595 - val_loss: 0.2838\n",
      "Epoch 116/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3735 - val_loss: 0.2955\n",
      "Epoch 117/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3456 - val_loss: 0.3398\n",
      "Epoch 118/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3254 - val_loss: 0.2896\n",
      "Epoch 119/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3614 - val_loss: 0.2866\n",
      "Epoch 120/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3307 - val_loss: 0.2629\n",
      "Epoch 121/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3121 - val_loss: 0.2751\n",
      "Epoch 122/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3282 - val_loss: 0.3041\n",
      "Epoch 123/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3542 - val_loss: 0.3168\n",
      "Epoch 124/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3553 - val_loss: 0.3689\n",
      "Epoch 125/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3614 - val_loss: 0.3096\n",
      "Epoch 126/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3271 - val_loss: 0.2996\n",
      "Epoch 127/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3462 - val_loss: 0.2644\n",
      "Epoch 128/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3299 - val_loss: 0.2598\n",
      "Epoch 129/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3390 - val_loss: 0.2783\n",
      "Epoch 130/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3184 - val_loss: 0.3075\n",
      "Epoch 131/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3556 - val_loss: 0.2950\n",
      "Epoch 132/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3630 - val_loss: 0.3048\n",
      "Epoch 133/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3397 - val_loss: 0.2779\n",
      "Epoch 134/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3152 - val_loss: 0.2642\n",
      "Epoch 135/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2876 - val_loss: 0.2822\n",
      "Epoch 136/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3223 - val_loss: 0.2779\n",
      "Epoch 137/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3052 - val_loss: 0.3009\n",
      "Epoch 138/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2884 - val_loss: 0.3281\n",
      "Epoch 139/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3191 - val_loss: 0.3150\n",
      "Epoch 140/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3358 - val_loss: 0.2972\n",
      "Epoch 141/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3553 - val_loss: 0.2770\n",
      "Epoch 142/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3375 - val_loss: 0.2755\n",
      "Epoch 143/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3177 - val_loss: 0.2910\n",
      "Epoch 144/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3511 - val_loss: 0.3017\n",
      "Epoch 145/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3039 - val_loss: 0.2713\n",
      "Epoch 146/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2886 - val_loss: 0.2854\n",
      "Epoch 147/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3070 - val_loss: 0.2574\n",
      "Epoch 148/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3170 - val_loss: 0.2607\n",
      "Epoch 149/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3453 - val_loss: 0.2739\n",
      "Epoch 150/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3089 - val_loss: 0.3637\n",
      "Epoch 151/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3067 - val_loss: 0.3363\n",
      "Epoch 152/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2946 - val_loss: 0.3225\n",
      "Epoch 153/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3070 - val_loss: 0.3237\n",
      "Epoch 154/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3087 - val_loss: 0.3362\n",
      "Epoch 155/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3215 - val_loss: 0.3229\n",
      "Epoch 156/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3059 - val_loss: 0.2621\n",
      "Epoch 157/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3513 - val_loss: 0.3082\n",
      "Epoch 158/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2674 - val_loss: 0.3602\n",
      "Epoch 159/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2948 - val_loss: 0.3074\n",
      "Epoch 160/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3120 - val_loss: 0.2640\n",
      "Epoch 161/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3166 - val_loss: 0.2806\n",
      "Epoch 162/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3401 - val_loss: 0.2855\n",
      "Epoch 163/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2953 - val_loss: 0.2780\n",
      "Epoch 164/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2853 - val_loss: 0.2743\n",
      "Epoch 165/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3140 - val_loss: 0.3042\n",
      "Epoch 166/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3062 - val_loss: 0.2926\n",
      "Epoch 167/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2925 - val_loss: 0.2915\n",
      "Epoch 168/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2791 - val_loss: 0.2975\n",
      "Epoch 169/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3427 - val_loss: 0.2925\n",
      "Epoch 170/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2964 - val_loss: 0.2954\n",
      "Epoch 171/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3161 - val_loss: 0.2969\n",
      "Epoch 172/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2711 - val_loss: 0.3077\n",
      "Epoch 173/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2996 - val_loss: 0.3380\n",
      "Epoch 174/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2652 - val_loss: 0.2889\n",
      "Epoch 175/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2684 - val_loss: 0.3104\n",
      "Epoch 176/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2656 - val_loss: 0.2981\n",
      "Epoch 177/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3204 - val_loss: 0.3592\n",
      "Epoch 178/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2825 - val_loss: 0.3105\n",
      "Epoch 179/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2680 - val_loss: 0.3041\n",
      "Epoch 180/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2726 - val_loss: 0.3209\n",
      "Epoch 181/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2948 - val_loss: 0.3198\n",
      "Epoch 182/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2789 - val_loss: 0.2934\n",
      "Epoch 183/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2409 - val_loss: 0.3102\n",
      "Epoch 184/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2816 - val_loss: 0.3188\n",
      "Epoch 185/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2862 - val_loss: 0.3529\n",
      "Epoch 186/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.3063 - val_loss: 0.3064\n",
      "Epoch 187/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2728 - val_loss: 0.4017\n",
      "Epoch 188/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2841 - val_loss: 0.3650\n",
      "Epoch 189/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2838 - val_loss: 0.2927\n",
      "Epoch 190/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2690 - val_loss: 0.2930\n",
      "Epoch 191/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2870 - val_loss: 0.2871\n",
      "Epoch 192/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3048 - val_loss: 0.2673\n",
      "Epoch 193/200\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2527 - val_loss: 0.2813\n",
      "Epoch 194/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2709 - val_loss: 0.3175\n",
      "Epoch 195/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2591 - val_loss: 0.2871\n",
      "Epoch 196/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2696 - val_loss: 0.3108\n",
      "Epoch 197/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.2920 - val_loss: 0.2938\n",
      "Epoch 198/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2523 - val_loss: 0.3139\n",
      "Epoch 199/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.2310 - val_loss: 0.2851\n",
      "Epoch 200/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.2913 - val_loss: 0.3202\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_8 (Bidirection (None, 24, 12)            684       \n",
      "_________________________________________________________________\n",
      "layer_normalization_6 (Layer (None, 24, 12)            24        \n",
      "_________________________________________________________________\n",
      "bidirectional_9 (Bidirection (None, 12)                684       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 1,405\n",
      "Trainable params: 1,405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Saved\n",
      "Predict time:  0.4411966800689697\n",
      "RMSE:  13.730532328516588\n",
      "RMSE2:  10.327176368245626\n",
      "MAE:  12.44150842887384\n",
      "MAE2:  12.44150842887384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'mse score')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "init = glorot_normal(seed=None) # 給 LSTM\n",
    "init_d = RandomUniform(minval=-0.05, maxval=0.05) # 給 Dense layer\n",
    "nadam = optimizers.Nadam(lr=0.0015,clipvalue=0.5)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(GRU(6, kernel_initializer=init ,return_sequences = True,kernel_regularizer=regularizers.l2(0.01)\n",
    "                             ,recurrent_regularizer = regularizers.l2(0.01) ,input_shape=(x_train.shape[1],x_train.shape[2]))))\n",
    "model.add(LayerNormalization())\n",
    "model.add(Bidirectional(GRU(6,kernel_initializer=init,kernel_regularizer=regularizers.l2(0.01),recurrent_regularizer = regularizers.l2(0.01))))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(units=1, kernel_initializer=init_d))\n",
    "model.compile(optimizer = nadam , loss=\"mse\")\n",
    "history = model.fit(x_train, y_train, epochs=200, batch_size=24, validation_split=0.1, shuffle=True)\n",
    "#model summary\n",
    "model.summary()\n",
    "#Save Model\n",
    "model.save('GRU_model_old.h5')  # creates a HDF5 file \n",
    "print('Model Saved')\n",
    "del model  # deletes the existing model\n",
    "\n",
    "custom_ob = {'LayerNormalization': LayerNormalization , 'SeqSelfAttention':SeqSelfAttention}\n",
    "model = load_model('GRU_model_old.h5', custom_objects=custom_ob)\n",
    "t1 = time.time()\n",
    "# y_pred = model.predict([x_test,x_test])\n",
    "y_pred2 = model.predict(x_test)\n",
    "y_pred = model.predict(x_train)\n",
    "t2 = time.time()\n",
    "print('Predict time: ',t2-t1)\n",
    "y_pred = scaler.inverse_transform(y_pred)#Undo scaling\n",
    "y_pred2 = scaler.inverse_transform(y_pred2)#Undo scaling\n",
    "rmse_lstm2 = np.sqrt(mean_squared_error(y_test, y_pred2))\n",
    "rmse_lstm = np.sqrt(mean_squared_error(y_train, y_pred))\n",
    "print('RMSE: ',rmse_lstm)\n",
    "print('RMSE2: ',rmse_lstm2)\n",
    "mae = mean_absolute_error(y_test, y_pred2)\n",
    "mae = mean_absolute_error(y_train, y_pred)\n",
    "print('MAE: ',mae)\n",
    "print('MAE2: ',mae)\n",
    "# r22 =  r2_score(y_test, y_pred2)\n",
    "# r2 =  r2_score(y_train, y_pred)\n",
    "# print('R-square: ',r2)\n",
    "# print('R-square2: ',r22)\n",
    "\n",
    "# n = len(y_test)\n",
    "# p = 12\n",
    "# Adj_r2 = 1-(1-r2)*(n-1)/(n-p-1)\n",
    "# Adj_r22 = 1-(1-r22)*(n-1)/(n-p-1)\n",
    "# print('Adj R-square: ',Adj_r2)\n",
    "# print('Adj R-square2: ',Adj_r22)\n",
    "\n",
    "plt.plot(history.history[\"loss\"],label=\"loss\")\n",
    "plt.plot(history.history[\"val_loss\"],label=\"val_loss\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"No. Of Epochs\")\n",
    "plt.ylabel(\"mse score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab6a3ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict time:  2.1748337745666504\n",
      "RMSE:  13.730532328516588\n",
      "RMSE2:  9.986844511264618\n",
      "MAE:  12.44150842887384\n",
      "MAE2:  12.44150842887384\n",
      "R-square:  -228.18943327528325\n",
      "R-square2:  -5.650869550090418\n"
     ]
    }
   ],
   "source": [
    "custom_ob = {'LayerNormalization': LayerNormalization , 'SeqSelfAttention':SeqSelfAttention}\n",
    "model = load_model('GRU_model_old.h5', custom_objects=custom_ob)\n",
    "t1 = time.time()\n",
    "# y_pred = model.predict([x_test,x_test])\n",
    "y_pred2 = model.predict(x_test)\n",
    "y_pred = model.predict(x_train)\n",
    "t2 = time.time()\n",
    "print('Predict time: ',t2-t1)\n",
    "y_pred = scaler.inverse_transform(y_pred)#Undo scaling\n",
    "#y_pred2 = scaler.inverse_transform(y_pred2)#Undo scaling\n",
    "rmse_lstm2 = np.sqrt(mean_squared_error(y_test, y_pred2))\n",
    "rmse_lstm = np.sqrt(mean_squared_error(y_train, y_pred))\n",
    "print('RMSE: ',rmse_lstm)\n",
    "print('RMSE2: ',rmse_lstm2)\n",
    "mae = mean_absolute_error(y_test, y_pred2)\n",
    "mae = mean_absolute_error(y_train, y_pred)\n",
    "print('MAE: ',mae)\n",
    "print('MAE2: ',mae)\n",
    "r22 =  r2_score(y_test, y_pred2)\n",
    "r2 =  r2_score(y_train, y_pred)\n",
    "print('R-square: ',r2)\n",
    "print('R-square2: ',r22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644fae9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
