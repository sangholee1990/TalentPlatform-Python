{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9009860",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from tcn import TCN\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error, r2_score,mean_absolute_error\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential , load_model , Model\n",
    "from keras.layers import Dense, Dropout , LSTM , Bidirectional ,GRU ,Flatten,Add,BatchNormalization\n",
    "from keras_layer_normalization import LayerNormalization\n",
    "from keras.initializers import  glorot_normal, RandomUniform\n",
    "from keras import optimizers,Input\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d22e5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:32: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160, 13) (144, 13) (40, 13)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d0c24c3db954daab59870162b2cb5da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:44: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a24d6329b2f4e46ae9b57b618e7c3c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:\n",
      "(120, 24, 12) (120,)\n",
      "Test size:\n",
      "(16, 24, 12) (16,)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"station_bike _Soho.csv\")\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "df = df.set_index(\"timestamp\")\n",
    "#df.head()\n",
    "\n",
    "df[\"hour\"] = df.index.hour\n",
    "df[\"day_of_month\"] = df.index.day\n",
    "df[\"day_of_week\"]  = df.index.dayofweek\n",
    "df[\"month\"] = df.index.month\n",
    "\n",
    "training_data_len = math.ceil(len(df) * 0.9) # taking 90% of data to train and 10% of data to test\n",
    "testing_data_len = len(df) - training_data_len\n",
    "\n",
    "time_steps = 24\n",
    "train, test = df.iloc[0:training_data_len], df.iloc[(training_data_len-time_steps):len(df)]\n",
    "print(df.shape, train.shape, test.shape)\n",
    "train_trans = train[['t1','t2', 'hum', 'wind_speed']].to_numpy()\n",
    "test_trans = test[['t1','t2', 'hum', 'wind_speed']].to_numpy()\n",
    "\n",
    "scaler = RobustScaler() # Handles outliers\n",
    "#scaler = MinMaxScaler(feature_range=(0, 1)) # scale to (0,1)\n",
    "train.loc[:, ['t1','t2','hum', 'wind_speed']]=scaler.fit_transform(train_trans)\n",
    "test.loc[:, ['t1','t2', 'hum', 'wind_speed']]=scaler.fit_transform(test_trans)\n",
    "\n",
    "train['cnt'] = scaler.fit_transform(train[['cnt']])\n",
    "test['cnt'] = scaler.fit_transform(test[['cnt']])\n",
    "\n",
    "#Split the data into x_train and y_train data sets\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in tqdm(range(len(train) - time_steps)):\n",
    "    x_train.append(train.drop(columns='cnt').iloc[i:i + time_steps].to_numpy())\n",
    "    y_train.append(train.loc[:,'cnt'].iloc[i + time_steps])\n",
    "\n",
    "#Convert x_train and y_train to numpy arrays\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "#Create the x_test and y_test data sets\n",
    "x_test = []\n",
    "y_test = df.loc[:,'cnt'].iloc[training_data_len:len(df)]\n",
    "\n",
    "for i in tqdm(range(len(test) - time_steps)):\n",
    "    x_test.append(test.drop(columns='cnt').iloc[i:i + time_steps].to_numpy())\n",
    "    # y_test.append(test.loc[:,'cnt'].iloc[i + time_steps])\n",
    "\n",
    "#Convert x_test and y_test to numpy arrays\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# All 12 columns of the data\n",
    "print('Train size:')\n",
    "print(x_train.shape, y_train.shape)\n",
    "print('Test size:')\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8848666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 24, 12)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 24, 24)       1800        input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "seq_self_attention_3 (SeqSelfAt (None, 24, 24)       577         bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 24, 24)       0           seq_self_attention_3[0][0]       \n",
      "                                                                 bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_3 (LayerNor (None, 24, 24)       48          add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 576)          0           layer_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 12)           6924        flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 12)           0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 588)          0           dropout_3[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            589         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 1)            2           dense_8[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 9,940\n",
      "Trainable params: 9,940\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Concatenate\n",
    "init = glorot_normal(seed=None) # 給 GRU\n",
    "init_d = RandomUniform(minval=-0.05, maxval=0.05) # 給 Dense layer\n",
    "\n",
    "def Encoder(layer):\n",
    "    shortcut = layer\n",
    "    layer = SeqSelfAttention(\n",
    "                     attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n",
    "                     kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     bias_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     attention_regularizer_weight=0.0001)(layer)\n",
    "    layer = Add()([layer,shortcut])\n",
    "    layer = LayerNormalization()(layer)\n",
    "    layer = Flatten()(layer)\n",
    "    \n",
    "    shortcut2 = layer\n",
    "    layer = Dense(12,kernel_initializer=init_d)(layer)\n",
    "    layer = Dropout(0.15)(layer)\n",
    "    layer = Concatenate()([layer,shortcut2])\n",
    "    output = Dense(1,kernel_initializer=init_d)(layer)\n",
    "    return output\n",
    "\n",
    "def Decoder(layer):\n",
    "    shortcut = layer\n",
    "    layer = SeqSelfAttention(\n",
    "                     attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n",
    "                     kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     bias_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     attention_regularizer_weight=0.0001)(layer)\n",
    "    layer = Add()([layer,shortcut])\n",
    "    layer = LayerNormalization()(layer)\n",
    "    layer = SeqSelfAttention(\n",
    "                     attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n",
    "                     kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     bias_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     attention_regularizer_weight=0.0001)(layer)\n",
    "    layer = LayerNormalization()(layer)\n",
    "    \n",
    "    layer = Flatten()(layer)\n",
    "    shortcut2 = layer\n",
    "    layer = Dense(10,kernel_initializer=init_d)(layer)\n",
    "    #layer = Dropout(0.2)(layer)\n",
    "    layer = Concatenate()([layer,shortcut2])\n",
    "    output = Dense(1,kernel_initializer=init_d)(layer)\n",
    "    return output\n",
    "\n",
    "def Bi_GRU(layer,unit):\n",
    "    output = Bidirectional(GRU(unit, dropout=0.1, recurrent_dropout=0.1, return_sequences=True,\n",
    "                            kernel_initializer=init))(layer)\n",
    "    return output\n",
    "\n",
    "#start = Input(shape = (x_train.shape[1],x_train.shape[2]))\n",
    "start = Input(shape = (x_train.shape[1:]))\n",
    "start2 = Input(shape = (x_train.shape[1:]))\n",
    "x = Bi_GRU(start,12)\n",
    "x = Encoder(x)\n",
    "\n",
    "# y = Bi_GRU(start2,8)\n",
    "# y = Decoder(y)\n",
    "\n",
    "#Merge = Add()([x,x])\n",
    "Last = Dense(1)(x)\n",
    "model = Model([start,start2] , Last)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbba544c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 108 samples, validate on 12 samples\n",
      "Epoch 1/500\n",
      "108/108 [==============================] - 2s 19ms/step - loss: 0.7637 - val_loss: 0.6613\n",
      "Epoch 2/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.6699 - val_loss: 0.5960\n",
      "Epoch 3/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.6143 - val_loss: 0.5504\n",
      "Epoch 4/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.5430 - val_loss: 0.6947\n",
      "Epoch 5/500\n",
      "108/108 [==============================] - 1s 8ms/step - loss: 0.5333 - val_loss: 0.5294\n",
      "Epoch 6/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.5252 - val_loss: 0.4377\n",
      "Epoch 7/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.5338 - val_loss: 0.4992\n",
      "Epoch 8/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.4817 - val_loss: 0.4998\n",
      "Epoch 9/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.4492 - val_loss: 0.3942\n",
      "Epoch 10/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.4257 - val_loss: 0.4787\n",
      "Epoch 11/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.4192 - val_loss: 0.3602\n",
      "Epoch 12/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.3664 - val_loss: 0.2828\n",
      "Epoch 13/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.3780 - val_loss: 0.3119\n",
      "Epoch 14/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.3274 - val_loss: 0.4146\n",
      "Epoch 15/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.3499 - val_loss: 0.2856\n",
      "Epoch 16/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.3124 - val_loss: 0.2133\n",
      "Epoch 17/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.2737 - val_loss: 0.2647\n",
      "Epoch 18/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2672 - val_loss: 0.3412\n",
      "Epoch 19/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.2345 - val_loss: 0.2919\n",
      "Epoch 20/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.2795 - val_loss: 0.2863\n",
      "Epoch 21/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.2567 - val_loss: 0.2964\n",
      "Epoch 22/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.2195 - val_loss: 0.2808\n",
      "Epoch 23/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.2425 - val_loss: 0.2040\n",
      "Epoch 24/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1853 - val_loss: 0.1797\n",
      "Epoch 25/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.2514 - val_loss: 0.2834\n",
      "Epoch 26/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.2097 - val_loss: 0.2097\n",
      "Epoch 27/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.2304 - val_loss: 0.1582\n",
      "Epoch 28/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1552 - val_loss: 0.2434\n",
      "Epoch 29/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.2158 - val_loss: 0.1567\n",
      "Epoch 30/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1574 - val_loss: 0.2636\n",
      "Epoch 31/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1355 - val_loss: 0.2075\n",
      "Epoch 32/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1796 - val_loss: 0.2162\n",
      "Epoch 33/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1644 - val_loss: 0.1959\n",
      "Epoch 34/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1646 - val_loss: 0.2195\n",
      "Epoch 35/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1300 - val_loss: 0.2110\n",
      "Epoch 36/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1180 - val_loss: 0.1785\n",
      "Epoch 37/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1379 - val_loss: 0.2021\n",
      "Epoch 38/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1002 - val_loss: 0.1765\n",
      "Epoch 39/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1304 - val_loss: 0.2038\n",
      "Epoch 40/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1382 - val_loss: 0.1762\n",
      "Epoch 41/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1333 - val_loss: 0.1879\n",
      "Epoch 42/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1182 - val_loss: 0.1536\n",
      "Epoch 43/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1314 - val_loss: 0.1788\n",
      "Epoch 44/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1343 - val_loss: 0.1866\n",
      "Epoch 45/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1239 - val_loss: 0.2572\n",
      "Epoch 46/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0945 - val_loss: 0.2530\n",
      "Epoch 47/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1222 - val_loss: 0.2585\n",
      "Epoch 48/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1382 - val_loss: 0.1509\n",
      "Epoch 49/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1256 - val_loss: 0.1335\n",
      "Epoch 50/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0858 - val_loss: 0.1708\n",
      "Epoch 51/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1101 - val_loss: 0.2740\n",
      "Epoch 52/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1192 - val_loss: 0.1941\n",
      "Epoch 53/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1429 - val_loss: 0.1642\n",
      "Epoch 54/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1299 - val_loss: 0.2035\n",
      "Epoch 55/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1123 - val_loss: 0.2684\n",
      "Epoch 56/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1057 - val_loss: 0.2144\n",
      "Epoch 57/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0958 - val_loss: 0.1313\n",
      "Epoch 58/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0838 - val_loss: 0.2697\n",
      "Epoch 59/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1094 - val_loss: 0.2712\n",
      "Epoch 60/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1201 - val_loss: 0.2572\n",
      "Epoch 61/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1154 - val_loss: 0.2468\n",
      "Epoch 62/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0919 - val_loss: 0.1988\n",
      "Epoch 63/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1048 - val_loss: 0.1912\n",
      "Epoch 64/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0810 - val_loss: 0.1855\n",
      "Epoch 65/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0863 - val_loss: 0.2308\n",
      "Epoch 66/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0928 - val_loss: 0.1472\n",
      "Epoch 67/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1016 - val_loss: 0.1834\n",
      "Epoch 68/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0659 - val_loss: 0.2907\n",
      "Epoch 69/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1075 - val_loss: 0.2407\n",
      "Epoch 70/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0676 - val_loss: 0.1907\n",
      "Epoch 71/500\n",
      "108/108 [==============================] - 1s 8ms/step - loss: 0.0632 - val_loss: 0.2110\n",
      "Epoch 72/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1126 - val_loss: 0.1553\n",
      "Epoch 73/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0904 - val_loss: 0.1663\n",
      "Epoch 74/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0787 - val_loss: 0.2061\n",
      "Epoch 75/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0722 - val_loss: 0.2270\n",
      "Epoch 76/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0846 - val_loss: 0.2644\n",
      "Epoch 77/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0869 - val_loss: 0.1547\n",
      "Epoch 78/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1266 - val_loss: 0.1736\n",
      "Epoch 79/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0823 - val_loss: 0.1670\n",
      "Epoch 80/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1232 - val_loss: 0.3147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0748 - val_loss: 0.2804\n",
      "Epoch 82/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0577 - val_loss: 0.1848\n",
      "Epoch 83/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0778 - val_loss: 0.1558\n",
      "Epoch 84/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0718 - val_loss: 0.2228\n",
      "Epoch 85/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0694 - val_loss: 0.1480\n",
      "Epoch 86/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0737 - val_loss: 0.1690\n",
      "Epoch 87/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0844 - val_loss: 0.2636\n",
      "Epoch 88/500\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.049 - 1s 7ms/step - loss: 0.0511 - val_loss: 0.1760\n",
      "Epoch 89/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0825 - val_loss: 0.2321\n",
      "Epoch 90/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0713 - val_loss: 0.2046\n",
      "Epoch 91/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0779 - val_loss: 0.2501\n",
      "Epoch 92/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0692 - val_loss: 0.2711\n",
      "Epoch 93/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0840 - val_loss: 0.2114\n",
      "Epoch 94/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0759 - val_loss: 0.2789\n",
      "Epoch 95/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1047 - val_loss: 0.1826\n",
      "Epoch 96/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0747 - val_loss: 0.2169\n",
      "Epoch 97/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0704 - val_loss: 0.2157\n",
      "Epoch 98/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0680 - val_loss: 0.2613\n",
      "Epoch 99/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0957 - val_loss: 0.1865\n",
      "Epoch 100/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0639 - val_loss: 0.2385\n",
      "Epoch 101/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0603 - val_loss: 0.2228\n",
      "Epoch 102/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0625 - val_loss: 0.3119\n",
      "Epoch 103/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0823 - val_loss: 0.3294\n",
      "Epoch 104/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0881 - val_loss: 0.2130\n",
      "Epoch 105/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0662 - val_loss: 0.2866\n",
      "Epoch 106/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0658 - val_loss: 0.2938\n",
      "Epoch 107/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0636 - val_loss: 0.3109\n",
      "Epoch 108/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0760 - val_loss: 0.2576\n",
      "Epoch 109/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0909 - val_loss: 0.2476\n",
      "Epoch 110/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0590 - val_loss: 0.2002\n",
      "Epoch 111/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0695 - val_loss: 0.2564\n",
      "Epoch 112/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0537 - val_loss: 0.2377\n",
      "Epoch 113/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0764 - val_loss: 0.3017\n",
      "Epoch 114/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0701 - val_loss: 0.2803\n",
      "Epoch 115/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0650 - val_loss: 0.1912\n",
      "Epoch 116/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0960 - val_loss: 0.2743\n",
      "Epoch 117/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0814 - val_loss: 0.3252\n",
      "Epoch 118/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0853 - val_loss: 0.2993\n",
      "Epoch 119/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0548 - val_loss: 0.2954\n",
      "Epoch 120/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0480 - val_loss: 0.1981\n",
      "Epoch 121/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0668 - val_loss: 0.1946\n",
      "Epoch 122/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0512 - val_loss: 0.2050\n",
      "Epoch 123/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0749 - val_loss: 0.2156\n",
      "Epoch 124/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0412 - val_loss: 0.2033\n",
      "Epoch 125/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0815 - val_loss: 0.2481\n",
      "Epoch 126/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0553 - val_loss: 0.1991\n",
      "Epoch 127/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0604 - val_loss: 0.2422\n",
      "Epoch 128/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0700 - val_loss: 0.2033\n",
      "Epoch 129/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0682 - val_loss: 0.2385\n",
      "Epoch 130/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0483 - val_loss: 0.2092\n",
      "Epoch 131/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1013 - val_loss: 0.2541\n",
      "Epoch 132/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0893 - val_loss: 0.1826\n",
      "Epoch 133/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0802 - val_loss: 0.1582\n",
      "Epoch 134/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0505 - val_loss: 0.2325\n",
      "Epoch 135/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0729 - val_loss: 0.2442\n",
      "Epoch 136/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0486 - val_loss: 0.1819\n",
      "Epoch 137/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0577 - val_loss: 0.2189\n",
      "Epoch 138/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0744 - val_loss: 0.2477\n",
      "Epoch 139/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0567 - val_loss: 0.1787\n",
      "Epoch 140/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0602 - val_loss: 0.1883\n",
      "Epoch 141/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0518 - val_loss: 0.2834\n",
      "Epoch 142/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0720 - val_loss: 0.2018\n",
      "Epoch 143/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0448 - val_loss: 0.1999\n",
      "Epoch 144/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0809 - val_loss: 0.2363\n",
      "Epoch 145/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0467 - val_loss: 0.2197\n",
      "Epoch 146/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0597 - val_loss: 0.2889\n",
      "Epoch 147/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0573 - val_loss: 0.2479\n",
      "Epoch 148/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0510 - val_loss: 0.2304\n",
      "Epoch 149/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0814 - val_loss: 0.2635\n",
      "Epoch 150/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0484 - val_loss: 0.2261\n",
      "Epoch 151/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0573 - val_loss: 0.2261\n",
      "Epoch 152/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0797 - val_loss: 0.1864\n",
      "Epoch 153/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0601 - val_loss: 0.2239\n",
      "Epoch 154/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0445 - val_loss: 0.2182\n",
      "Epoch 155/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0598 - val_loss: 0.1960\n",
      "Epoch 156/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0519 - val_loss: 0.2351\n",
      "Epoch 157/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0564 - val_loss: 0.2032\n",
      "Epoch 158/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0516 - val_loss: 0.1908\n",
      "Epoch 159/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0720 - val_loss: 0.2123\n",
      "Epoch 160/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0861 - val_loss: 0.1924\n",
      "Epoch 161/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0574 - val_loss: 0.3091\n",
      "Epoch 162/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0433 - val_loss: 0.3128\n",
      "Epoch 163/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0682 - val_loss: 0.2330\n",
      "Epoch 164/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0402 - val_loss: 0.2436\n",
      "Epoch 165/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0589 - val_loss: 0.2868\n",
      "Epoch 166/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0551 - val_loss: 0.2445\n",
      "Epoch 167/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0442 - val_loss: 0.2803\n",
      "Epoch 168/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0430 - val_loss: 0.2218\n",
      "Epoch 169/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0650 - val_loss: 0.2299\n",
      "Epoch 170/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0570 - val_loss: 0.3050\n",
      "Epoch 171/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0487 - val_loss: 0.2559\n",
      "Epoch 172/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0715 - val_loss: 0.4028\n",
      "Epoch 173/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0599 - val_loss: 0.1970\n",
      "Epoch 174/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0507 - val_loss: 0.2409\n",
      "Epoch 175/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0516 - val_loss: 0.2204\n",
      "Epoch 176/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0618 - val_loss: 0.2099\n",
      "Epoch 177/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0781 - val_loss: 0.1735\n",
      "Epoch 178/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1012 - val_loss: 0.3009\n",
      "Epoch 179/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0634 - val_loss: 0.2387\n",
      "Epoch 180/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0448 - val_loss: 0.1695\n",
      "Epoch 181/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0434 - val_loss: 0.2152\n",
      "Epoch 182/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0763 - val_loss: 0.2657\n",
      "Epoch 183/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0757 - val_loss: 0.2157\n",
      "Epoch 184/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0632 - val_loss: 0.2271\n",
      "Epoch 185/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0567 - val_loss: 0.2338\n",
      "Epoch 186/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0785 - val_loss: 0.4713\n",
      "Epoch 187/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0486 - val_loss: 0.3143\n",
      "Epoch 188/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0807 - val_loss: 0.2906\n",
      "Epoch 189/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0503 - val_loss: 0.3447\n",
      "Epoch 190/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0447 - val_loss: 0.2509\n",
      "Epoch 191/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0569 - val_loss: 0.3128\n",
      "Epoch 192/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0479 - val_loss: 0.2887\n",
      "Epoch 193/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0488 - val_loss: 0.2787\n",
      "Epoch 194/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0465 - val_loss: 0.3228\n",
      "Epoch 195/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0370 - val_loss: 0.2718\n",
      "Epoch 196/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0495 - val_loss: 0.2944\n",
      "Epoch 197/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0422 - val_loss: 0.2233\n",
      "Epoch 198/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0509 - val_loss: 0.2081\n",
      "Epoch 199/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0398 - val_loss: 0.2580\n",
      "Epoch 200/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0398 - val_loss: 0.2548\n",
      "Epoch 201/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0350 - val_loss: 0.2283\n",
      "Epoch 202/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0401 - val_loss: 0.2177\n",
      "Epoch 203/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0653 - val_loss: 0.1967\n",
      "Epoch 204/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0437 - val_loss: 0.2113\n",
      "Epoch 205/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0464 - val_loss: 0.2205\n",
      "Epoch 206/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0459 - val_loss: 0.2255\n",
      "Epoch 207/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0444 - val_loss: 0.2184\n",
      "Epoch 208/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0444 - val_loss: 0.2412\n",
      "Epoch 209/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0727 - val_loss: 0.2601\n",
      "Epoch 210/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0400 - val_loss: 0.2050\n",
      "Epoch 211/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0452 - val_loss: 0.1925\n",
      "Epoch 212/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0469 - val_loss: 0.2427\n",
      "Epoch 213/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0506 - val_loss: 0.2467\n",
      "Epoch 214/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0449 - val_loss: 0.2171\n",
      "Epoch 215/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0320 - val_loss: 0.2310\n",
      "Epoch 216/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0514 - val_loss: 0.2033\n",
      "Epoch 217/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0427 - val_loss: 0.1944\n",
      "Epoch 218/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0367 - val_loss: 0.2362\n",
      "Epoch 219/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0324 - val_loss: 0.2115\n",
      "Epoch 220/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0393 - val_loss: 0.1711\n",
      "Epoch 221/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0377 - val_loss: 0.2556\n",
      "Epoch 222/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0412 - val_loss: 0.2155\n",
      "Epoch 223/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0709 - val_loss: 0.2621\n",
      "Epoch 224/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0567 - val_loss: 0.2658\n",
      "Epoch 225/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0673 - val_loss: 0.2971\n",
      "Epoch 226/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0380 - val_loss: 0.2365\n",
      "Epoch 227/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0348 - val_loss: 0.2704\n",
      "Epoch 228/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0804 - val_loss: 0.2518\n",
      "Epoch 229/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0424 - val_loss: 0.2208\n",
      "Epoch 230/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0495 - val_loss: 0.2091\n",
      "Epoch 231/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0358 - val_loss: 0.2229\n",
      "Epoch 232/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0454 - val_loss: 0.2254\n",
      "Epoch 233/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0445 - val_loss: 0.2605\n",
      "Epoch 234/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0560 - val_loss: 0.2516\n",
      "Epoch 235/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0626 - val_loss: 0.2965\n",
      "Epoch 236/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0549 - val_loss: 0.2984\n",
      "Epoch 237/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0365 - val_loss: 0.2472\n",
      "Epoch 238/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0310 - val_loss: 0.1676\n",
      "Epoch 239/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0550 - val_loss: 0.2507\n",
      "Epoch 240/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0538 - val_loss: 0.2204\n",
      "Epoch 241/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0471 - val_loss: 0.3239\n",
      "Epoch 242/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0394 - val_loss: 0.2414\n",
      "Epoch 243/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0397 - val_loss: 0.2443\n",
      "Epoch 244/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0411 - val_loss: 0.2053\n",
      "Epoch 245/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0436 - val_loss: 0.2190\n",
      "Epoch 246/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0419 - val_loss: 0.2112\n",
      "Epoch 247/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0381 - val_loss: 0.2744\n",
      "Epoch 248/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0284 - val_loss: 0.2164\n",
      "Epoch 249/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0656 - val_loss: 0.2169\n",
      "Epoch 250/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0392 - val_loss: 0.2644\n",
      "Epoch 251/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0437 - val_loss: 0.2442\n",
      "Epoch 252/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0407 - val_loss: 0.3132\n",
      "Epoch 253/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0418 - val_loss: 0.2058\n",
      "Epoch 254/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0368 - val_loss: 0.2084\n",
      "Epoch 255/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0713 - val_loss: 0.2445\n",
      "Epoch 256/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0289 - val_loss: 0.1853\n",
      "Epoch 257/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0344 - val_loss: 0.2200\n",
      "Epoch 258/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0368 - val_loss: 0.1962\n",
      "Epoch 259/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0360 - val_loss: 0.2690\n",
      "Epoch 260/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0355 - val_loss: 0.2807\n",
      "Epoch 261/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0604 - val_loss: 0.2088\n",
      "Epoch 262/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0405 - val_loss: 0.1644\n",
      "Epoch 263/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0384 - val_loss: 0.1785\n",
      "Epoch 264/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0550 - val_loss: 0.2628\n",
      "Epoch 265/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0337 - val_loss: 0.2266\n",
      "Epoch 266/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0475 - val_loss: 0.2906\n",
      "Epoch 267/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0578 - val_loss: 0.2206\n",
      "Epoch 268/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0472 - val_loss: 0.2955\n",
      "Epoch 269/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0418 - val_loss: 0.2291\n",
      "Epoch 270/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0474 - val_loss: 0.2724\n",
      "Epoch 271/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0456 - val_loss: 0.2040\n",
      "Epoch 272/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0391 - val_loss: 0.2254\n",
      "Epoch 273/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0620 - val_loss: 0.3487\n",
      "Epoch 274/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0522 - val_loss: 0.1847\n",
      "Epoch 275/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0515 - val_loss: 0.1502\n",
      "Epoch 276/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0363 - val_loss: 0.2193\n",
      "Epoch 277/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0321 - val_loss: 0.3403\n",
      "Epoch 278/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0513 - val_loss: 0.2631\n",
      "Epoch 279/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0504 - val_loss: 0.2350\n",
      "Epoch 280/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0350 - val_loss: 0.2266\n",
      "Epoch 281/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0331 - val_loss: 0.1748\n",
      "Epoch 282/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0280 - val_loss: 0.1753\n",
      "Epoch 283/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0438 - val_loss: 0.2624\n",
      "Epoch 284/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0385 - val_loss: 0.1551\n",
      "Epoch 285/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0388 - val_loss: 0.1877\n",
      "Epoch 286/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0305 - val_loss: 0.1606\n",
      "Epoch 287/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0243 - val_loss: 0.2083\n",
      "Epoch 288/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0434 - val_loss: 0.2061\n",
      "Epoch 289/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0267 - val_loss: 0.1583\n",
      "Epoch 290/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0343 - val_loss: 0.2055\n",
      "Epoch 291/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0450 - val_loss: 0.2115\n",
      "Epoch 292/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0384 - val_loss: 0.1981\n",
      "Epoch 293/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0334 - val_loss: 0.1687\n",
      "Epoch 294/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0335 - val_loss: 0.1930\n",
      "Epoch 295/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0359 - val_loss: 0.1719\n",
      "Epoch 296/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0399 - val_loss: 0.1865\n",
      "Epoch 297/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0307 - val_loss: 0.1754\n",
      "Epoch 298/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0238 - val_loss: 0.2255\n",
      "Epoch 299/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0305 - val_loss: 0.1782\n",
      "Epoch 300/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0372 - val_loss: 0.1889\n",
      "Epoch 301/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0366 - val_loss: 0.1720\n",
      "Epoch 302/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0238 - val_loss: 0.1899\n",
      "Epoch 303/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0747 - val_loss: 0.2244\n",
      "Epoch 304/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0357 - val_loss: 0.2324\n",
      "Epoch 305/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0327 - val_loss: 0.1868\n",
      "Epoch 306/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0287 - val_loss: 0.2539\n",
      "Epoch 307/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0411 - val_loss: 0.1823\n",
      "Epoch 308/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0319 - val_loss: 0.2021\n",
      "Epoch 309/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0305 - val_loss: 0.2971\n",
      "Epoch 310/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0340 - val_loss: 0.1885\n",
      "Epoch 311/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0318 - val_loss: 0.2682\n",
      "Epoch 312/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0357 - val_loss: 0.2889\n",
      "Epoch 313/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0300 - val_loss: 0.1861\n",
      "Epoch 314/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0334 - val_loss: 0.1991\n",
      "Epoch 315/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0282 - val_loss: 0.1685\n",
      "Epoch 316/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0324 - val_loss: 0.1719\n",
      "Epoch 317/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0213 - val_loss: 0.2877\n",
      "Epoch 318/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0389 - val_loss: 0.2144\n",
      "Epoch 319/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0307 - val_loss: 0.1354\n",
      "Epoch 320/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0263 - val_loss: 0.2055\n",
      "Epoch 321/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0294 - val_loss: 0.2273\n",
      "Epoch 322/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0251 - val_loss: 0.1946\n",
      "Epoch 323/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0397 - val_loss: 0.1741\n",
      "Epoch 324/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0368 - val_loss: 0.1824\n",
      "Epoch 325/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0485 - val_loss: 0.2599\n",
      "Epoch 326/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0442 - val_loss: 0.1983\n",
      "Epoch 327/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0281 - val_loss: 0.1739\n",
      "Epoch 328/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0276 - val_loss: 0.2518\n",
      "Epoch 329/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0534 - val_loss: 0.2517\n",
      "Epoch 330/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0409 - val_loss: 0.2120\n",
      "Epoch 331/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0246 - val_loss: 0.2004\n",
      "Epoch 332/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0267 - val_loss: 0.2429\n",
      "Epoch 333/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0310 - val_loss: 0.2654\n",
      "Epoch 334/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0415 - val_loss: 0.2359\n",
      "Epoch 335/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0269 - val_loss: 0.2265\n",
      "Epoch 336/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0353 - val_loss: 0.2862\n",
      "Epoch 337/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0338 - val_loss: 0.1691\n",
      "Epoch 338/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0397 - val_loss: 0.1828\n",
      "Epoch 339/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0389 - val_loss: 0.2196\n",
      "Epoch 340/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0250 - val_loss: 0.2143\n",
      "Epoch 341/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0223 - val_loss: 0.2213\n",
      "Epoch 342/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0357 - val_loss: 0.1717\n",
      "Epoch 343/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0320 - val_loss: 0.2308\n",
      "Epoch 344/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0261 - val_loss: 0.1912\n",
      "Epoch 345/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0709 - val_loss: 0.1738\n",
      "Epoch 346/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0408 - val_loss: 0.2357\n",
      "Epoch 347/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0348 - val_loss: 0.2067\n",
      "Epoch 348/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0262 - val_loss: 0.2442\n",
      "Epoch 349/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0453 - val_loss: 0.2277\n",
      "Epoch 350/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0320 - val_loss: 0.1919\n",
      "Epoch 351/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0321 - val_loss: 0.2646\n",
      "Epoch 352/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0269 - val_loss: 0.2163\n",
      "Epoch 353/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0285 - val_loss: 0.1813\n",
      "Epoch 354/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0356 - val_loss: 0.1906\n",
      "Epoch 355/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0240 - val_loss: 0.1760\n",
      "Epoch 356/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0340 - val_loss: 0.1875\n",
      "Epoch 357/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0290 - val_loss: 0.1912\n",
      "Epoch 358/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0477 - val_loss: 0.1997\n",
      "Epoch 359/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0339 - val_loss: 0.2000\n",
      "Epoch 360/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0330 - val_loss: 0.1673\n",
      "Epoch 361/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0294 - val_loss: 0.1630\n",
      "Epoch 362/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0301 - val_loss: 0.1739\n",
      "Epoch 363/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0257 - val_loss: 0.1814\n",
      "Epoch 364/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0209 - val_loss: 0.1631\n",
      "Epoch 365/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0187 - val_loss: 0.2220\n",
      "Epoch 366/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0226 - val_loss: 0.1686\n",
      "Epoch 367/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0306 - val_loss: 0.1616\n",
      "Epoch 368/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0346 - val_loss: 0.1897\n",
      "Epoch 369/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0315 - val_loss: 0.2423\n",
      "Epoch 370/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0338 - val_loss: 0.2719\n",
      "Epoch 371/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0345 - val_loss: 0.1895\n",
      "Epoch 372/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0348 - val_loss: 0.1791\n",
      "Epoch 373/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0238 - val_loss: 0.1935\n",
      "Epoch 374/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0327 - val_loss: 0.2015\n",
      "Epoch 375/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0254 - val_loss: 0.1600\n",
      "Epoch 376/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0197 - val_loss: 0.1890\n",
      "Epoch 377/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0253 - val_loss: 0.1992\n",
      "Epoch 378/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0203 - val_loss: 0.1593\n",
      "Epoch 379/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0256 - val_loss: 0.2089\n",
      "Epoch 380/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0216 - val_loss: 0.3247\n",
      "Epoch 381/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0346 - val_loss: 0.2577\n",
      "Epoch 382/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0192 - val_loss: 0.1837\n",
      "Epoch 383/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0409 - val_loss: 0.2572\n",
      "Epoch 384/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0297 - val_loss: 0.1595\n",
      "Epoch 385/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0248 - val_loss: 0.2048\n",
      "Epoch 386/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0436 - val_loss: 0.1942\n",
      "Epoch 387/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0251 - val_loss: 0.1629\n",
      "Epoch 388/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0501 - val_loss: 0.1484\n",
      "Epoch 389/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0258 - val_loss: 0.1475\n",
      "Epoch 390/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0536 - val_loss: 0.1712\n",
      "Epoch 391/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0279 - val_loss: 0.2105\n",
      "Epoch 392/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0305 - val_loss: 0.1832\n",
      "Epoch 393/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0351 - val_loss: 0.1712\n",
      "Epoch 394/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0281 - val_loss: 0.2040\n",
      "Epoch 395/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0268 - val_loss: 0.2018\n",
      "Epoch 396/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0372 - val_loss: 0.1906\n",
      "Epoch 397/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0207 - val_loss: 0.2554\n",
      "Epoch 398/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0272 - val_loss: 0.1759\n",
      "Epoch 399/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0243 - val_loss: 0.1717\n",
      "Epoch 400/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0222 - val_loss: 0.1934\n",
      "Epoch 401/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0247 - val_loss: 0.1949\n",
      "Epoch 402/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0392 - val_loss: 0.1569\n",
      "Epoch 403/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0250 - val_loss: 0.2363\n",
      "Epoch 404/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0234 - val_loss: 0.1795\n",
      "Epoch 405/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0361 - val_loss: 0.2663\n",
      "Epoch 406/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0222 - val_loss: 0.2345\n",
      "Epoch 407/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0214 - val_loss: 0.2301\n",
      "Epoch 408/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0199 - val_loss: 0.2008\n",
      "Epoch 409/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0192 - val_loss: 0.1744\n",
      "Epoch 410/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0206 - val_loss: 0.1628\n",
      "Epoch 411/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0317 - val_loss: 0.1429\n",
      "Epoch 412/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0366 - val_loss: 0.2077\n",
      "Epoch 413/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0241 - val_loss: 0.2013\n",
      "Epoch 414/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0209 - val_loss: 0.1702\n",
      "Epoch 415/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0212 - val_loss: 0.2048\n",
      "Epoch 416/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0286 - val_loss: 0.1488\n",
      "Epoch 417/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0235 - val_loss: 0.2141\n",
      "Epoch 418/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0315 - val_loss: 0.2468\n",
      "Epoch 419/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0301 - val_loss: 0.1768\n",
      "Epoch 420/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0269 - val_loss: 0.1523\n",
      "Epoch 421/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0303 - val_loss: 0.1642\n",
      "Epoch 422/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0230 - val_loss: 0.1654\n",
      "Epoch 423/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0203 - val_loss: 0.1816\n",
      "Epoch 424/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0253 - val_loss: 0.1958\n",
      "Epoch 425/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0330 - val_loss: 0.1338\n",
      "Epoch 426/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0271 - val_loss: 0.1967\n",
      "Epoch 427/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0349 - val_loss: 0.2359\n",
      "Epoch 428/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0222 - val_loss: 0.1705\n",
      "Epoch 429/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0203 - val_loss: 0.1784\n",
      "Epoch 430/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0272 - val_loss: 0.2054\n",
      "Epoch 431/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0230 - val_loss: 0.1639\n",
      "Epoch 432/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0214 - val_loss: 0.1544\n",
      "Epoch 433/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0260 - val_loss: 0.1599\n",
      "Epoch 434/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0313 - val_loss: 0.1716\n",
      "Epoch 435/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0174 - val_loss: 0.2032\n",
      "Epoch 436/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0220 - val_loss: 0.1723\n",
      "Epoch 437/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0281 - val_loss: 0.1744\n",
      "Epoch 438/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0238 - val_loss: 0.1634\n",
      "Epoch 439/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0246 - val_loss: 0.1669\n",
      "Epoch 440/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0236 - val_loss: 0.2010\n",
      "Epoch 441/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0224 - val_loss: 0.1427\n",
      "Epoch 442/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0206 - val_loss: 0.1630\n",
      "Epoch 443/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0270 - val_loss: 0.1707\n",
      "Epoch 444/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0301 - val_loss: 0.2031\n",
      "Epoch 445/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0294 - val_loss: 0.1819\n",
      "Epoch 446/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0347 - val_loss: 0.1697\n",
      "Epoch 447/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0257 - val_loss: 0.1399\n",
      "Epoch 448/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0248 - val_loss: 0.1729\n",
      "Epoch 449/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0309 - val_loss: 0.2048\n",
      "Epoch 450/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0229 - val_loss: 0.2064\n",
      "Epoch 451/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0156 - val_loss: 0.1673\n",
      "Epoch 452/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0302 - val_loss: 0.1918\n",
      "Epoch 453/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0157 - val_loss: 0.1684\n",
      "Epoch 454/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0205 - val_loss: 0.1574\n",
      "Epoch 455/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0581 - val_loss: 0.1733\n",
      "Epoch 456/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0303 - val_loss: 0.1803\n",
      "Epoch 457/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0172 - val_loss: 0.1851\n",
      "Epoch 458/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0413 - val_loss: 0.2493\n",
      "Epoch 459/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0561 - val_loss: 0.1787\n",
      "Epoch 460/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0265 - val_loss: 0.1921\n",
      "Epoch 461/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0476 - val_loss: 0.1810\n",
      "Epoch 462/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0223 - val_loss: 0.1830\n",
      "Epoch 463/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0309 - val_loss: 0.2032\n",
      "Epoch 464/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0361 - val_loss: 0.1649\n",
      "Epoch 465/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0302 - val_loss: 0.1667\n",
      "Epoch 466/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0218 - val_loss: 0.2038\n",
      "Epoch 467/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0228 - val_loss: 0.3137\n",
      "Epoch 468/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0324 - val_loss: 0.1736\n",
      "Epoch 469/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0286 - val_loss: 0.2017\n",
      "Epoch 470/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0247 - val_loss: 0.1748\n",
      "Epoch 471/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0305 - val_loss: 0.2263\n",
      "Epoch 472/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0196 - val_loss: 0.1991\n",
      "Epoch 473/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0272 - val_loss: 0.1768\n",
      "Epoch 474/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0199 - val_loss: 0.2143\n",
      "Epoch 475/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0338 - val_loss: 0.1755\n",
      "Epoch 476/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0183 - val_loss: 0.1852\n",
      "Epoch 477/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0274 - val_loss: 0.1613\n",
      "Epoch 478/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0245 - val_loss: 0.1462\n",
      "Epoch 479/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0208 - val_loss: 0.1566\n",
      "Epoch 480/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0252 - val_loss: 0.1769\n",
      "Epoch 481/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0272 - val_loss: 0.1647\n",
      "Epoch 482/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0239 - val_loss: 0.1423\n",
      "Epoch 483/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0220 - val_loss: 0.1516\n",
      "Epoch 484/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0176 - val_loss: 0.1982\n",
      "Epoch 485/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0210 - val_loss: 0.1626\n",
      "Epoch 486/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0480 - val_loss: 0.1964\n",
      "Epoch 487/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0187 - val_loss: 0.1527\n",
      "Epoch 488/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0151 - val_loss: 0.1877\n",
      "Epoch 489/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0194 - val_loss: 0.1834\n",
      "Epoch 490/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0227 - val_loss: 0.1808\n",
      "Epoch 491/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0215 - val_loss: 0.1589\n",
      "Epoch 492/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0240 - val_loss: 0.1688\n",
      "Epoch 493/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0218 - val_loss: 0.1560\n",
      "Epoch 494/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0192 - val_loss: 0.1288\n",
      "Epoch 495/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0180 - val_loss: 0.1532\n",
      "Epoch 496/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0369 - val_loss: 0.1672\n",
      "Epoch 497/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0208 - val_loss: 0.1423\n",
      "Epoch 498/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0259 - val_loss: 0.1483\n",
      "Epoch 499/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0263 - val_loss: 0.1475\n",
      "Epoch 500/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0226 - val_loss: 0.1685\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 24, 12)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 24, 24)       1800        input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "seq_self_attention_3 (SeqSelfAt (None, 24, 24)       577         bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 24, 24)       0           seq_self_attention_3[0][0]       \n",
      "                                                                 bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_3 (LayerNor (None, 24, 24)       48          add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 576)          0           layer_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 12)           6924        flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 12)           0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 588)          0           dropout_3[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            589         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 1)            2           dense_8[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 9,940\n",
      "Trainable params: 9,940\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Predict time:  0.27513813972473145\n",
      "RMSE:  18.60344187315346\n",
      "RMSE2:  15.785144189220999\n",
      "MAE:  13.15352331002553\n",
      "MAE2:  13.15352331002553\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'mse score')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAABgPUlEQVR4nO2dd3gcxdnAf6+6ZMtd7t24YoMBYboBg+klEIjp5SMQSOiExIQaIKGXUAKYTkKxacFgg2kGF8C23Bu25S65SXJTr/P9Mbt3e3t7p5Oskyzd/J7nnrvd2zJ7tzvvvHVEKYXBYDAYYpe4pm6AwWAwGJoWIwgMBoMhxjGCwGAwGGIcIwgMBoMhxjGCwGAwGGIcIwgMBoMhxomqIBCR00RklYhki8h4j+97i8h0EVkoIktE5IxotsdgMBgMwUi08ghEJB5YDYwFcoB5wMVKqRWObSYAC5VSL4nIMGCqUqpvVBpkMBgMBk+iqRGMArKVUuuUUhXAB8C5rm0U0Mb63BbYEsX2GAwGg8GDhCgeuwew2bGcAxzh2uYB4GsRuQloBZxc20E7deqk+vbt20BNNBgMhthg/vz5+UqpDK/voikIIuFi4C2l1FMichTwHxEZrpSqcW4kItcB1wH07t2brKysJmiqwWAwNF9EZGOo76JpGsoFejmWe1rrnFwDTAJQSv0MpACd3AdSSk1QSmUqpTIzMjwFmsFgMBjqSTQFwTxgoIj0E5Ek4CJgsmubTcBJACIyFC0I8qLYJoPBYDC4iJogUEpVATcC04CVwCSl1HIReVBEzrE2uwO4VkQWA+8DVylTDtVgMBgalaj6CJRSU4GprnX3OT6vAI6JZhsMBkPLoLKykpycHMrKypq6Kfs1KSkp9OzZk8TExIj3aWpnscFgMERETk4O6enp9O3bFxFp6ubslyilKCgoICcnh379+kW8nykxYTAYmgVlZWV07NjRCIEwiAgdO3ass9ZkBIHBYGg2GCFQO/X5jWJGEMzbsJOnvl5FVXVN7RsbDAZDDBEzgmDhpl08/3025VVGEBgMhvrRunXrpm5CVIgZQZAYry+10mgEBoPBEEDMCYIKoxEYDIZ9RCnFnXfeyfDhwxkxYgQTJ04EYOvWrYwePZqRI0cyfPhwZs6cSXV1NVdddZVv22eeeaaJWx9MzISPJtmCwGgEBkOz5++fL2fFlr0Nesxh3dtw/9kHRrTtJ598wqJFi1i8eDH5+fkcfvjhjB49mvfee49TTz2Vu+++m+rqakpKSli0aBG5ubksW7YMgN27dzdouxuCmNEIkhJs05BJXDYYDPvGrFmzuPjii4mPj6dLly4cf/zxzJs3j8MPP5w333yTBx54gKVLl5Kenk7//v1Zt24dN910E1999RVt2rSp/QSNTMxoBMY0ZDC0HCIduTc2o0ePZsaMGUyZMoWrrrqK22+/nSuuuILFixczbdo0Xn75ZSZNmsQbb7zR1E0NIGY0gsR4HVtrnMUGg2FfOe6445g4cSLV1dXk5eUxY8YMRo0axcaNG+nSpQvXXnstv//971mwYAH5+fnU1NTw29/+locffpgFCxY0dfODiBmNwDYNGR+BwWDYV8477zx+/vlnDj74YESExx9/nK5du/L222/zxBNPkJiYSOvWrXnnnXfIzc3l6quvpqZG9z2PPPJIE7c+mNgRBMY0ZDAY9pGioiJAZ+8+8cQTPPHEEwHfX3nllVx55ZVB++2PWoCT2DENJZg8AoPBYPAiZgRBkkkoMxgMBk9iRhD4o4ZM+KjBYDA4iRlBkJSgo4aMs9hgMBgCiR1BEB8PQKVxFhsMBkMAURUEInKaiKwSkWwRGe/x/TMissh6rRaR3dFqS2KCySMwGAwGL6IWPioi8cCLwFggB5gnIpOteYoBUErd5tj+JuCQaLUn0dQaMhgMBk+iqRGMArKVUuuUUhXAB8C5Yba/GHg/Wo0xJSYMBkNjEm7ugg0bNjB8+PBGbE14oikIegCbHcs51rogRKQP0A/4PsT314lIlohk5eXl1asxyVYegZTvhb1b63UMg8FgaInsL5nFFwEfKaWqvb5USk0AJgBkZmbWK/7T1gjGzb0AZuXBA3vq2VSDwdDkfDketi1t2GN2HQGnPxry6/Hjx9OrVy/+9Kc/AfDAAw+QkJDA9OnT2bVrF5WVlTz88MOce244w0cwZWVl3HDDDWRlZZGQkMDTTz/NiSeeyPLly7n66qupqKigpqaGjz/+mO7du/O73/2OnJwcqquruffeexk3btw+XTZEVxDkAr0cyz2tdV5cBPwpim0hPk6IE2hdUT+NwmAwxDbjxo3j1ltv9QmCSZMmMW3aNG6++WbatGlDfn4+Rx55JOecc06dJpB/8cUXERGWLl3Kr7/+yimnnMLq1at5+eWXueWWW7j00kupqKigurqaqVOn0r17d6ZMmQLAnj0NM6CNpiCYBwwUkX5oAXARcIl7IxEZArQHfo5iWwB/4TmDwdDMCTNyjxaHHHIIO3bsYMuWLeTl5dG+fXu6du3KbbfdxowZM4iLiyM3N5ft27fTtWvXiI87a9YsbrrpJgCGDBlCnz59WL16NUcddRT/+Mc/yMnJ4fzzz2fgwIGMGDGCO+64g7/+9a+cddZZHHfccQ1ybVHrGZVSVcCNwDRgJTBJKbVcRB4UkXMcm14EfKCUinrKr20eMhgMhvpw4YUX8tFHHzFx4kTGjRvHu+++S15eHvPnz2fRokV06dKFsrKyBjnXJZdcwuTJk0lNTeWMM87g+++/Z9CgQSxYsIARI0Zwzz338OCDDzbIuaLqI1BKTQWmutbd51p+IJptcJIUHweeXgiDwWConXHjxnHttdeSn5/Pjz/+yKRJk+jcuTOJiYlMnz6djRs31vmYxx13HO+++y5jxoxh9erVbNq0icGDB7Nu3Tr69+/PzTffzKZNm1iyZAlDhgyhQ4cOXHbZZbRr147XXnutQa5rf3EWNwpJCUYQGAyG+nPggQdSWFhIjx496NatG5deeilnn302I0aMIDMzkyFDhtT5mH/84x+54YYbGDFiBAkJCbz11lskJyczadIk/vOf/5CYmEjXrl3529/+xrx587jzzjuJi4sjMTGRl156qUGuSxrBItOgZGZmqqysrHrtO/rx6cwo+Y1eMFFDBkOzYuXKlQwdOrSpm9Es8PqtRGS+UirTa/uYMponG2exwWAwBBE7pqGcLC6v+ripW2EwGGKIpUuXcvnllwesS05OZs6cOU3UIm9iRxBsnM0VJW/7l5WCOsT6GgyGpkcpVacY/aZmxIgRLFq0qFHPWR9zf+zYShLTApeVqTlkMDQnUlJSKCgoqFdHFysopSgoKCAlJaVO+8WORpDg+mFqqiEuvmnaYjAY6kzPnj3JycmhvvXGYoWUlBR69uxZp31iRxAkpgYu11QBSU3SFIPBUHcSExPp169fUzejRRI7pqEgjaCqadphMBgM+xmxIwgSXYLAu9CpwWAwxByxIwgS3KYhIwgMBoMBYkkQuDUCIwgMBoMBiCVBEKQRGB+BwWAwQCwJgiCNwAgCg8FggFgSBG6NwDiLDQaDAYglQWB8BAaDweBJ7AgCEzVkMBgMnkRVEIjIaSKySkSyRWR8iG1+JyIrRGS5iLwXtcYkJKNwFKsyPgKDwWAAolhiQkTigReBsUAOME9EJiulVji2GQjcBRyjlNolIp2j1R5EUPHJSLU1n6gRBAaDwQBEVyMYBWQrpdYppSqAD4BzXdtcC7yolNoFoJTaEcX2QHyi/7NxFhsMBgMQXUHQA9jsWM6x1jkZBAwSkdki8ouInOZ1IBG5TkSyRCRrXyoPqjiHIDA+AoPBYACa3lmcAAwETgAuBl4VkXbujZRSE5RSmUqpzIyMjPqfLd4IAoPBYHATTUGQC/RyLPe01jnJASYrpSqVUuuB1WjBEBXEGUJqfAQGg8EARFcQzAMGikg/EUkCLgImu7b5H1obQEQ6oU1F66LVIHGGkBpBYDAYDEAUBYFSqgq4EZgGrAQmKaWWi8iDInKOtdk0oEBEVgDTgTuVUgXRalNAUplxFhsMBgMQ5RnKlFJTgamudfc5PivgdusVdcQ5S5nxERgMBgPQ9M7ixsU5S5kRBAaDwQDEmiBIND4Cg8FgcBNbgiDBRA0ZDAaDm9gSBE6NQNU0XTsMBoNhPyK2BIHRCAwGgyGI2BIEJmrIYDAYgogtQWA0AoPBYAgitgRBt4P8n40gMBgMBiDWBMGwc3m+xxP6s3EWGwwGAxBrggDITx8KgKqubOKWGAwGw/5BzAmCYT07ALCzqLSJW2IwGAz7BzEnCI4coGfDnDhnA5Pmba5la4PBYGj5xJwg6N0pHYCi0jL+8vGSJm6NwWAwND0xJwgkPgmAREwegcFgMEAMCgLi4lAST6JU0bN9au3bGwwGQwsn9gQBIAnJDO2cQnycNHVTDAaDocmJSUFAfCKpUkVxuTEPGQwGQ1QFgYicJiKrRCRbRMZ7fH+ViOSJyCLr9ftotsdHfBLJcdWUVpjsYoPBYIjaVJUiEg+8CIwFcoB5IjJZKbXCtelEpdSN0WqHJ/HJJEs1JZXVKKUQMSYig8EQu0RTIxgFZCul1imlKoAPgHOjeL7IiU8kiUqUgrJKU2rCYDDENtEUBD0AZ8ZWjrXOzW9FZImIfCQivaLYHj8JyQza8RXvJT5MWd66RjmlwWAw7K80tbP4c6CvUuog4Bvgba+NROQ6EckSkay8vLx9P2t8IgBHx6+gOnfhvh/PYDAYmjHRFAS5gHOE39Na50MpVaCUKrcWXwMO8zqQUmqCUipTKZWZkZGx7y2LT/Z9rCgvD7OhwWAwtHyiKQjmAQNFpJ+IJAEXAZOdG4hIN8fiOcDKKLbHj5VdDFBZYQSBwWCIbaIWNaSUqhKRG4FpQDzwhlJquYg8CGQppSYDN4vIOUAVsBO4KlrtCSDBCAKDwWCwiZogAFBKTQWmutbd5/h8F3BXNNvgiUMjkMJcqCyDxJQwOxgMBkPLJSLTkIj0EZGTrc+pIpIe3WZFmTi//Buw4kWYeFkTNsZgMBialloFgYhcC3wEvGKt6gn8L4ptij7uaSqzv2madhjqRuku2G3mkDAYGppINII/AccAewGUUmuAztFsVNSpMTWGmiUvHgHPDm/qVhgMLY5IBEG5lRkMgIgkACp6TWoElBEEzZKi7U3dAoOhRRKJIPhRRP4GpIrIWOBDdCJY88VoBAaDweAjEkHwVyAPWAr8AR0FdE80GxV13D4Cg8FgiGHCho9aFUSXK6WGAK82TpMagRpTftpgMBhswmoESqlqYJWI9G6k9jQOxjRkMBgMPiJJKGsPLBeRuUCxvVIpdU7UWhVtjLPYYDAYfEQiCO6NeisaG5dGoCQOMzWNwWCIVWoVBEqpH0WkC3C4tWquUmpHdJsVZdwagTR1NW5DnVAKzKxyBkODEUlm8e+AucCFwO+AOSJyQbQbFlUOuzpgUTX5tAyGOmF8PAZDgxKJaehu4HBbCxCRDOBbdNmJ5snh1+jXA20BUMYw1LyoqYL4qNZLNBhiikiGwnEuU1BBhPs1G2qMIGheGGe/wdCgRDKs+kpEpgHvW8vjgC+j16TGxwiCZobJAzEYGpRInMV3isj5wLHWqglKqU+j26zGpaJGiKuuITG+RSk6LRfjIzAYGpRaBYGI9AOmKqU+sZZTRaSvUmpDtBvXWNQo2LxtB/2TC6HTwKZujqE2jEZgMDQokQyBPwScxXmqrXUthhri6PrZxfBCZlM3xRAJRiMwGBqUSARBgrMMtfU5Kcz2PkTkNBFZJSLZIjI+zHa/FRElIk3SE9cgpO1Y0BSnNtQHoxEYDA1KJIIgz5pgHgARORfIr20nq2Ddi8DpwDDgYhEZ5rFdOnALMCfSRjc0AeGjNaYyaZNRtEMni9WGEQQGQ4MSiSC4HvibiGwSkc3ostR/iGC/UUC2UmqdpUV8AJzrsd1DwGNAWYRtbnDinJavmsqmakZss2MlPDkQ5r1W+7bGNGQwNCi1CgKl1Fql1JHoUf1QpdTRSqnsCI7dA3BOMJtjrfMhIocCvZRSU8IdSESuE5EsEcnKy8uL4NR1IwFHx1JtBEGTkL9Gv6/7ofZtTR6BwdCgRFJi4hYRaYOuPPqsiCwQkVP29cQiEgc8DdxR27ZKqQlKqUylVGZGRsa+njqIRKcgMGaHJqIOs5+a/8hgaFAiMQ39n1JqL3AK0BG4HHg0gv1ygV6O5Z7WOpt0YDjwg4hsAI4EJjeqw/hPc6npcECgRmA6mabB9g1EUgDQ/EcGQ4MSiSCwPalnAO8opZY71oVjHjBQRPqJSBJwETDZ/lIptUcp1Ukp1Vcp1Rf4BThHKZVVpyvYFzIGI8PPJ1GMIGhy7OlDI6kqav4jg6FBiUQQzBeRr9GCYJoV5VNraI1Sqgq4EZgGrAQmKaWWi8iDziikpkbiXZGwxkfQRNimoUgEgYnsMhgakkhqDV0DjATWKaVKRKQjcHX4XTRKqanoye6d6+4Lse0JkRyzwXFXsTSjzabBZxoyGoHB0NhEUmuoBljgWC5AVyBtGcQlBi6bTqZpUHXRCMx/ZDA0JKbKWrxLEBjTUBNhNAKDoakwgiDOmIb2C+qiEZg8AoOhQYlIEIjIsSJytfU5w6pI2jJwawQms7iJqItGYASBwdCQRJJQdj+6rMRd1qpE4L/RbFSj4o4aMp1M0+ALHzV5BAZDYxOJRnAecA46sxil1BZ0MljLwPgI9g9MQpnB0GREIggqlFIKS3cXkVbRbVIjk5ASsFhRWRFiQ0N0MVFDBkNTEYkgmCQirwDtRORa4Fvg1eg2qxFJSA5YfOLLZU3UkBinTnkELSihbO6r8MrxTd0KQ4wTSR7BkyIyFtgLDAbuU0p9E/WWNRbxgYJg7bbdTdOOWMcXCRSBIPjiVshfDWPujmaLGoepf27qFhgMETmLWwHfK6XuRGsCqSKSWMtuzQeXaag1pfBYP1g9rYkaFKPY5p5IqlhVFMGMx6PanEanpga+uQ/25Na+rcHQwERiGpoBJItID+ArdPXRt6LZqEbFZRrqH7cVSnfqh9LQeNTUQSNoiWxdCLP/BZ9c19QtMcQgEVUfVUqVAOcDLymlLgQOjG6zGhGXIPB1Q8Yh2bj4NIIYzXG0feWVJU3aDENsEpEgEJGjgEsBeyax+Og1qZFxCYJWcVaH1FIEQdEOePdCKK51mummxScIYlQjsFEtyBFuaDZEIghuRSeTfWqVke4PTI9qqxoTt49ASvWHlpJYNusZWPM1LH4/9DYrJsOC/zRem7zwCd5mIAhysqCigUfutgCIxfIZlaXw0/Mt55lrhkQyZ/GPSqlzlFKPWcvrlFI3R79pjYQrszixxhIELSWxbE+Ofk/vFnqbSZfD5Bsbpz2hsDuBUBqBCjGV5UvHwvONN6kdhdvhtZMa/veyS5u0pNDYSFk/E76+B3IX1L6tISrUGj5qTR35N6Cvc3ul1EHRa1Yj4tIIWlGuP7QU09BeKwolMbVp21Ebtf3eoUwm25c2fFvCUVGk33PnN+xxK60BSCxqBNXWM1dV2rTtiGEiMQ29i44S+i1wtuPVMnD5CNIo0x/2N0Gw41d4oB0UrK3bfnY4YiQaTlUjZFVXVcCyj4PNAPbvHco84CUINs1p2LZFgq2xeGkoZXth3muhtZdwVNn3XSwKAuverCpv2nY0JJWl8NVdUF7Y1C2JiEgEQZ5SarJSar1SaqP9iuTgInKaiKwSkWwRGe/x/fUislREFonILBEZVucr2FfiAv3e+62PYNlHgIKlH9Ztv6Jt+r2mCt6/BKaFScIqzqt38yLmk2vho/+DjbMD19uCINSI2Ov/eOOUhm1bJNidvFdnP+UO/dr0c92PawuCWNQI7P++sgVpBPPfgl/+DTOfauqWREQkU1XeLyKvAd8BPpGtlPok3E4iEg+8CIwFcoB5IjJZKbXCsdl7SqmXre3PAZ4GTqvbJTQsaT7T0H7mI0i26vzVd4RRXQmrrKCvU//hvU3xDmjbo37Hj5QV/9PvTu2jqkLbiSG0jXx/iaaxR69e7Sncam1TD82q0hYE+8l1NiYtUSOw74Fm4muMRBBcDQxBl5+271IFhBUEwCggWym1DkBEPgDOBXyCQCm117F9K/zR1E1Gq/3VNJTUWr/bNuq6UrYncFkpWPwBDDvHv66oETQCm2rHQ//NfbDFchTaHWFlKRRugw7W1Be1jZSVapzQU18n73Gr+kpp1yO62mcaikFBYA+6WpKPoC61s/YDIhEEhyulBtfj2D2AzY7lHOAI90Yi8ifgdiAJGFOP8zQorXymoSo9QnH5EJqMfdUICtYELucugP9dD+t+8K/bHZHFr2GwOz6AHcv9n+0OP+sNmP4IjN8EcXG1j5SrK/b9v9q7RQcPpHUIXF+yE356Dg6/NrxGYJuv4uojCCzB2FCmobK9UL4X2vZsmONFk5aoEVCHsur7AZG08qdo2u6VUi8qpQagJ7+5x2sbEblORLJEJCsvL7qj1lY4bsaHOzd8dEh9sW+o8npqBPmrvdfnZvk///xC/Ryd9cFpGnJOF2p3piUFUFHoHyXW5rOZO2Hf2/T0UHjWIxjuw6usfIxp/tGr1+8UaeG8+W/BR9cErrOvs6FMQxNOgGeaSQGAlugj8P2PzUMjiEQQHAksspy+Syzn7pII9ssFejmWe1rrQvEB8BuvL5RSE5RSmUqpzIyMjAhOXT9UYhrJ4rLpbayH46++bFuqI2q8sDvC+pqGNs/T7wlWGKn98Nl5Bm16wK4N9bNv21RVRG7acJqG4hw1DO3O1Nc52LbzWgTU1/c0jBCr8NC4Ci2He2VpeNOQ/R+F8y+tngaf32I5/x3Yo+GGClLYaUWXVVVA9X5m5nRj/6YtSSNoZqahSATBacBA4BR02OhZRBY+Og8YKCL9RCQJuAiY7NxARAY6Fs8EXPaLxkXiPIqqlhQ0XgPmva6jTrywO8byvd7fe+7j6FTsEac9+raXbRNNqmUOqa8gqKmGhzN0hxwJuzbqWvzONjnbbHdeVa74+iFnhT7mvgixcELEHt1VFPvP4TVyt9sYzkH43u+810crj+Dx/vD8ofrzgnfg539Hvu+6H/yCOJr4TEONcK5Gow4TLe0HRJJZvNHrFcF+VcCNwDRgJTDJKlHxoBUhBHCjiCwXkUVoP8GV9b+UBiBed0hVkghtLNtqQTYseh/WNMIUDNUVurPxwicI6qAReHWMlcW603Or4ant9HtdcgmUgh8ehZ3r/ceb81Jk+856Wtfi37k+0KZud7D2qNodTTNgDJz1jPcx96UjCSdg7WurKK7FR2CtizRSxCl8GlojsKko9Pt+ln0MSz6IbL+tS+CdcyMX7PuCfW+3JEHgkwPNQxBE4iyuN0qpqcBU17r7HJ9vieb5I2bo2frh3aazVCskicnHf8WYhTfTbuc67VQFeGBPmIM0ANUV+lVVAQmBpS98D0s409CWRXpE2eMwvexWtVt11iGiVeWhBUFdRtV7NsMPj8DyT+EqKzQ1nI3bq5N78QjocmDwNjVujcA6blx8oAbhZF9MCyU7Q39nm4sCBEEYH4HTNLTgHR1LftNC7fR24hQYDe0j8KK6KvLfqHiHfi/Ijl57bIxG0OQ0D5d2tBn3X7hkoi/qpIIkbv9wCd9trKybKSYUpbsjGyXa21R6aAU+k0mYjnrC8fCqI/DKfU47GqayJPihS21f+/Hd2J1hRUlkjj6v36C63B86CsHmFVsj8NUiiguqD+VjX5yNpSEEgVJ+LayyJNhHUF0J+WsC2+j8DSffpH0vXqGRznXu64wGNZWRd7a2aS6+Eeagcmt/LQFfKHHz6GKbRysbC6vuUJnSN3+5SmwYB9ZjfWCSw+pll1kocwkZu/Pzqmzpy7wNcY5NvwSvq3a13fYDVBQH171PaRvYhohQ/vdIfqdIkvR8pqEQGoHE+wVB/xPh/xwzye2TRrDL/9lZsruqzC+cKoqCTUNf3wMvZOrQU+XybzjxatuHVwWex3ncaFBdGbnpz/6vvPxm9aVwO/zwWLA2Vd0CTUM2zcQ0ZASBE0sjKK7WNutyEhvu5lw1BbYshBlPwLS7dJmFrDcCt7EfPi8/gU8QhLBNv3Fq8Hp35+PUCNyjr+Q2+r0uGoH9ACsVWTJQJELGbRry+QicGoHVOcUlQNcR/n335b9yagRPDPB/duZtVJQ4wketdetn6PeSgmCNwCnQq8qCr3/t94HfQ3RLTNRJI7AFQQNOPfLxNfDDP2GbK+iwZh9NQ+WF+18injKmoeaLpRFUYGkEJKHcnem2ZboAXKQ4Vf0JJ8D3D2tTAUCe6zi+DsThB1j9NSz/n7cgWD8TigtCRza5Ox7b/FNRFNxx25nLdRIEDjOJ83da9aX3qDiSbG3lcriW7YYH2sLC/+rluHh8D1dcPCS1gkutkNt9EQRuW7h9PQGCoDjYNGS3JXc+7FqvP9sdm1O4VJXBznWhz+8uOldT03Bx9Xamc6Q+gleO18XzoHbT0A+P6vkZImHvFuuYLtPevvgIKsvgkZ6N49SuCz4N1giC5oelEZShb9RyEoIfnJePgX8HJUh7U13pPQq2bc5BgsB2CDs0gvcuhA+v9HcQytFRvH2WjuzYmxN4nJ3r9cNs176xsTWCCi+NIN3f5kixTU/K1Wm9f5GOCgravh4awe5N+t0u3iVxDu3A6uDsjOJQHUnBWljzbehzVlXAzy8GrrPzK2Y/619XGSZq6HNH3INXpmzpLnhxVOg2ODWf6ir4/iH4R9eGmQDHtlNXVwT/Rpvn+X0coP/HrYv8RQHDmYaU0sECr50UWTvsAU5Q5dl9yCy2BzRZr9d932jim2ioyavmRIQRBE6sZKtipTuWcpWI1Kf80aov4e/t4aFO3k5I2wGdtypQpfVpBGFMQ/a7ve32pf6Rls2sZ/TDvNSVtJTWUb/v3aJnhHKSXB+NwBn14nqIba3HSUQ+ApcgcDvrJc5RysG6fe25Fr79O7x1VrA2MvNp+PQPIc6nIPsb3Um17+tfv3uT/m7BO3o5rZN3+KjXiM9rhFu62/v8NgHb7vSfN1Q4cV2wBUFNpV/Q2Lx+svZx2NjJczbxYQIL65q3YQ+A3L4ruz2b5+o8mnCd557cwBBq2+exv/kX3M/rfo4RBE6skWUJ2kRUTj0dZZ/f6u8oinYEf28XgKssCawB5IueqEUQKBX4ELoFge/mcz1QGUP1+6fXBT+MPo3A4+EuyoN/9oQcV7kNX3KVh4/As4OM4KHw2dmt38I917LEOeY3dmkEuVmwYWZw1u6ezaHDbhe/Dx9coj+36uxfv3tTYCfcurMroSyMDbjao3OqLfrMKUiL8x0aRx0HImunwwsuzcP+L+zf3/3fgzZtbZ4XLAjCaQR17Xzt+9o9aPBV6izXJqlwSZzPDIM3HAWKva5lf8AIgmaM5SMowQ4jracg6D7S/9kdnQM6WshOWNs817/eFzUURhDY2wUIAlfljlDmizbdoNMg7zYnuUxDBWthr2VaWv+jjqX/+QX/9kU7HKNcj6ghr7C5cCPIQafDAWODNQK3IIiLh1ZWmZGMIfrdNcscO9cHLu/N1Z2WV2im027f2iEI9uRo/4RN217a1Pb9Q3rZpxF4XIsv4snxm7gjxNw4BWlJPgHhqXVhyh2Qv8r7u3AmmFfHaO3A7cgN5yOob5SWe7+6lnx3zkrXGJMp1caqL4O1b/t/M4KgGWJrBCqERhBpzRanautl460ohIzBOmTTWdTOLQicxwkQBBWBnWqpI/QRHJEtLkGQkAI9D/dus21esY/7/KHw9JDAdfGJWjjsyYEnB8LES/3ncTs2vQRBuAc+MUV38m4fQYmHRtD/eLjsExh9p/+6nDgn2FHKrzFtWQSP9QsUvs6KpXYILWjzjK25nfoIdB0eeI7qcktQekiCwq36HPukEVj/fZ3nxfCqgWRrAhE4ZbdFOPXn7s36VR/cAwL3cxVK+HmZjGozT5Xs1MEGy/8XcfPqzPsX6YgoJ3a79rcJrkJgBIETl0ZQrlyC4KGOkR3H+aB5mXlAR06kdw/s6NxRQ07NwHlDuQWBu+xEqAc+IdnvJ3Bjd4juB+vre/0dSVyCFg5eVS3d5/JKDgonSBNSdCe/bYnuuH2mIVe1WdscdMBJfvt1kCBwmOPKdvu1ssk36g7eDvl07+vMWC7Z6dd4Og/1F+tz8sm13iawrDfg9bF10wgqy/AJlZIC6q0ReFFTpX1RkZRycDqOw53/2eHwWh2qxnuV0/C1z3WOUJ27lwZSm2lou1XivCGq09aFGqMRNF+ssg6ltiAgRAYrhHdoOR+0ADOPo9OIT9AOWmcn7isjYe3jnEwmSCNwPDxu+7d9/hWfBa5PSPH7AtzYJgD3g//Tczo7FkJnSSqlHd9O3BPhQPADf8mH0G2k1bZk//FfPNJhGnLZi73a4J6HwGlO2uMwm9kjWFtTKtweGMro/Fy6028aSm0XXPIDAk1HAMMvCBQYzv/e6/dwUlUKrbv426/qKQhC3Zc1jgg239wHHtvaUVrO/RoCZ+cepBG4BUGIc3rlqngFJTixNdVElyCvS7mN+mAPeva3mQ5DYASBB1XxaQBUhCvFFE4lrSrzdyrOzsDZicUl6Bh45/fuqKFwgsAdmugkVKRJQrI+p02fY/yf7fZWl4fuTNydhE1JPsx7NXCds+1vnQW/vBT8gPfM9OcvJKT4R9fle0I7zt31eux9nTgd9E5Hul0zqKRAO0WfGgQ/Pu7/vpcjLNipEaS0g3iXsOl/gnVeh3Bv3VmbuGyc/0ttpqHqCr1vSjut0fgEQR1s4BUlwfeC7/iVwT4Crw630BV4EGS2qdKmlrrivCfdGol71Bzqmt1a5q9TA7OzvSZtsrVBtyD4z2/0fCPRwmcaMhpB88MK5bz4qP784fj+4aOGvJzAvu/K/CNv53ZBgqC1fkC2r4DPbvQ/ILZfwdl5hHMWb54TeP6QgiDF3/FKHJzykP87nyCoCH1tdSlA5hQEG2bCV+P9HVGy1ZGkdfA/oAnJgcItlBmplcfD63ZoOjUCd44FwJyXYf7b+rPTNDfwZLhjFRx8iRYE9jWktgvWOlLaanOP839JahUYZePslGszDYHet+sIHb3jLrURCS8cHqyl2FSV+4/pEwQhRsTJjo7eeZ/98BgseDvy9jhx3lNBUUORmoZcGoGzRhX4kw6d2MLBbdrbMNP7HA2FzzRkfATND+uh69quNXedPjTYR+AkXKJPVZljjmGnIHCMHn2CoBAmXgYL/+PveGxTT4BG4PYRhFE5Q3Xk8Ul+jUDVuMwiDtNQqFLXkWS6jrlXv+9YDvnZge20O/fLPoJ7rFG7fY2tMgJ/K68OsH2/wJISNs7fNbmt1igqivXxlkzybucP/wxel5QO6V21gPKZhkQf0y0IkttoQe38rZNau8xLddAIQP8HA8boqBhbEwqnEdRUQ/Z31nZV3kLPxqlZ5a+Cl48L7ex1Rr05TRs//BOm3B72EkLipfl6nQNC39tujcA9N3S2R9KgLRjdGkG0cUYNrflGVwHYjzGCwIkrYzVs+Gi4TrGqzF+7x2m/D2Uact/49kNTtN2/zvnwzH8bVn8Z+vyhOnIRv4CCQHOHUyMIFXMfSWc2+s/++QJeOAz+e77/O/uBj0/yd6w5VgTPgDGBnarXuQaeUnvKfm/LvLN1sc6X2FSHGeZs53Nqe92WRe/pOX/j4kJrBM7OKalVYAJWgEawO3D/Qy4LPn9cQmAnDOEF/k/P69939dc6gTAcTiE740ntlF/8nve2yelwklUt3hbe4cpjQOgwztLduiMMMA2FSCjzLUeoETjrILXpGRxqDP7/IFTp8obEaVJ1CoLvH9YZ2PsxRhA48UXH6J8lrGloxwp4/xJvJ2BVCNOQ80YJEASuB8N+aJz2badddc5LOns4FM5Im3Z9Ar9LdggCpwPUKQi8bK0QuZki8/8gvZv+7IzQsR8Opynn8Gv1e+dhgb+VUwjadD8k9Dltk0y/463zzoSVn0fWXjetOun3PZv95jO3jyC5jR5lO23qyemBGsHi9/W7xPlNQ7bGZCf3OYlPDBTUEF4Q2NNRFm4JTgRz49QIfEIphFBNbgPH3QHdD9UC4OkD4bkwvz2EHjx8ci28e4G/DhN4OItrWbYJF6Lcprt3IpotCEKZwRpyGk+ngLMHPdWV+rfxSizdjzCCwElNoEbw0G8zQ2876XJdUdRdQRT0DWELAudILGCe3gS9TVVZ8A1uP1R7HKp+OJ+EG2cFy4tcoz6ns9jZadkjpsoyXduoPvzGMTuZrRE5sQWJ87xnPgn37dQj/drq6nQZFvq7Npbgad8H2vX2F00DOPC88Md1YyeqAfQ6Ur87NYJrp0OKx/WltPMeeSa28ms4w3+ri+Qd+cfg7eKTIDEtcF3YqBOrI1cqtG/AxvnbOjPbvbDv3fhEbUYKZ3LyHT+EILDraTmjv4KcxfU0DTk1grY9AqOtbGxBEGqug0iq5oYjoMxKWfD6mmr92xft0POfF3oMcPYDjCBwYs+U1aE/AKMO6Fr7PtuWBS4r5dIIQuURJPo7ZbcZZNsSPb+AUyOo76QdSa6OJckRPuoc5Yrojmjr4tpD8rxo0xNGXuJf9gpTtZNu3J2l/UCH+q2S28Axt0AXD/+ATXp3/Z6QorWg4h1aoI/fDGc85d/u1H/CQI+S3ac95v/s9EOkW/eAUxD0ODRY0J39nDZveYUkJiT7NYKEZO2UDhX95BTUEDg6Xvejv/heACp0tJCNV6cfSouw/7u6zEUQMjzWElbO9nmZhhId1+285sJt/qTLoDImTtNQD/29O1DCbleo3Il9nQzHOYjzFARVuk3le+DN0yIv0NfIRFUQiMhpIrJKRLJFZLzH97eLyAoRWSIi34lIH6/jNBqj/gDXfq8zVyE4LNGL/NWBy/ZN7tMIQnRudgnlULxxamDpiNpGLr2P9l6f1BpuXwk3/GQtOzUC14Menwwbfwp/nlA4s3LBe8Qc6rw29kPVtnfg+u6HwNgHvTtPG7tgXHmhNhOANjeltAkUhq27QO8jA/dNbgtHXu9fdv5Gtk/CbRpy+iou/xQOu1Kb2vZ4jJ4TUvxaWrh7ylMQVMIHl8L8t+Cdc+C7B/2dl90GVVN7UbtIBIFtzvNpBHWwq4c6v91Gp7nSy1nsNFk6v3/FMeueu9N2ar5220vy9aRPyz6x9rHuKafwcR6nrhpB4XYdBOHbv8z7sy9qqDJwgLOnntnYUSZqgkBE4oEXgdOBYcDFIuLW7RcCmUqpg4CPgMdpSuLi/PP9QrCD0IuiHbouz9f36hvMvrF8GkGIG82OGnLjHOUUbvV3HJWlwZ2Rk85DddkFN4lpumO0tZ0AH4HrePGJoUfldrJTKJJd1xJqOkkIPdK0O6sO/QLXe/1Obk79J2ReA4NP9wuCTgP1u7PzTWrt9wHYeNmPr58Nf5rnX3b/Vvbyqf/UmoCNfc6A9ocwx7lJ9BAEZbvh1y8Cy1z7Bh9WJ1tZVjfTkI1bEHS2/BZ2523/T5E4WkOe3yUIJN4KZXWYcCpL/HNlAHx0tQ6pBiiy2pj1JsxwdQ9OB7X9nxcX6EmfProaXhntDxYIVe7j2RHwqjVKz10QXFixplrXEaqp1sUXnxqk5xVxtt33uQx2rNSDKVsjKC9qFrkE0dQIRgHZSql1SqkK4APgXOcGSqnpSin7l/wF6BnF9tSdECP2TQc67LtF23Rdnp+e0yGgPo3AjhoKpRGEEATOTqtsr7/AWmVpsJnHpnVXOOlef+fkFCZum3NimE4pVCd15lOBE8x74RZ44TKva5vspG2vwGW3kPGiVUc462kdJmhPyWnPv+AcvSe1Cj6+l9mg63DIcBTocwuCYefBuHfhiBsC118yEX7/Xej2O49z+uNwxpOO71KD4929TD47Vlof7Dmji2rXCJyzodkUuQRBF6ueku3YtP8nt7YHcJMrht82waydroXWro162acRWBE9aR307/33dnq2vlVf6X3TXMJ5xhOBy1/cGqh92yZYmzY99LszL2TrYv9n57bunI7cLP3+6on+shlf3wsfX6uf6Y+v0b7APOt3r3AEU1S6NIJ/Hwlvnu4XEPaxw1FeFN0s5wiIZkxVD8CpB+UA4WZ0uQbwjIkUkeuA6wB69+7ttUl0CDFNX1FqD+/tnRN/2A9/SEGQ6O+onASMyJXeZs9m3dEmpnl3DKc8pEdUdkee1FrbJCHYnBIXB0fdCIPPCA7FtFXyMffAoNPg5WPhis90Fm22q3OzkTgYcqY/WsfZ9lDUNsJsnRG4HM6EFo5Ux+8bn6SvLykt0BkcKW5BEBcHQ88K3q5tz2D/iE/gS+C1H2HNkTD1z/5zxMVpYW3fB14dvF1d1O6EKor0iDwuMbRz2V2aG4JHqqOu1ZE3R92ol+22prTV952ziKE7Lt9u5+SbYc8m/RsfeQN+jWCH/pzcxj9K//5h//7uZ8FdssRN6S6HQMRfObY4Xw+mQtW+KtweWd2hn57T77aWtGtDYHXayjKtwTlNSwG1pWopKeLkEas/OfMpOPz3ke/XgOwXzmIRuQzIBJ7w+l4pNUEplamUyszIyPDapFEpkVRtt+40OPALpfw3nFf4qJO4eH2MZI/RlhN7pFRVFjopxh712yO4UJqDzan/gL7HBK8fdZ1+73Wkdpg+sEcLAQhtJktKh3H/1Z2IE1sjGHauP/LGJpRGcP0sOPNpj460joLgsCvhkMvhKIfm1t42N0n9BEs4s5wbpxZ27G3+60lI9s6DsDtc+/91/n9egn/mU/Dzv/2ROuWWRtA2xAAlUlI7wG/+7Y/AcmoE7hpPTs1V4mD3Rm1q2rPJu93Fefp3T0jxLlXiNtc55+nw4vF+gbk09v4l+d5+GFswZL0Oc18Jf2wnTtOs85rmv2Wtdwgc5yxpkWSSu5lyR933aSCiKQhyAacO3tNaF4CInAzcDZyjlNpPZ5kIpLwmDq77AY7/S+AXSyf51W2v8FHwdxJxCfpBG3RK4Pfu+HK7WmhlaRhBYN2stk3XbQ6qDTuK5oTx2jbe99jgbbyqb0Ltk60ffAlcM82//+AzQztMu46Aw68JNF9BZD4CJylt4dwXAu3OF76ltaDOlpvqjlVw8QeRHzMSf5GNU9Cd/IDD+RriGO4Jdpz/32JXG+2or2l3+Ued89/UJog2+2hZDdJ6wgiCxDQt5Nr21prC3Ak6wMHG7jTt/Up36X1C/Y4p7QKX6zr7WVJr/fsW5+GpjdqCwB3cYRNQ8t1xT9safWWpLjti89VfdTSTUyNYMtFxvHqWlsj+rkmmt4ymIJgHDBSRfiKSBFwETHZuICKHAK+ghcD+nXHhoLTGenCdHQ1om+RHVohkqPBRex97FHjwxYHfn/DXwLh3e6RTUxm6g7cfWNssUJcR793b/LkGIto27jVq9T3Aru9CnavjAP1uq/x3rIQ/Z8PF79WeHew+Zl0FgRddhsHF7/uFZnpXv1CIhLoIAje+wnohfDD272ELS+f1ujuUdg7TqLtejtOnAdBzlN/M48bL+e822cU7TENBGkGyFnK3OeYv2LUBhv1Ga18+QeD4r1PbBU4H6sTLD+TOJzjxbu/8C/s8rTppZ7FXSKgtCELVy3I6kJ3mOPs6ivOCtZzS3fWbU9qZe+ScqhZ0pvivU+p+zH0kaoJAKVUF3AhMA1YCk5RSy0XkQRE5x9rsCaA18KGILBKRySEO12RUe5hu9pTDlt2lwaMY0KqpxOuszLjEYDusLQjsUWP/E3Umrk1Cqr80MwTaub2StMDvy7BHnN0OglsWww0RlFdITI0sTNBZHM6Js61OTv47XPQ+9LKmTUxtH2z7D0WQIKinj6A2QpXk9qIupiHQ13/lF4Hnqe0Y9m8bTui0DTPq7zzMHyYM8Ptv4JSHtVbmpsOA4HVuAR1OIwglzAefrv/r0l06hNM5Ak/rFFxCw8adAQ+BI3DQTv6uB3nvD5YgyPOOAqss051uwVrvfe3Z+CBwPgt7zvG9ucGCoGRn5Nnrzn7ki9sc7fLwIU69M/jao0xUfQRKqalKqUFKqQFKqX9Y6+5TSk22Pp+slOqilBppvc4Jf8TGJ/6wK4LWfbhoO0c/+j01oUwcvY/Uox+vKBy7M7c777g4GOuoAprSJrDjczrRQtn+7ZFcl2G6xv8ZT+qRV7hM3Lpit6P7IXDiPf5aNAPHem+fmAJDzqjfuaKhEXgRSrB6UVukk5tjb4V+x1nnqaX99n1kC9tw5wonJNr10dFdV3+p/S2gO+xjbw3etmP/8G0Cv1mkwwBClqMA7eS06TbSLwg+ujpwu1YdodvB3sdo0x3udmXdun0Jqe1Da1WgBY172lawQlbLdCmOUD673Rv9n511lQotAVGQHTxJUt6vOmdh8Jn+daGur70ryKWyVJdm9woGKNziHeUVRfYLZ/F+zckPwp2BBbcqle7Et6X0h/MmBI+WMiwnsm8GLYd93V7nVMMDpktsF+gLcDrR3NUWbZzhkINOiU6lRduUULYXjr8Tjr4ZLnhTx+43NG5BEEn4aH2oS8JUbeascNh2fa8wTPALAvs+CLUdBE8/6qS9Naruc3RgdrTXrHS2RuAO23Ri1zLqPDT0pESgI11sE2engbrDdk7B6mtHJx18cOXncKirjEliWuBcDhCc4Z7aLtgcC/7ns1WnEI7oDG02dVcBCDiXQxB84NCg7Oz+6gqd7e/UXLLe0Mc97g743TtwV64OCfai/4mBy7P/pUuzh3JcO6sKNAJGENRGXJweyThQ1uhoXV4xHDwO7t+l6+V0tmLt7U7T1gicmoHdmTsFgfNzSttAX4DzIQ6aT1bgrxv3PVokEnyCwHJQxifC8PPDZ/vWlx6Z0G80nPA3vRwt01BjYQsyO+nJjd0B2gOGkx8IHGU7cdrNh18Af1nv71Tb9fbex8uE2aG/nvP5/77SiYin/CN4G9ue3nkoHGDF15//qm6fm3Oe1/diXLx3Zw3+QU2/0QQ5dL20PrtQnS0Yk1pB39Eex7WekY4HeNc8sn+XjbO92wV+oedm9yb/c1222y9sQZuNUjvokiPDztX/s1OIn/x3OOxqOP81HeJ9306/GdgWWF5zRCemaU2jyJmNXaUT2+pT/iUCjCDYByZmOdIk4uL9o0Y7Ccy2sTpHtD6TkEP9dzvUnB2f88Zy1ySKS9DbNwZ2DHVtGawNQWKKHjWOvERnentV6mworpoanBwVtm11jMgCf1nwUIIgweV/6XJg6Hjyg8fp98s+1p1yWgctNG7/NbQmGJ+gI7G6jvD7KRJTdb5Ip4F6/uejPZzKY+7V27ftBee9ojOtD/qdjhYKOkei/14MNS+2U/twl632EvZ2LsGFb+uAhq4j9LXcsUpHt7lx+qvsUGjwd95LXbkUYx/0C9xNP+trvezjwG2qymD4efhMY+7yJ50GuubDcJgbR14KZz8LB12ol+Pi4QDLlGoLWTuD2klyG11W/L0LdajwEwN1Nd6Pr4maycgIgnog1mjm88VbWJ/vMeGGTyPwyMz0aQQhzDyJaYEPtFObuPxTAmy1oR64aGBfU6gqk9GgXS9d+ylSJ3N96HuMP8KpNq77sW5Cw8buiML5UyAyk97w38J9u+AAR+G6+ER/7H8o7lip228TSR2tUdfCvTt0R5eYGhyVFIqhZ3uvd5q13OGhXoLAThhr11snLdqEivhyOqL7Huu33dsaQeGWwNyfY27xT0+6dbEWwEGJkcCQsxxCroOuHjv8t9axXU7utI5a6PY5Njg3AiDdeo5yrfvI6Zi2sUPQtyzUtaWKd/jzKtpER/tvhNkaWh7iUGt37C2jXyfrJrYzC+3Rs1MQ3GFFT3xxq34PlV0r4o+jT0r3HyMhVd+o9qxm4H2jRYtWUeyMmwuhIl5qY9CpWoCEEjh2pxyuDpGT+pjjfIMRFXjOaNB1OJzzgh4tb1+mAxc+uiZQELprMrkFQUpbPcsd+CvAOvH6DZyZvwmp/pG6M1T2gjfgZUcyZSeHcOt2sH7eLp6oTTBf/VWvzxiin8nSXdrsNfBk7ZRe9nGwQEtMgb+sDS3U7Sq57gzw9G7aMS3xgRNk2Z+3LNLvRhA0LTWnPkrcNF1ANZ+2DOzcmjU7ithT6vhDbfut3UHbD3ZyG/9IwHa6hXO+2fQ63J/cYj8MnYf6Z/XyKlERLeITYdDpMOKCxjtnSyKc1mF3yqGSkP6crSciSgthe68LdrKS2zHb0Bx6uX63K72O3xj4/eg7ofdRerS+6We/hnzFZP38fH6rvs+T0sP7iEKZDRNT/ILAqTm7zXPOQI0Df6PfB5+mHcM2Iv5j2f6Pfpav4qDfeZw7jGaX3jWwhIjNJZPgleP0APGyz+DT6wPngdiy0Lv9DYQRBBESd9QNTCg/iTlZ89hQ1o4T2qcGC4Ij/gDf3u8fgThrtdjYAiBc5mH3kTD6L/p4KW11gtlxVvr5xR/A5Jv0pDiNaRoCuKQOmbiGyPGVMXAlQp39Lz2CbJ0Bp3nMsVwvGkEjiIT4RBhgRdI4haRdAj5jkBYE6V2C97X5y/rg67BH0Qmp3oMuLx/Pb17WFV6dZiH3dj6hYg2+OvTTJrq6amci2s9RiU74swd1dgc/YIwOOz7zSXj/Iv9+WxbqawrliN9HjI+gDlx3wiD6D9VT9nVvp6V+gCA49la4f7d/BGPbFZ2CwB751IQRBHHxMOZuPTKKT9TlEexwwFYdYZiVbhEqS9PQvLDnQuhxaOD6w64KLkHSUOxLpnRjYI/0wyX9pXUIzq2xtfHEVP9ERund4Jpv4dZl3tc98mK46N1Ap68tCGxBYwsTZ0dc34g5O/LOfo5BD+r++Is2XUGwKXb3Rn3ufQljDoPRCOpIh1b6RmqVFI+ISxBA4B9lO5JS2vJTdj7VSnGc7Sx2x4Of8nDkI/zhF+goAmdkhKH50v8EXeCvMbBNQ6FqR+0vHHaVnuTHPYlQbaR11M+Gqobjbtej67oeA/w5JnZmvy0IvEJx68qYe+H7hwLnsRDxVzoF777AXdusATGCoI60TtE/WXFFNW1SEskvKmfrnlLiRSiuqPY7joG98e1oAxCfyCWvzQFgw+HWDeXWCI6+KfJGxCfoiAeDoc7YgmA/1wiSW8Ppj9Z9vzOegP/doBPm4uLrJwQAWnXWwtI2ydlafkMka47+s37ZYcVeORxOjUDiYcSFkHl18HYNhBEEdSQ9Wf9kRWVVtE1N5P25m3l/7ma6tU1h654yVj54Gqu3F3JQz7Y8/3MBdycSkEZeLfHEQ/2rExoMDUE0ss/3B/oeC7d6JGnVlaQ0uMcxcc+Fb8OCt71rNNWX5NY6ycwrlNzpIL83P2omIRvjI6gjJw7pzEE923LTmANom+pPCtu6Rzv6jnv8e859cTbTV+1gjdKhXsoRvra33DIJhfMRGAzR4rwJOpY+0lDVlkok02866Tig9nmz69WOEPlEzo4/Ls4Igv2NtqmJTL7xWAZ2SaddWnBxsPwiHVc8Zck2fqg5hCsr/sqqAf66Kks6Ww6i/h6JKyEoq6xmU0E9yt0aDG4OuhBunBv1jmW/5pbFOhPb4MMIgn1gYOfQEQ2fLtQxwD/WHMzM7N2+9QvVIO0YrEPEzy0fLGT0E9P5cXUeqgkmrTAYWhTt+0Y3W72huOi9uk2etA8YQbAPDOseWMY4OUH/nD3apVLj6K9XbvPXCNq8U89olLOrhHfnbIyoY/96hS7Pe+Ubc5m6dFstWxsMhhbBkDP1/A6NgBEE+8CAjMCMx2MO0DHMRw0IDP1alqtDA7u2SWHzTm3iuf+z5dz96TJmZefXeh6nrNi6pzT0hgaDwVAPjCDYB0b2asftY/21Sp4ZN5K3/28UQ7oGmoxWby8iIz2Zow/oyCZLEGwv1M7lj+fn4EYpxWsz17G3rDLou7hYtu0aDIaoEFVBICKnicgqEckWkfEe348WkQUiUiUiza6IjYhw+ZH+6oNtUxM5flAGHVr5IzJsc9FR/TsyuEs62/aWsSG/mNXbdQyxHW1kk19UzqzsfB6espJ7/xc8kUackQMGg6GBiZogEJF44EXgdGAYcLGIuGvHbgKuAt6LVjuiTXpKcBhae4cgSLIEweCu6Zw+XJcKnjBzHRVVOox0zvqdzFyjJ6BQSpH58Lfc+sEiAJ8ZyUltHoXJi7dQXF5Vy1YGg8HgJ5oawSggWym1TilVAXwAnOvcQCm1QSm1BAgz/97+TUJ88E/YIc0vCMor9aVltE6md8c0hnRN57OFel7Vvh11PZPLX59LSUUVd1saQEGxDkEtLAvu0EsqQucfLMnZzc3vL+T+ycvreTXe1NQoXvphbXA5jSiSs6vEF3llMBiiSzQFQQ/AMYUXOda6OiMi14lIlohk5eXl1b5DE3Bwr3a+z7ZpSAQqqi1BkK5T+g/v24FiqzM/sLu/GN2EGet4b07gfKvegiD0aN/eftPOEmpqFNU1tUckKaXYUVgWdpv5m3bx2Fe/Mv7jJbUer6G44KWfuW3iYsqrTOKdwRBtmoWzWCk1QSmVqZTKzMjY/+J/F99/ChOv89c0sRPN7HIU4BcEmX391Qt7tPen+X+3MnimokIPZ3FxeeiO0TYJCXDev2dz4P1f1dr2yYu3MOof37Fg066Q29h+iUWbd9d6vIZi214tnLyEocFgaFiiKQhygV6O5Z7WuhZH29REUhL9qeKtkxO47eRBTPzDUb51nS1BcFDPdr51g7v4o4uW5gZXnyyuqKayOtBqFs7+v9MyKYnA4pw9lFlmqZoaRU2Nf3rNBxymo4WbdgMwZ93OkMe1zVFux3ZjYASBwRB9oikI5gEDRaSfiCQBFwGTo3i+/QYR4ZaTBzK0mz/hzDYX9engr59+/qE9uOWkgUH7O1mxJXDCeqePoKyyOiAhbWeJJQgIDC266q15HP/kdABuen8hb/20wfedrb18s2IbpSH8D85zhjNN1cbmnTqJri7sbUS/hMEQq0RNECilqoAbgWnASmCSUmq5iDwoIucAiMjhIpIDXAi8IiIN6+XcD7hpzAG0S0v0OZXjHPGfIsL5h4Z3m1z22hxaJfm1jWKrIy6vqmbIvV9x+r9m8uPqPOasK+Dxr1YBUFrp77h3FJYxY3Uem3eWBgiNMmsbu/NfsGk37/y8wff9+vxitlkagLPzn7/R24Tk9DMs3LTLp4E4GffKz9z96TJPk1co3LkUSqk67W8wGGonqmWolVJTgamudfc5Ps9Dm4xaLHecMpg7ThkcsG7yjcdQZJl4urTRMyCNHpTBjNXBjvDC8irOP7QH+UUVzFidx4695WRt2OkTLL9uK+TKN+b6IpAA8grLfZ+/d/ge7GgkgN0llXRtG8+uEv86pxnmxCd/AGDDo2cGaATz1u/kuIGBfprZ2flc+tocXrsik7TkeC55dQ7nH9qD1dsLee/aI2mTorWOLZZg2VVcSXpKcME+G6dm4mzTii17uX/yMuZt2MXcv51E5zYNP92iUop+d03lxhMP4M+nDq59B4OhBdAsnMUtjYN6tuPoAbocRUpiPN/fcTyvX5kZsE3HVkm0t8w2w7q14Z3/G8XJQzuzYuteLnj5Zz6YGxhhtMFRnTR3t78MxYqtftPSvPV+P8D6/GKOeuQ7vliylf6dWpEQJxSVV/mymp3YHXNaUjy5u8uoqVHMzs5nfb6egNt2Is/bsNOX+/DJglyW5e719D0UFJcHrXPiLKNRUFTOmCd/4NwXZ3PGczOZt0FrJFui5K+wtakXpmdH5fjRoqKqhtdmrgvyKRkMkWAEwX5A/4zWJLryEeLihKOt2kUDOrcGoJ0jP+GDeZvxYuywwMm+nT6GH1b5NY5py7exdU8ZJRXVdG6TTPd2qewqqSCvsJyHp6z0bZe9o4jtVgRPj3apfLwgh2Me+55LX5vDmKd+4Ke1+TwxTZukFOC2CNlmJecof6dDM7Epr6rmrk+WsGV3KRsdQm1dfjHr8otZ7IpYCuXP8OLe/y1j6L3hI6gmztvE9r1lPk1tX1iWuydImEabd37ewMNTVvK2w/8TDZbl7uFbqwiioeVgZijbT6mqruEfvxnO0K7pHGsJhN4d0sLuk9mnPc9ffAhDHJ3ecocgmJjlFx5Oh/He0irat0oiv6ic12etDzjmyU//6PvcuU0ya3YU+aKHlII/vDM/YPvyysAO2t42Z5e/c//3D2uZnV3AfWf7E82zNuzi/bmb2bK7jBMG+01PXtnVQIBJywulFGLVZfrPLxuD1jnZWVzBXz/Ws1o9/tuDwh43FLtLKnyC+qznZwFwzbH9PM8XDWwBFu2kP/vaNjx6ZlTPY2hcjEawH+EsVrerpJJ2aUncOGagT1uwBUHP9qn870/HMNJKYvvneSP44qZjefWKTJ+p6fhBujMtrazm4F7tSEsKMRMSOty0fVois7MLeGVG6JFs+7TgWa3KHaaImhrFrpLAjmiLZaZymnLmb9zFG7O1wFFKcfbzs3hz9gZAd/wbC0ponZxAm5QEX5E+N+EEwYJNu+h311Tmbww0S9nhtG6cGspfPJLmdJuKQ55vWe4eRj74DZ8tCoyODpcFblNRVcMuDw2priRYQQhVESQRNgRmXoyWhREE+xEf33A0c/52EmOHdeEPo/sHfd/bcgjX1CifEADdiQ/v0dZX46h/RmtevzLTl8Q2rFsbvrn9eC48rCd/PW1IwDHfvOpwXrr0sICyGKFonRysQNo1kwBem7We2a6y2t+t3MFrM9exdXdw+ewfV+exp7SSpbl7+HalNjfsKCxnbV4RfTqm0bF1Mtk7ijzbEq7znLk633duJ7tLvfepTbs47vHpHP/EDyG/X729EIDvfw08nz1Kf/bb1UxdutW33pktfedHiznkoW+o2kfbfrw1haJXtFY0KK00Gd9V1TUtJoLNCIL9iFbJCXRpk8KrV2Ry1xlDg763NYJhVmkK+5Fv59GJJ8TH8fJlhzIusxfjTxtCj3apPHHhwdxwQuDk2ycO6Uzvjmmex3BTWV17J5PlCi/N3V3Kw1NWMv4TbXrp1Np/nivfmMvbPwXmFRSVVzFzTT4H9WxHz/b+CX5OHhro+3jn543c8sFCzzbY7hZ3iY3dJd4PbTihYnfyAOvyipixOs8XemtjR3A5hSL4I56e/XYNf3x3AYs272bu+p0MvucrX6HBzxZtAXT0175gW6AaSyPw8vPEGrdOXMSIB76mpkbx7Lerm/VcIUYQNCM6tU7mvd8fwdPjDgb86nlyovffeFifDjx2wUG0dc2tvP6RM4K27dIm2fMYr13hj2ZyR6R0rSV8s1Pr4GP26xQ4mc8z36723DezT3t6Okpw/PO84QG+gx2F5Xy2aIvP96CU8jnGbRNQeVVNgAljd0kl2/aUUVFVw3crt/u+C6cRzFrj13Ae/2oVV7wxl3NemBWwje0Xcf8+ReVVAaP/37w4m9+98jMAU5ZoDaFXB32N//puDbuKK5i+akeQQIkE23leX81izroCjnn0e885MLzYVRz9kXDu7lL6jp8SZOLbX/jC+g9nr83n2W/XeJaNby4YQdDMOPqATr64/LMO0mWte7RLDbdLECLCFzcdy5Sbj/WtO3FI56DtPv3j0ZzsiEJyTs3ZrW0K395xvC/E1YtLjugdsDx2WBefucpNfJwwrFsbX1nvYw7oRM/2fud4Rnoyz118SNB+Y578kbdmr6ffXVM547mZ/Ly2gPwiHZ66ZXdpgBbwxZItHPnId5z9/CyueTuLr5bpaT93Wp2aW1t65pvVLMvdQ6fWSSTECb+sLwD0REPOEN29ZXaSn0sQlFVRUOQtZHJ2lTJnXYFvoqFvVmzn9kmLuPrNeSGFYzhsU01RmFpUXnw0P4fpq3bw7LdryN1dyvwNoWtOOQklPNflFfHrtr2e39WFSVmbOebR7wF495dNtWzdtNgBGUty9vh8YnVlzfbCgEFHY2Oihpox1x7Xn3GH96ZtaujOOBTDe7QNWB7UJZ2De7bl5KFdeOunDRQUV/iSvsZl9qJdWiLXHtefzD7tefnHtdw+djCtkxNYeN8p9B0/xXecZX8/lS+XbuX7X3dwiOXHOKp/R9679ghEhLssE5GbP58ymPSUBI4flEFaUjwdWycHCDgRIT05gfMP7cGvWwt9+REV1TU88PkK33bLt+zxJdR9vWI7X6/4xvfdu1Z111WWuWfuhp2cPqIbu0oqSEmMo7tLoP7ruzUAjBnSmewdRQGO66+WbePMEd3ILyr3lcGYt2FngJmpqLwyILnPyazsfN80pScMzuCHVXlMt8J752/cxZ7SSpbl7uGYAzpRWV3Dhvxi8grLmbthJ7eePCjoeHYNqrrYrJVS/PnDxb5rBNgQxinuZOPOEq57J4v7zzkw4H8a85SOMtuXqKK8wnL+5rhP9ne3dJYlPHcUlnPikz+w6uG6zzM89pkZQNNFYxlB0IwRkXoJgVB8dqPWECZmbYZi/+xqj13gD6nM7NuB1/p2CNjvhhMG0LFVEqce2JXWyQlcmNmLCzN7UVhWycE923L3mUN9YZRjhnTmfVcyXJc2yUGjcYAj++u5nzP76IqtIsLTvxtJflE5N723kIfPG84Hczfx6kx/yOtrM9fT2jFZ0IgebdlQUOyz14/L7OULo527ficFReVsKiihQ1pSSIf52Qd348OsHDbtLKFjqyQy0pN59MuVTJixlu17y31aUVllDYc85Bc8ReXVPu3EiTuL/MDubZi5Jt/n01ifX8yjX/7K+3M38dH1R/H1iu1McERz/WZkD9qnJbEuv4hDeuvfxo5QKiyr4t05G+mQlsTpI7oFnbuyusYXhWZrRAA/rdVCafmWvazPL+aRqSt59qKRbNldRlJ8HL07pgWYnSbO28Sy3L2kJsXzr4uCNbWKqhrfpEyRsj6/mKlLt/LKj2sDfB01ISKUisqrSIwXkhNCR8Q1BnMtTRGCtcK6EirEOdoYQWAI4rUrM3n7pw1BI+RQuCORbNJTEn3CxWbssC7M+uuJHPvYdB44exhnHtQ9yPlq07VtCisfPI2qmsCHq1PrZN63yn47q7mO6NFWV3Hdq81S9501jJTEeJRSDLz7Sy7M7MnYYV18gmD5lr0c9vC3AJx9cPeAKUbPHNGNKUu3MqpvB847pCc/r9UPe0Z6Mmcd1I0nv15NaUU1rZLig0JmbT5fvCUoURC0MDr2gI5UViuemLaKrm1T6dU+1ZcdnldYzro8HS11wcs/B+3/ycJcPszazNY9ZSy+/xT+PT2bT63JjvaWVXL3p9pWveHRM8krLCdOoGPrZHYVV3DIQ9/w4LkHcvZB3bnh3QW+Y9p+lewdRdzzv6XMzi5g8qItPPD5csoqa/jmttEB4bDLcrVG5psDo6AkoKz6+vxiBrvm7rYJ1dndOnFRUOIgQFW1oqyymtXbC/lpbQHXH68HDcPvn8bhfdvz19OGMKRbG19U27wNO0mKjwuYIyTSNtSF+Dihukb5TIM2v6wrYHiPtp5RdrVRUlFNK8d+2TsKWbBxN787vFeYvfYdIwgMQQzp2oZHzq9fYlUk9GyfFrEKnJoUD4Qe8Z04pDNH9e/IPWcN5YDOrTnruVms2VHEpUf09pUGFxF+feg04uPEs5R2ekoC9501LCDX4uaTBjJl6VZ+e5guCjjIKhleWFbFdaMHMHpQBgd0bs3c9Tu56s15AEz/8wkkJ8Rx0YRf2LSzhB+tUX9KYhxvXT2Kiyb8AmiH+ZmWf+foAR0Z0aMts9fks6GghJ7tU7X/YH1oB+lzlsnK/uxMAnQmEJ7379m+MuN/PW2IL8v70S9/ZUhXv7+nTUqCrzPbWFDsq3813mGe+f07Wb6M7/SUBJ8AmLkmj1lr8rns9TkBbXxhejbPXTTS19k+/c1qnvtuDX07prF1Txm/P64fFx3em15WJNzfP1/uKQQApizdyhRH+O0VR/XxVdidt2EXF7z8M6cM68KEKzIpLq/iQkt42vfYntJK0pMTfAUfv12xnX9+uZLyyhpmjx/jO25ZZXVAOXmbwrJKxj49g7vOGMK5I/1FIhMsQeDmogm/cOZB3fjbGUPZvreMQy2tbfPOEh776leeuOBg674OZmdxRYAgOP/fP7G3rIpzRnb3bFtDYQSBoVnTOjnBpx0ATPrDUSzO2R0w+xv4Qzy7tdWd3NhhXeif0Yq3Zm/gl7tO8j18391xPFt2lzK4azrz7j7Z59w+d2QPHp6yktSkeJIS4nyayGF9/BMN2RFRM/5yos9v8v0dx6OAARmt+fq20fz3l40BI2XbtPOH4/vz1fJt3HLSQL5YspUfV+dx6oFd2F1SGSAU/nXRSF6duY7MPh347y8bgzLBndhCAOCxr371fS6pqOaG/+qM8Jl/OZHbJi7yhf3uKqkM0HBGD8rgp+z8gLIfvz20py8zvbJaBQmBpIQ4Pl+8hUWbdzHpD0dRWaV8wqu8qobyqhpenL6WD+ZuZv69YwF8CYWRsLGgJKgDnrtB/0bOfI2yymo+WZDLA58vJ05gePe2/OviQ/j9O1m+bYrLq2iVnMDni7dwx4eL+frW0cTHiU9Arc8v5tRnZlBRXcMtHyzi4SkrufuMoXy1bFtYM9BXy7ZRUVXDj6vyOHZgJw7u2c6XL3PGiG6cYZntlFIBZVl2lVT4zg3+QIRNO0t8g5FoYASBoUXRvlUSJwwOjoCyEREW3TdWd+jxcdx28qCAkdaAjNYMyNC1nZwRThnpyfz3miN84Z426SmJxAlBZojBXdI5tE97+lvHAq1VPHjucM92HdK7PQvuHUv7tEQO6d2OH5/O48DubUlPSQgQBOeO7OEblf66bS+/rNvJiYMzfI7mO08dzFNfr+LJCw+mpKKaQ3u3543Z6/lofuD8z3Yl2h7tUhncNZ2sjbsY3qONz+Rj8/bVh3PKMzNY40jscyYznjSkM9+5Eulm/uVE7v50Gd+u3M5z32UzIEMLyFevyOTI/h0Y8cDXvja8Pmt9SNNgKE7/10wuHhVoKrEzqxc4hN/fPlnKJwv92d5ZG3cFOKFBV9kd1a8Dv24rpKKqhnNemMXesiqevPBgzj64G3dMWuSbbha02e7WiYtqbWN1jeIbqybT97/uCEg2/OO7C7j++AEszd1NRVVNQDSc/b/sLK4I8P+d/q+Z/PGEAUGVjBsKaW6p4pmZmSorK6v2DQ2GRqKsspr4OAnyB+yLHXpTQQld26ZQWFbJYQ9/S+8OaTz0m+G+0iEAr/y4lke+/JWXLzuM6/87n4z0ZObdfXKAQ9gmv6icTMsfcuepg3nuuzUcc0An3rjqcIrLq/h5bQFd2qRw9guzOO+QHlxyRG8qq2s4ekAn7vxwMR/Oz+Hsg7sTL/DAOQfy6cJcOrZO5rgDOvHJwlw+W5TLkpw9/PrQaT7Beu//lvH+3E10aZNCq+R4vr7teADOfWEWi3OCZ+Rzk5oYz3Wj+/ObQ3rQKimeN3/awEs/rA25/dv/N4pHpq5kZ3EFO1zRWs9ffAg3va8TEONE14FyBhmE476zhnFQz7Zc9vqckGVKbjzxAD5ZkMOWPWX8LrMnk7K04B3WrU1ABWAv+nZM8/mHbjt5ENOWb2PF1r0ce0AnX2SZzX+vOYJjB3aKqN1uRGS+UirT8zsjCAyG/Ztte8ro0CopKAqnukbx89oCjjmgI+vyi0lPSaBzeugkv6venEtJeTWTrj8q5DYFReV0aJUUIMD2lFbywOTlXHNsv6CwY5vCskq27y3ngM5+DWjrnlKOekTnAvztjCFcN1o7eXcVV5C7u9RXwM7JUf078vb/jQoZcdR3/BT6dExjSNd0Lj2iD+3SEpm7ficTZqzzdf63jx3E+vxiPl2YyxkjunL98QM4qGc7vly6lRveXcB1o/tz1+lDfJrOkK7ppCbF+0xpd58xlA6tknhxejY5u0v5efwYOrZOpryqmvfnbAoIV7aZcPlhHNanPQ9+sYK/n3MgT329mq17ynh63MH8lF3A9f8NLM7YNjWRscO6MCCjdYDZLhSZfdqTlBDHDScMCJoPJFKMIDAYDE3CL+sK+GrZNv52xtCgzn1Z7h4Ky6rISE9iQEZr/vPLRk4Z1pWubUMLsz0llaQkxQWFjJZVVjMpazOV1YpLj+hNQpywq6QyKIFxfX4xvTukER8nlFdVk7ur1Ge+u/G9BazNK2bqzcciIlRV17C7tDIgQ94p3ACeHTeSWycuYvqfTwjKmrdRSvG/Rbn0aJfGjNV5pCXHc/XR/Xzh2Ze9Pkf7kFZsY/vech4690AuP6ovny3K5bWZ67n/7GEc3KudZwRaXWgyQSAipwH/Qod9vKaUetT1fTLwDnAYUACMU0ptCHdMIwgMBkM0UEqhVOB0sl7YVWZ7tk/jsD7tPU1xdT2viLA2r4jPFm3hpjEH7HOn70WTCAIRiQdWA2OBHPRk9hcrpVY4tvkjcJBS6noRuQg4Tyk1LtxxjSAwGAyGuhNOEESz1tAoIFsptU4pVQF8AJzr2uZc4G3r80fASdIUaXUGg8EQw0RTEPQAnPMp5ljrPLdRSlUBe4COUWyTwWAwGFw0i+qjInKdiGSJSFZeXl7tOxgMBoMhYqIpCHIBZ9ZHT2ud5zYikgC0RTuNA1BKTVBKZSqlMjMy6hc6ZTAYDAZvoikI5gEDRaSfiCQBFwGTXdtMBq60Pl8AfK+aWzyrwWAwNHOiVmJCKVUlIjcC09Dho28opZaLyINAllJqMvA68B8RyQZ2ooWFwWAwGBqRqNYaUkpNBaa61t3n+FwGXBjNNhgMBoMhPM3CWWwwGAyG6NHsSkyISB6wsZ67dwKabmLQpsFcc2xgrjk22Jdr7qOU8oy2aXaCYF8QkaxQmXUtFXPNsYG55tggWtdsTEMGg8EQ4xhBYDAYDDFOrAmCCU3dgCbAXHNsYK45NojKNceUj8BgMBgMwcSaRmAwGAwGFzEjCETkNBFZJSLZIjK+qdvTUIjIGyKyQ0SWOdZ1EJFvRGSN9d7eWi8i8pz1GywRkUObruX1R0R6ich0EVkhIstF5BZrfYu9bhFJEZG5IrLYuua/W+v7icgc69omWuVcEJFkaznb+r5vk15APRGReBFZKCJfWMst+noBRGSDiCwVkUUikmWti+q9HROCwJok50XgdGAYcLGIDGvaVjUYbwGnudaNB75TSg0EvrOWQV//QOt1HfBSI7WxoakC7lBKDQOOBP5k/Z8t+brLgTFKqYOBkcBpInIk8BjwjFLqAGAXcI21/TXALmv9M9Z2zZFbgJWO5ZZ+vTYnKqVGOkJFo3tv6+nZWvYLOAqY5li+C7irqdvVgNfXF1jmWF4FdLM+dwNWWZ9fQc8SF7Rdc34Bn6FnwouJ6wbSgAXAEejkogRrve8+R9f4Osr6nGBtJ03d9jpeZ0+r0xsDfAFIS75ex3VvADq51kX13o4JjYDIJslpSXRRSm21Pm8DulifW9zvYJkADgHm0MKv2zKTLAJ2AN8Aa4HdSk/qBIHX1RImfXoW+AtQYy13pGVfr40CvhaR+SJynbUuqvd2VIvOGZoepZQSkRYZGiYirYGPgVuVUnuds5y2xOtWSlUDI0WkHfApMKRpWxQ9ROQsYIdSar6InNDEzWlsjlVK5YpIZ+AbEfnV+WU07u1Y0QgimSSnJbFdRLoBWO87rPUt5ncQkUS0EHhXKfWJtbrFXzeAUmo3MB1tGmlnTeoEgdcV0aRP+zHHAOeIyAb0fOdjgH/Rcq/Xh1Iq13rfgRb4o4jyvR0rgiCSSXJaEs4Jf65E29Dt9VdYkQZHAnsc6mazQfTQ/3VgpVLqacdXLfa6RSTD0gQQkVS0T2QlWiBcYG3mvuZmO+mTUuoupVRPpVRf9PP6vVLqUlro9dqISCsRSbc/A6cAy4j2vd3UjpFGdMCcAaxG21Xvbur2NOB1vQ9sBSrR9sFr0LbR74A1wLdAB2tbQUdPrQWWAplN3f56XvOxaDvqEmCR9TqjJV83cBCw0LrmZcB91vr+wFwgG/gQSLbWp1jL2db3/Zv6Gvbh2k8AvoiF67Wub7H1Wm73VdG+t01mscFgMMQ4sWIaMhgMBkMIjCAwGAyGGMcIAoPBYIhxjCAwGAyGGMcIAoPBYIhxjCAwtAhERInIU47lP4vIA/twvGOtap+/Wq/rHN9lWBUuF4rIca79fhBd5XaR9fqovm0I0a4NItKpIY9pMJgSE4aWQjlwvog8opTK35cDiUhX4D3gN0qpBVbHO01EcpVSU4CTgKVKqd+HOMSlSqmsfWmDwdCYGI3A0FKoQk/jd5v7CxHpKyLfW/XavxOR3rUc60/AW0qpBQCWYPkLMF5ERgKPA+daI/7USBonIm+JyMsikiUiq61aOvY8A29a9ecXisiJ1vp4EXlSRJZZ7b7JcbibRGSBtc8Qa/vjHVrIQjs71WCIBCMIDC2JF4FLRaSta/3zwNtKqYOAd4HnajnOgcB817os4ECl1CLgPmCi0vXiSz32f9fRKT/hWN8XXTfmTOBlEUlBCx2llBoBXAy8ba2/ztp+pKPdNvlKqUPRtef/bK37M/AnpdRI4DjAq10GgydGEBhaDEqpvcA7wM2ur45Cm3oA/oMuURFNLrWExEil1J2O9ZOUUjVKqTXAOnT10GOB/wIopX4FNgKDgJOBV5RVclkptdNxHLvI3ny0sACYDTwtIjcD7ZS/VLPBUCtGEBhaGs+i6y212odjrAAOc607DF37ZV9w13Opb32Xcuu9GsvPp5R6FPg9kArMtk1GBkMkGEFgaFFYI+dJ+KcwBPgJXcES4FJgZi2HeRG4yvIHICId0VMfPr6PzbtQROJEZAC6uNgqqy2XWucZBPS21n8D/MEuuSwiHcIdWEQGKKWWKqUeQ1fbNYLAEDFGEBhaIk8BzhDLm4CrRWQJcDl6HlxE5HoRud69s9JlfC8DXrUmBfkJeEMp9XmE53f6CL51rN+Eroz5JXC9UqoM+DcQJyJLgYnAVUqpcuA1a/slIrIYuKSWc95qO5bRlWi/jLCtBoOpPmowNAYi8ha6lHKD5hUYDA2B0QgMBoMhxjEagcFgMMQ4RiMwGAyGGMcIAoPBYIhxjCAwGAyGGMcIAoPBYIhxjCAwGAyGGMcIAoPBYIhx/h+fVCq+0/xqkgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "opt = optimizers.Adam(lr=0.001,decay = 0.0001)\n",
    "print('Train...')\n",
    "# model.compile(optimizer = opt , loss=\"mse\")\n",
    "model.compile(optimizer = \"adam\" , loss=\"mse\")\n",
    "history = model.fit([x_train,x_train], y_train, epochs = 500, batch_size=8, validation_split=0.1, shuffle=True)\n",
    "# history = model.fit(x_train, y_train, epochs = 500, batch_size=6, validation_split=0.1, shuffle=True)\n",
    "model.summary()\n",
    "#Save Model\n",
    "model.save('GRU_Single_Attention_model_Soho.h5')  # creates a HDF5 file \n",
    "del model\n",
    "\n",
    "custom_ob = {'LayerNormalization': LayerNormalization , 'SeqSelfAttention':SeqSelfAttention}\n",
    "model = load_model('GRU_Single_Attention_model_Soho.h5', custom_objects=custom_ob)\n",
    "t1 = time.time()\n",
    "# y_pred = model.predict([x_test,x_test])\n",
    "y_pred2 = model.predict(x_test)\n",
    "y_pred = model.predict(x_train)\n",
    "t2 = time.time()\n",
    "print('Predict time: ',t2-t1)\n",
    "y_pred = scaler.inverse_transform(y_pred)#Undo scaling\n",
    "rmse_lstm2 = np.sqrt(mean_squared_error(y_test, y_pred2))\n",
    "rmse_lstm = np.sqrt(mean_squared_error(y_train, y_pred))\n",
    "print('RMSE: ',rmse_lstm)\n",
    "print('RMSE2: ',rmse_lstm2)\n",
    "mae = mean_absolute_error(y_test, y_pred2)\n",
    "mae = mean_absolute_error(y_train, y_pred)\n",
    "print('MAE: ',mae)\n",
    "print('MAE2: ',mae)\n",
    "# r22 =  r2_score(y_test, y_pred2)\n",
    "# r2 =  r2_score(y_train, y_pred)\n",
    "# print('R-square: ',r2)\n",
    "# print('R-square2: ',r22)\n",
    "\n",
    "# n = len(y_test)\n",
    "# p = 12\n",
    "# Adj_r2 = 1-(1-r2)*(n-1)/(n-p-1)\n",
    "# Adj_r22 = 1-(1-r22)*(n-1)/(n-p-1)\n",
    "# print('Adj R-square: ',Adj_r2)\n",
    "# print('Adj R-square2: ',Adj_r22)\n",
    "\n",
    "plt.plot(history.history[\"loss\"],label=\"loss\")\n",
    "plt.plot(history.history[\"val_loss\"],label=\"val_loss\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"No. Of Epochs\")\n",
    "plt.ylabel(\"mse score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ce396d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 108 samples, validate on 12 samples\n",
      "Epoch 1/200\n",
      "108/108 [==============================] - 2s 19ms/step - loss: 1.3693 - val_loss: 1.4978\n",
      "Epoch 2/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.3197 - val_loss: 1.4289\n",
      "Epoch 3/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.2690 - val_loss: 1.3627\n",
      "Epoch 4/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.2274 - val_loss: 1.3007\n",
      "Epoch 5/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.1758 - val_loss: 1.2399\n",
      "Epoch 6/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.1433 - val_loss: 1.1915\n",
      "Epoch 7/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.1033 - val_loss: 1.1518\n",
      "Epoch 8/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.0497 - val_loss: 1.1118\n",
      "Epoch 9/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.0127 - val_loss: 1.0684\n",
      "Epoch 10/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.9805 - val_loss: 1.0321\n",
      "Epoch 11/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.9496 - val_loss: 1.0013\n",
      "Epoch 12/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.9059 - val_loss: 0.9587\n",
      "Epoch 13/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.8819 - val_loss: 0.9290\n",
      "Epoch 14/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.8605 - val_loss: 0.9034\n",
      "Epoch 15/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.8184 - val_loss: 0.8798\n",
      "Epoch 16/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.8044 - val_loss: 0.8551\n",
      "Epoch 17/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.7836 - val_loss: 0.8283\n",
      "Epoch 18/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.7627 - val_loss: 0.8087\n",
      "Epoch 19/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.7393 - val_loss: 0.7900\n",
      "Epoch 20/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.7197 - val_loss: 0.7754\n",
      "Epoch 21/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.7035 - val_loss: 0.7592\n",
      "Epoch 22/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.6888 - val_loss: 0.7386\n",
      "Epoch 23/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.6863 - val_loss: 0.7243\n",
      "Epoch 24/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.6634 - val_loss: 0.7094\n",
      "Epoch 25/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.6466 - val_loss: 0.6944\n",
      "Epoch 26/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.6303 - val_loss: 0.6802\n",
      "Epoch 27/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.6154 - val_loss: 0.6679\n",
      "Epoch 28/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.6238 - val_loss: 0.6563\n",
      "Epoch 29/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.6039 - val_loss: 0.6440\n",
      "Epoch 30/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.5840 - val_loss: 0.6316\n",
      "Epoch 31/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.5837 - val_loss: 0.6211\n",
      "Epoch 32/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.5638 - val_loss: 0.6106\n",
      "Epoch 33/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.5530 - val_loss: 0.6026\n",
      "Epoch 34/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.5497 - val_loss: 0.5927\n",
      "Epoch 35/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.5431 - val_loss: 0.5854\n",
      "Epoch 36/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.5329 - val_loss: 0.5778\n",
      "Epoch 37/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.5158 - val_loss: 0.5680\n",
      "Epoch 38/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.5319 - val_loss: 0.5641\n",
      "Epoch 39/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.5150 - val_loss: 0.5565\n",
      "Epoch 40/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4935 - val_loss: 0.5498\n",
      "Epoch 41/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4984 - val_loss: 0.5420\n",
      "Epoch 42/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4847 - val_loss: 0.5313\n",
      "Epoch 43/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4886 - val_loss: 0.5289\n",
      "Epoch 44/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.4838 - val_loss: 0.5199\n",
      "Epoch 45/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4635 - val_loss: 0.5108\n",
      "Epoch 46/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4667 - val_loss: 0.5083\n",
      "Epoch 47/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4656 - val_loss: 0.5086\n",
      "Epoch 48/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4675 - val_loss: 0.5014\n",
      "Epoch 49/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4569 - val_loss: 0.5015\n",
      "Epoch 50/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4509 - val_loss: 0.5004\n",
      "Epoch 51/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4455 - val_loss: 0.4996\n",
      "Epoch 52/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4424 - val_loss: 0.4919\n",
      "Epoch 53/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4343 - val_loss: 0.4904\n",
      "Epoch 54/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4348 - val_loss: 0.4787\n",
      "Epoch 55/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4273 - val_loss: 0.4788\n",
      "Epoch 56/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4353 - val_loss: 0.4853\n",
      "Epoch 57/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4293 - val_loss: 0.4688\n",
      "Epoch 58/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4165 - val_loss: 0.4590\n",
      "Epoch 59/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4267 - val_loss: 0.4581\n",
      "Epoch 60/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4097 - val_loss: 0.4528\n",
      "Epoch 61/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3997 - val_loss: 0.4505\n",
      "Epoch 62/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3967 - val_loss: 0.4560\n",
      "Epoch 63/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3993 - val_loss: 0.4512\n",
      "Epoch 64/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4011 - val_loss: 0.4398\n",
      "Epoch 65/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3862 - val_loss: 0.4374\n",
      "Epoch 66/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3974 - val_loss: 0.4416\n",
      "Epoch 67/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3762 - val_loss: 0.4495\n",
      "Epoch 68/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3797 - val_loss: 0.4378\n",
      "Epoch 69/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3781 - val_loss: 0.4374\n",
      "Epoch 70/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3758 - val_loss: 0.4413\n",
      "Epoch 71/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3739 - val_loss: 0.4298\n",
      "Epoch 72/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3637 - val_loss: 0.4248\n",
      "Epoch 73/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3752 - val_loss: 0.4263\n",
      "Epoch 74/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3696 - val_loss: 0.4213\n",
      "Epoch 75/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3676 - val_loss: 0.4313\n",
      "Epoch 76/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3671 - val_loss: 0.4326\n",
      "Epoch 77/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3583 - val_loss: 0.4233\n",
      "Epoch 78/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3554 - val_loss: 0.4224\n",
      "Epoch 79/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3496 - val_loss: 0.4349\n",
      "Epoch 80/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3447 - val_loss: 0.4248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3670 - val_loss: 0.4028\n",
      "Epoch 82/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3426 - val_loss: 0.4158\n",
      "Epoch 83/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3251 - val_loss: 0.4113\n",
      "Epoch 84/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3390 - val_loss: 0.4071\n",
      "Epoch 85/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3366 - val_loss: 0.3943\n",
      "Epoch 86/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3185 - val_loss: 0.4045\n",
      "Epoch 87/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3398 - val_loss: 0.3892\n",
      "Epoch 88/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3160 - val_loss: 0.3863\n",
      "Epoch 89/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3163 - val_loss: 0.4119\n",
      "Epoch 90/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2994 - val_loss: 0.4064\n",
      "Epoch 91/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3149 - val_loss: 0.3974\n",
      "Epoch 92/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.3106 - val_loss: 0.3832\n",
      "Epoch 93/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3125 - val_loss: 0.4048\n",
      "Epoch 94/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3143 - val_loss: 0.4049\n",
      "Epoch 95/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2944 - val_loss: 0.3931\n",
      "Epoch 96/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2967 - val_loss: 0.3951\n",
      "Epoch 97/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2990 - val_loss: 0.3667\n",
      "Epoch 98/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.2924 - val_loss: 0.4081\n",
      "Epoch 99/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2920 - val_loss: 0.3651\n",
      "Epoch 100/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.2916 - val_loss: 0.3727\n",
      "Epoch 101/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.2746 - val_loss: 0.3809\n",
      "Epoch 102/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2821 - val_loss: 0.3790\n",
      "Epoch 103/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2755 - val_loss: 0.3687\n",
      "Epoch 104/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2679 - val_loss: 0.3912\n",
      "Epoch 105/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2934 - val_loss: 0.3472\n",
      "Epoch 106/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2714 - val_loss: 0.3633\n",
      "Epoch 107/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2595 - val_loss: 0.3621\n",
      "Epoch 108/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2596 - val_loss: 0.3735\n",
      "Epoch 109/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2663 - val_loss: 0.3777\n",
      "Epoch 110/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2687 - val_loss: 0.3800\n",
      "Epoch 111/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2439 - val_loss: 0.3595\n",
      "Epoch 112/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2675 - val_loss: 0.3538\n",
      "Epoch 113/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2652 - val_loss: 0.3969\n",
      "Epoch 114/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2509 - val_loss: 0.3680\n",
      "Epoch 115/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2427 - val_loss: 0.3653\n",
      "Epoch 116/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2624 - val_loss: 0.3681\n",
      "Epoch 117/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2411 - val_loss: 0.3480\n",
      "Epoch 118/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2477 - val_loss: 0.3366\n",
      "Epoch 119/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2426 - val_loss: 0.4296\n",
      "Epoch 120/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2394 - val_loss: 0.3433\n",
      "Epoch 121/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2370 - val_loss: 0.3678\n",
      "Epoch 122/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2252 - val_loss: 0.3956\n",
      "Epoch 123/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2428 - val_loss: 0.3838\n",
      "Epoch 124/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2381 - val_loss: 0.3722\n",
      "Epoch 125/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2341 - val_loss: 0.3567\n",
      "Epoch 126/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2314 - val_loss: 0.3617\n",
      "Epoch 127/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2466 - val_loss: 0.3386\n",
      "Epoch 128/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2412 - val_loss: 0.3429\n",
      "Epoch 129/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2194 - val_loss: 0.3426\n",
      "Epoch 130/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2126 - val_loss: 0.3334\n",
      "Epoch 131/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2126 - val_loss: 0.3625\n",
      "Epoch 132/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2219 - val_loss: 0.3779\n",
      "Epoch 133/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2319 - val_loss: 0.3413\n",
      "Epoch 134/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2357 - val_loss: 0.3232\n",
      "Epoch 135/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2031 - val_loss: 0.3587\n",
      "Epoch 136/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2081 - val_loss: 0.3392\n",
      "Epoch 137/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2279 - val_loss: 0.3512\n",
      "Epoch 138/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2229 - val_loss: 0.3504\n",
      "Epoch 139/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2238 - val_loss: 0.3445\n",
      "Epoch 140/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2245 - val_loss: 0.3678\n",
      "Epoch 141/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2066 - val_loss: 0.3610\n",
      "Epoch 142/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2085 - val_loss: 0.3523\n",
      "Epoch 143/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.1978 - val_loss: 0.4358\n",
      "Epoch 144/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2039 - val_loss: 0.4139\n",
      "Epoch 145/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.1950 - val_loss: 0.3286\n",
      "Epoch 146/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2271 - val_loss: 0.3290\n",
      "Epoch 147/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2013 - val_loss: 0.3493\n",
      "Epoch 148/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2199 - val_loss: 0.3817\n",
      "Epoch 149/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2052 - val_loss: 0.3567\n",
      "Epoch 150/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.1931 - val_loss: 0.3468\n",
      "Epoch 151/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.1944 - val_loss: 0.3492\n",
      "Epoch 152/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2051 - val_loss: 0.3309\n",
      "Epoch 153/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.1846 - val_loss: 0.3169\n",
      "Epoch 154/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.1884 - val_loss: 0.3942\n",
      "Epoch 155/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.1933 - val_loss: 0.3578\n",
      "Epoch 156/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2059 - val_loss: 0.3729\n",
      "Epoch 157/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.1788 - val_loss: 0.3705\n",
      "Epoch 158/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2100 - val_loss: 0.3496\n",
      "Epoch 159/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.1943 - val_loss: 0.3456\n",
      "Epoch 160/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 0s 4ms/step - loss: 0.1934 - val_loss: 0.3378\n",
      "Epoch 161/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2101 - val_loss: 0.3046\n",
      "Epoch 162/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2055 - val_loss: 0.3844\n",
      "Epoch 163/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2317 - val_loss: 0.3006\n",
      "Epoch 164/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.1845 - val_loss: 0.3166\n",
      "Epoch 165/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.1848 - val_loss: 0.2999\n",
      "Epoch 166/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.1935 - val_loss: 0.3110\n",
      "Epoch 167/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.1862 - val_loss: 0.3803\n",
      "Epoch 168/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2015 - val_loss: 0.3059\n",
      "Epoch 169/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.1727 - val_loss: 0.3061\n",
      "Epoch 170/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2111 - val_loss: 0.3104\n",
      "Epoch 171/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.1865 - val_loss: 0.3814\n",
      "Epoch 172/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.1967 - val_loss: 0.3022\n",
      "Epoch 173/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2006 - val_loss: 0.3418\n",
      "Epoch 174/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.1771 - val_loss: 0.2921\n",
      "Epoch 175/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.1820 - val_loss: 0.3426\n",
      "Epoch 176/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.1876 - val_loss: 0.3320\n",
      "Epoch 177/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.1747 - val_loss: 0.3121\n",
      "Epoch 178/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.1665 - val_loss: 0.3130\n",
      "Epoch 179/200\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.187 - 0s 4ms/step - loss: 0.1816 - val_loss: 0.3060\n",
      "Epoch 180/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.1854 - val_loss: 0.3262\n",
      "Epoch 181/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.1747 - val_loss: 0.3865\n",
      "Epoch 182/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.1782 - val_loss: 0.3308\n",
      "Epoch 183/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.1939 - val_loss: 0.3664\n",
      "Epoch 184/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.1759 - val_loss: 0.3106\n",
      "Epoch 185/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.1823 - val_loss: 0.3201\n",
      "Epoch 186/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.1716 - val_loss: 0.2938\n",
      "Epoch 187/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.1733 - val_loss: 0.3658\n",
      "Epoch 188/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.1650 - val_loss: 0.3251\n",
      "Epoch 189/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.1702 - val_loss: 0.2965\n",
      "Epoch 190/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.1841 - val_loss: 0.3014\n",
      "Epoch 191/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.1802 - val_loss: 0.3580\n",
      "Epoch 192/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.1828 - val_loss: 0.2988\n",
      "Epoch 193/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.1757 - val_loss: 0.3326\n",
      "Epoch 194/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.1747 - val_loss: 0.3054\n",
      "Epoch 195/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.1862 - val_loss: 0.3037\n",
      "Epoch 196/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.1766 - val_loss: 0.3106\n",
      "Epoch 197/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.1738 - val_loss: 0.3257\n",
      "Epoch 198/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.1856 - val_loss: 0.3000\n",
      "Epoch 199/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.1656 - val_loss: 0.3338\n",
      "Epoch 200/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.1826 - val_loss: 0.3124\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_8 (Bidirection (None, 24, 12)            684       \n",
      "_________________________________________________________________\n",
      "layer_normalization_6 (Layer (None, 24, 12)            24        \n",
      "_________________________________________________________________\n",
      "bidirectional_9 (Bidirection (None, 12)                684       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 1,405\n",
      "Trainable params: 1,405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Saved\n",
      "Predict time:  0.38297486305236816\n",
      "RMSE:  16.42297523001503\n",
      "RMSE2:  15.888377775663942\n",
      "MAE:  13.09981512886579\n",
      "MAE2:  13.09981512886579\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'mse score')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "init = glorot_normal(seed=None) # 給 LSTM\n",
    "init_d = RandomUniform(minval=-0.05, maxval=0.05) # 給 Dense layer\n",
    "nadam = optimizers.Nadam(lr=0.0015,clipvalue=0.5)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(GRU(6, kernel_initializer=init ,return_sequences = True,kernel_regularizer=regularizers.l2(0.01)\n",
    "                             ,recurrent_regularizer = regularizers.l2(0.01) ,input_shape=(x_train.shape[1],x_train.shape[2]))))\n",
    "model.add(LayerNormalization())\n",
    "model.add(Bidirectional(GRU(6,kernel_initializer=init,kernel_regularizer=regularizers.l2(0.01),recurrent_regularizer = regularizers.l2(0.01))))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Dense(units=1, kernel_initializer=init_d))\n",
    "model.compile(optimizer = nadam , loss=\"mse\")\n",
    "history = model.fit(x_train, y_train, epochs=200, batch_size=24, validation_split=0.1, shuffle=True)\n",
    "#model summary\n",
    "model.summary()\n",
    "#Save Model\n",
    "model.save('GRU_model_soho.h5')  # creates a HDF5 file \n",
    "print('Model Saved')\n",
    "del model  # deletes the existing model\n",
    "\n",
    "custom_ob = {'LayerNormalization': LayerNormalization , 'SeqSelfAttention':SeqSelfAttention}\n",
    "model = load_model('GRU_model_soho.h5', custom_objects=custom_ob)\n",
    "t1 = time.time()\n",
    "# y_pred = model.predict([x_test,x_test])\n",
    "y_pred2 = model.predict(x_test)\n",
    "y_pred = model.predict(x_train)\n",
    "t2 = time.time()\n",
    "print('Predict time: ',t2-t1)\n",
    "y_pred = scaler.inverse_transform(y_pred)#Undo scaling\n",
    "rmse_lstm2 = np.sqrt(mean_squared_error(y_test, y_pred2))\n",
    "rmse_lstm = np.sqrt(mean_squared_error(y_train, y_pred))\n",
    "print('RMSE: ',rmse_lstm)\n",
    "print('RMSE2: ',rmse_lstm2)\n",
    "mae = mean_absolute_error(y_test, y_pred2)\n",
    "mae = mean_absolute_error(y_train, y_pred)\n",
    "print('MAE: ',mae)\n",
    "print('MAE2: ',mae)\n",
    "# r22 =  r2_score(y_test, y_pred2)\n",
    "# r2 =  r2_score(y_train, y_pred)\n",
    "# print('R-square: ',r2)\n",
    "# print('R-square2: ',r22)\n",
    "\n",
    "# n = len(y_test)\n",
    "# p = 12\n",
    "# Adj_r2 = 1-(1-r2)*(n-1)/(n-p-1)\n",
    "# Adj_r22 = 1-(1-r22)*(n-1)/(n-p-1)\n",
    "# print('Adj R-square: ',Adj_r2)\n",
    "# print('Adj R-square2: ',Adj_r22)\n",
    "\n",
    "plt.plot(history.history[\"loss\"],label=\"loss\")\n",
    "plt.plot(history.history[\"val_loss\"],label=\"val_loss\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"No. Of Epochs\")\n",
    "plt.ylabel(\"mse score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fa4261",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e01767",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
