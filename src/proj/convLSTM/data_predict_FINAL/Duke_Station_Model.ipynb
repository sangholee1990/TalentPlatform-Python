{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8610359",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from tcn import TCN\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error, r2_score,mean_absolute_error\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential , load_model , Model\n",
    "from keras.layers import Dense, Dropout , LSTM , Bidirectional ,GRU ,Flatten,Add,BatchNormalization\n",
    "from keras_layer_normalization import LayerNormalization\n",
    "from keras.initializers import  glorot_normal, RandomUniform\n",
    "from keras import optimizers,Input\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc70fba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160, 13) (144, 13) (40, 13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:32: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9ffd7bdfbb346dfa43d8ddda3f2b8aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:44: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7de9d857e738459b892bff3d921b36f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:\n",
      "(120, 24, 12) (120,)\n",
      "Test size:\n",
      "(16, 24, 12) (16,)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"station_bike _Duke.csv\")\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "df = df.set_index(\"timestamp\")\n",
    "#df.head()\n",
    "\n",
    "df[\"hour\"] = df.index.hour\n",
    "df[\"day_of_month\"] = df.index.day\n",
    "df[\"day_of_week\"]  = df.index.dayofweek\n",
    "df[\"month\"] = df.index.month\n",
    "\n",
    "training_data_len = math.ceil(len(df) * 0.9) # taking 90% of data to train and 10% of data to test\n",
    "testing_data_len = len(df) - training_data_len\n",
    "\n",
    "time_steps = 24\n",
    "train, test = df.iloc[0:training_data_len], df.iloc[(training_data_len-time_steps):len(df)]\n",
    "print(df.shape, train.shape, test.shape)\n",
    "train_trans = train[['t1','t2', 'hum', 'wind_speed']].to_numpy()\n",
    "test_trans = test[['t1','t2', 'hum', 'wind_speed']].to_numpy()\n",
    "\n",
    "scaler = RobustScaler() # Handles outliers\n",
    "#scaler = MinMaxScaler(feature_range=(0, 1)) # scale to (0,1)\n",
    "train.loc[:, ['t1','t2','hum', 'wind_speed']]=scaler.fit_transform(train_trans)\n",
    "test.loc[:, ['t1','t2', 'hum', 'wind_speed']]=scaler.fit_transform(test_trans)\n",
    "\n",
    "train['cnt'] = scaler.fit_transform(train[['cnt']])\n",
    "test['cnt'] = scaler.fit_transform(test[['cnt']])\n",
    "\n",
    "#Split the data into x_train and y_train data sets\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in tqdm(range(len(train) - time_steps)):\n",
    "    x_train.append(train.drop(columns='cnt').iloc[i:i + time_steps].to_numpy())\n",
    "    y_train.append(train.loc[:,'cnt'].iloc[i + time_steps])\n",
    "\n",
    "#Convert x_train and y_train to numpy arrays\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "#Create the x_test and y_test data sets\n",
    "x_test = []\n",
    "y_test = df.loc[:,'cnt'].iloc[training_data_len:len(df)]\n",
    "\n",
    "for i in tqdm(range(len(test) - time_steps)):\n",
    "    x_test.append(test.drop(columns='cnt').iloc[i:i + time_steps].to_numpy())\n",
    "    # y_test.append(test.loc[:,'cnt'].iloc[i + time_steps])\n",
    "\n",
    "#Convert x_test and y_test to numpy arrays\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# All 12 columns of the data\n",
    "print('Train size:')\n",
    "print(x_train.shape, y_train.shape)\n",
    "print('Test size:')\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac790b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 24, 12)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 24, 24)       1800        input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "seq_self_attention_3 (SeqSelfAt (None, 24, 24)       577         bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 24, 24)       0           seq_self_attention_3[0][0]       \n",
      "                                                                 bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_3 (LayerNor (None, 24, 24)       48          add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 576)          0           layer_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 12)           6924        flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 12)           0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 588)          0           dropout_3[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            589         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 1)            2           dense_8[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 9,940\n",
      "Trainable params: 9,940\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Concatenate\n",
    "init = glorot_normal(seed=None) # 給 GRU\n",
    "init_d = RandomUniform(minval=-0.05, maxval=0.05) # 給 Dense layer\n",
    "\n",
    "def Encoder(layer):\n",
    "    shortcut = layer\n",
    "    layer = SeqSelfAttention(\n",
    "                     attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n",
    "                     kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     bias_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     attention_regularizer_weight=0.0001)(layer)\n",
    "    layer = Add()([layer,shortcut])\n",
    "    layer = LayerNormalization()(layer)\n",
    "    layer = Flatten()(layer)\n",
    "    \n",
    "    shortcut2 = layer\n",
    "    layer = Dense(12,kernel_initializer=init_d)(layer)\n",
    "    layer = Dropout(0.15)(layer)\n",
    "    layer = Concatenate()([layer,shortcut2])\n",
    "    output = Dense(1,kernel_initializer=init_d)(layer)\n",
    "    return output\n",
    "\n",
    "def Decoder(layer):\n",
    "    shortcut = layer\n",
    "    layer = SeqSelfAttention(\n",
    "                     attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n",
    "                     kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     bias_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     attention_regularizer_weight=0.0001)(layer)\n",
    "    layer = Add()([layer,shortcut])\n",
    "    layer = LayerNormalization()(layer)\n",
    "    layer = SeqSelfAttention(\n",
    "                     attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n",
    "                     kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     bias_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     attention_regularizer_weight=0.0001)(layer)\n",
    "    layer = LayerNormalization()(layer)\n",
    "    \n",
    "    layer = Flatten()(layer)\n",
    "    shortcut2 = layer\n",
    "    layer = Dense(10,kernel_initializer=init_d)(layer)\n",
    "    #layer = Dropout(0.2)(layer)\n",
    "    layer = Concatenate()([layer,shortcut2])\n",
    "    output = Dense(1,kernel_initializer=init_d)(layer)\n",
    "    return output\n",
    "\n",
    "def Bi_GRU(layer,unit):\n",
    "    output = Bidirectional(GRU(unit, dropout=0.1, recurrent_dropout=0.1, return_sequences=True,\n",
    "                            kernel_initializer=init))(layer)\n",
    "    return output\n",
    "\n",
    "#start = Input(shape = (x_train.shape[1],x_train.shape[2]))\n",
    "start = Input(shape = (x_train.shape[1:]))\n",
    "start2 = Input(shape = (x_train.shape[1:]))\n",
    "x = Bi_GRU(start,12)\n",
    "x = Encoder(x)\n",
    "\n",
    "# y = Bi_GRU(start2,8)\n",
    "# y = Decoder(y)\n",
    "\n",
    "#Merge = Add()([x,x])\n",
    "Last = Dense(1)(x)\n",
    "model = Model([start,start2] , Last)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1f7bab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 108 samples, validate on 12 samples\n",
      "Epoch 1/500\n",
      "108/108 [==============================] - 2s 17ms/step - loss: 1.5022 - val_loss: 1.1752\n",
      "Epoch 2/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.3625 - val_loss: 0.9401\n",
      "Epoch 3/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.2980 - val_loss: 0.8276\n",
      "Epoch 4/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.0988 - val_loss: 0.7354\n",
      "Epoch 5/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.0464 - val_loss: 0.6995\n",
      "Epoch 6/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.9962 - val_loss: 0.6023\n",
      "Epoch 7/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.0434 - val_loss: 0.5373\n",
      "Epoch 8/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.0815 - val_loss: 0.7561\n",
      "Epoch 9/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.7911 - val_loss: 0.6271\n",
      "Epoch 10/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.8983 - val_loss: 0.5076\n",
      "Epoch 11/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6585 - val_loss: 0.5842\n",
      "Epoch 12/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6861 - val_loss: 0.5195\n",
      "Epoch 13/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6828 - val_loss: 0.4794\n",
      "Epoch 14/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6674 - val_loss: 0.5542\n",
      "Epoch 15/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5728 - val_loss: 0.5296\n",
      "Epoch 16/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5068 - val_loss: 0.4914\n",
      "Epoch 17/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6873 - val_loss: 0.5265\n",
      "Epoch 18/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6009 - val_loss: 0.5904\n",
      "Epoch 19/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5127 - val_loss: 0.4577\n",
      "Epoch 20/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4006 - val_loss: 0.5843\n",
      "Epoch 21/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4477 - val_loss: 0.4729\n",
      "Epoch 22/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4141 - val_loss: 0.4438\n",
      "Epoch 23/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4008 - val_loss: 0.4668\n",
      "Epoch 24/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.3481 - val_loss: 0.4164\n",
      "Epoch 25/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.3348 - val_loss: 0.3764\n",
      "Epoch 26/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.4074 - val_loss: 0.3990\n",
      "Epoch 27/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.3588 - val_loss: 0.3881\n",
      "Epoch 28/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.3467 - val_loss: 0.3990\n",
      "Epoch 29/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.3705 - val_loss: 0.3887\n",
      "Epoch 30/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.3031 - val_loss: 0.2809\n",
      "Epoch 31/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.2842 - val_loss: 0.3401\n",
      "Epoch 32/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.2414 - val_loss: 0.3480\n",
      "Epoch 33/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.3222 - val_loss: 0.4977\n",
      "Epoch 34/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.2526 - val_loss: 0.3822\n",
      "Epoch 35/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.2337 - val_loss: 0.3560\n",
      "Epoch 36/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.3634 - val_loss: 0.3504\n",
      "Epoch 37/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.3369 - val_loss: 0.4126\n",
      "Epoch 38/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2157 - val_loss: 0.3881\n",
      "Epoch 39/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.2900 - val_loss: 0.3140\n",
      "Epoch 40/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1825 - val_loss: 0.3462\n",
      "Epoch 41/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1888 - val_loss: 0.3530\n",
      "Epoch 42/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.2851 - val_loss: 0.3534\n",
      "Epoch 43/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1684 - val_loss: 0.2202\n",
      "Epoch 44/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1735 - val_loss: 0.2723\n",
      "Epoch 45/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2004 - val_loss: 0.3620\n",
      "Epoch 46/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1470 - val_loss: 0.2957\n",
      "Epoch 47/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1416 - val_loss: 0.3288\n",
      "Epoch 48/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1739 - val_loss: 0.3402\n",
      "Epoch 49/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2282 - val_loss: 0.2980\n",
      "Epoch 50/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1363 - val_loss: 0.3080\n",
      "Epoch 51/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1993 - val_loss: 0.3116\n",
      "Epoch 52/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1599 - val_loss: 0.3439\n",
      "Epoch 53/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1922 - val_loss: 0.3066\n",
      "Epoch 54/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1701 - val_loss: 0.3211\n",
      "Epoch 55/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.2917 - val_loss: 0.3283\n",
      "Epoch 56/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.3958 - val_loss: 0.3098\n",
      "Epoch 57/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2179 - val_loss: 0.2931\n",
      "Epoch 58/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1988 - val_loss: 0.2771\n",
      "Epoch 59/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1451 - val_loss: 0.2676\n",
      "Epoch 60/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1336 - val_loss: 0.3225\n",
      "Epoch 61/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1545 - val_loss: 0.2878\n",
      "Epoch 62/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1151 - val_loss: 0.2772\n",
      "Epoch 63/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1625 - val_loss: 0.2702\n",
      "Epoch 64/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1840 - val_loss: 0.2641\n",
      "Epoch 65/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1487 - val_loss: 0.2237\n",
      "Epoch 66/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1694 - val_loss: 0.2322\n",
      "Epoch 67/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1665 - val_loss: 0.2674\n",
      "Epoch 68/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1755 - val_loss: 0.2736\n",
      "Epoch 69/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1837 - val_loss: 0.2500\n",
      "Epoch 70/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1336 - val_loss: 0.2937\n",
      "Epoch 71/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.2622 - val_loss: 0.3779\n",
      "Epoch 72/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1821 - val_loss: 0.2708\n",
      "Epoch 73/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1928 - val_loss: 0.2864\n",
      "Epoch 74/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1813 - val_loss: 0.2923\n",
      "Epoch 75/500\n",
      "108/108 [==============================] - 1s 8ms/step - loss: 0.1202 - val_loss: 0.3031\n",
      "Epoch 76/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1972 - val_loss: 0.3633\n",
      "Epoch 77/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1381 - val_loss: 0.3123\n",
      "Epoch 78/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1166 - val_loss: 0.2867\n",
      "Epoch 79/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1108 - val_loss: 0.2747\n",
      "Epoch 80/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.3303 - val_loss: 0.3058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1619 - val_loss: 0.3424\n",
      "Epoch 82/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1031 - val_loss: 0.2988\n",
      "Epoch 83/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1111 - val_loss: 0.2465\n",
      "Epoch 84/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1617 - val_loss: 0.2437\n",
      "Epoch 85/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1218 - val_loss: 0.2243\n",
      "Epoch 86/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1376 - val_loss: 0.3062\n",
      "Epoch 87/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1833 - val_loss: 0.3368\n",
      "Epoch 88/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1324 - val_loss: 0.3004\n",
      "Epoch 89/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1181 - val_loss: 0.3034\n",
      "Epoch 90/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1112 - val_loss: 0.3093\n",
      "Epoch 91/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1345 - val_loss: 0.2678\n",
      "Epoch 92/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1126 - val_loss: 0.2578\n",
      "Epoch 93/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1172 - val_loss: 0.2519\n",
      "Epoch 94/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1091 - val_loss: 0.3096\n",
      "Epoch 95/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0931 - val_loss: 0.3334\n",
      "Epoch 96/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1167 - val_loss: 0.2504\n",
      "Epoch 97/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1084 - val_loss: 0.2537\n",
      "Epoch 98/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1252 - val_loss: 0.2658\n",
      "Epoch 99/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1667 - val_loss: 0.2338\n",
      "Epoch 100/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1230 - val_loss: 0.2572\n",
      "Epoch 101/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1412 - val_loss: 0.2226\n",
      "Epoch 102/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0985 - val_loss: 0.3325\n",
      "Epoch 103/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1254 - val_loss: 0.2498\n",
      "Epoch 104/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1347 - val_loss: 0.2895\n",
      "Epoch 105/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1577 - val_loss: 0.2884\n",
      "Epoch 106/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1081 - val_loss: 0.2835\n",
      "Epoch 107/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1112 - val_loss: 0.2560\n",
      "Epoch 108/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0995 - val_loss: 0.2638\n",
      "Epoch 109/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1076 - val_loss: 0.2394\n",
      "Epoch 110/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1034 - val_loss: 0.2537\n",
      "Epoch 111/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1162 - val_loss: 0.2492\n",
      "Epoch 112/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1530 - val_loss: 0.2871\n",
      "Epoch 113/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.2054 - val_loss: 0.2730\n",
      "Epoch 114/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1254 - val_loss: 0.3004\n",
      "Epoch 115/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0833 - val_loss: 0.2335\n",
      "Epoch 116/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0869 - val_loss: 0.2719\n",
      "Epoch 117/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0804 - val_loss: 0.3037\n",
      "Epoch 118/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1736 - val_loss: 0.3382\n",
      "Epoch 119/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1202 - val_loss: 0.3040\n",
      "Epoch 120/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0934 - val_loss: 0.3299\n",
      "Epoch 121/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1980 - val_loss: 0.2811\n",
      "Epoch 122/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1141 - val_loss: 0.2510\n",
      "Epoch 123/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1121 - val_loss: 0.3194\n",
      "Epoch 124/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1117 - val_loss: 0.3132\n",
      "Epoch 125/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1223 - val_loss: 0.2732\n",
      "Epoch 126/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0943 - val_loss: 0.3028\n",
      "Epoch 127/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0975 - val_loss: 0.2336\n",
      "Epoch 128/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1744 - val_loss: 0.2125\n",
      "Epoch 129/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0979 - val_loss: 0.2596\n",
      "Epoch 130/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0884 - val_loss: 0.2685\n",
      "Epoch 131/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0783 - val_loss: 0.2297\n",
      "Epoch 132/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0858 - val_loss: 0.2508\n",
      "Epoch 133/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1340 - val_loss: 0.3261\n",
      "Epoch 134/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1099 - val_loss: 0.2556\n",
      "Epoch 135/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0920 - val_loss: 0.2355\n",
      "Epoch 136/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0827 - val_loss: 0.2059\n",
      "Epoch 137/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1047 - val_loss: 0.2750\n",
      "Epoch 138/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1141 - val_loss: 0.2905\n",
      "Epoch 139/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0874 - val_loss: 0.2452\n",
      "Epoch 140/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0863 - val_loss: 0.3027\n",
      "Epoch 141/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1695 - val_loss: 0.2918\n",
      "Epoch 142/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.2208 - val_loss: 0.2840\n",
      "Epoch 143/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1375 - val_loss: 0.2283\n",
      "Epoch 144/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1796 - val_loss: 0.2415\n",
      "Epoch 145/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1388 - val_loss: 0.2815\n",
      "Epoch 146/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1589 - val_loss: 0.2900\n",
      "Epoch 147/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0943 - val_loss: 0.2534\n",
      "Epoch 148/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0875 - val_loss: 0.2851\n",
      "Epoch 149/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0640 - val_loss: 0.2918\n",
      "Epoch 150/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0955 - val_loss: 0.2866\n",
      "Epoch 151/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1376 - val_loss: 0.2753\n",
      "Epoch 152/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1049 - val_loss: 0.2545\n",
      "Epoch 153/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0885 - val_loss: 0.2617\n",
      "Epoch 154/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0797 - val_loss: 0.2797\n",
      "Epoch 155/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0726 - val_loss: 0.2209\n",
      "Epoch 156/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0764 - val_loss: 0.2224\n",
      "Epoch 157/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0795 - val_loss: 0.2439\n",
      "Epoch 158/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1070 - val_loss: 0.2936\n",
      "Epoch 159/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0950 - val_loss: 0.2793\n",
      "Epoch 160/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0773 - val_loss: 0.2807\n",
      "Epoch 161/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1179 - val_loss: 0.2669\n",
      "Epoch 162/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0826 - val_loss: 0.2752\n",
      "Epoch 163/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0837 - val_loss: 0.2069\n",
      "Epoch 164/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1232 - val_loss: 0.2168\n",
      "Epoch 165/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1070 - val_loss: 0.2406\n",
      "Epoch 166/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0847 - val_loss: 0.2358\n",
      "Epoch 167/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0959 - val_loss: 0.2806\n",
      "Epoch 168/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1035 - val_loss: 0.2236\n",
      "Epoch 169/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0853 - val_loss: 0.2997\n",
      "Epoch 170/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0600 - val_loss: 0.2583\n",
      "Epoch 171/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0719 - val_loss: 0.2595\n",
      "Epoch 172/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0846 - val_loss: 0.2384\n",
      "Epoch 173/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0693 - val_loss: 0.2500\n",
      "Epoch 174/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0852 - val_loss: 0.2721\n",
      "Epoch 175/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1216 - val_loss: 0.2862\n",
      "Epoch 176/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0891 - val_loss: 0.2738\n",
      "Epoch 177/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0738 - val_loss: 0.2513\n",
      "Epoch 178/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0607 - val_loss: 0.2690\n",
      "Epoch 179/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1626 - val_loss: 0.2776\n",
      "Epoch 180/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1242 - val_loss: 0.2277\n",
      "Epoch 181/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0998 - val_loss: 0.2871\n",
      "Epoch 182/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0739 - val_loss: 0.3391\n",
      "Epoch 183/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0751 - val_loss: 0.2945\n",
      "Epoch 184/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0934 - val_loss: 0.3136\n",
      "Epoch 185/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0855 - val_loss: 0.2963\n",
      "Epoch 186/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0784 - val_loss: 0.3026\n",
      "Epoch 187/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1049 - val_loss: 0.2887\n",
      "Epoch 188/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1241 - val_loss: 0.3060\n",
      "Epoch 189/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0756 - val_loss: 0.3017\n",
      "Epoch 190/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0742 - val_loss: 0.2812\n",
      "Epoch 191/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0681 - val_loss: 0.2943\n",
      "Epoch 192/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0882 - val_loss: 0.3168\n",
      "Epoch 193/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0480 - val_loss: 0.2844\n",
      "Epoch 194/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0578 - val_loss: 0.2818\n",
      "Epoch 195/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0614 - val_loss: 0.2503\n",
      "Epoch 196/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0762 - val_loss: 0.2584\n",
      "Epoch 197/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0747 - val_loss: 0.2309\n",
      "Epoch 198/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.2238 - val_loss: 0.2458\n",
      "Epoch 199/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0766 - val_loss: 0.2475\n",
      "Epoch 200/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1132 - val_loss: 0.2441\n",
      "Epoch 201/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1038 - val_loss: 0.2651\n",
      "Epoch 202/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0862 - val_loss: 0.2538\n",
      "Epoch 203/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0640 - val_loss: 0.2713\n",
      "Epoch 204/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0693 - val_loss: 0.2824\n",
      "Epoch 205/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0715 - val_loss: 0.2929\n",
      "Epoch 206/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0769 - val_loss: 0.2414\n",
      "Epoch 207/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0869 - val_loss: 0.2819\n",
      "Epoch 208/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0757 - val_loss: 0.3102\n",
      "Epoch 209/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1011 - val_loss: 0.2713\n",
      "Epoch 210/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0716 - val_loss: 0.2647\n",
      "Epoch 211/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0608 - val_loss: 0.2742\n",
      "Epoch 212/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0725 - val_loss: 0.2946\n",
      "Epoch 213/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0667 - val_loss: 0.2656\n",
      "Epoch 214/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0622 - val_loss: 0.2690\n",
      "Epoch 215/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0883 - val_loss: 0.2511\n",
      "Epoch 216/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0680 - val_loss: 0.2281\n",
      "Epoch 217/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0721 - val_loss: 0.2912\n",
      "Epoch 218/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0807 - val_loss: 0.4130\n",
      "Epoch 219/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0926 - val_loss: 0.2181\n",
      "Epoch 220/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0679 - val_loss: 0.2509\n",
      "Epoch 221/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0639 - val_loss: 0.2836\n",
      "Epoch 222/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1038 - val_loss: 0.2385\n",
      "Epoch 223/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1709 - val_loss: 0.3019\n",
      "Epoch 224/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0938 - val_loss: 0.2579\n",
      "Epoch 225/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1937 - val_loss: 0.2636\n",
      "Epoch 226/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0962 - val_loss: 0.2489\n",
      "Epoch 227/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0765 - val_loss: 0.2698\n",
      "Epoch 228/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0674 - val_loss: 0.2785\n",
      "Epoch 229/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0811 - val_loss: 0.2713\n",
      "Epoch 230/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0605 - val_loss: 0.2692\n",
      "Epoch 231/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0621 - val_loss: 0.2775\n",
      "Epoch 232/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1023 - val_loss: 0.2500\n",
      "Epoch 233/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0689 - val_loss: 0.2646\n",
      "Epoch 234/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0712 - val_loss: 0.2287\n",
      "Epoch 235/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.2192 - val_loss: 0.2233\n",
      "Epoch 236/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0822 - val_loss: 0.2328\n",
      "Epoch 237/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0738 - val_loss: 0.2509\n",
      "Epoch 238/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1065 - val_loss: 0.2895\n",
      "Epoch 239/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0587 - val_loss: 0.2431\n",
      "Epoch 240/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0748 - val_loss: 0.2192\n",
      "Epoch 241/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0791 - val_loss: 0.2542\n",
      "Epoch 242/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0681 - val_loss: 0.2335\n",
      "Epoch 243/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0703 - val_loss: 0.2748\n",
      "Epoch 244/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0885 - val_loss: 0.2864\n",
      "Epoch 245/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1023 - val_loss: 0.2787\n",
      "Epoch 246/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0762 - val_loss: 0.2708\n",
      "Epoch 247/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0696 - val_loss: 0.2838\n",
      "Epoch 248/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0987 - val_loss: 0.2360\n",
      "Epoch 249/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0735 - val_loss: 0.2756\n",
      "Epoch 250/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0588 - val_loss: 0.2447\n",
      "Epoch 251/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0571 - val_loss: 0.2381\n",
      "Epoch 252/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0665 - val_loss: 0.2316\n",
      "Epoch 253/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0685 - val_loss: 0.2714\n",
      "Epoch 254/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0714 - val_loss: 0.2371\n",
      "Epoch 255/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0650 - val_loss: 0.2638\n",
      "Epoch 256/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0603 - val_loss: 0.2463\n",
      "Epoch 257/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0642 - val_loss: 0.2665\n",
      "Epoch 258/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0631 - val_loss: 0.2752\n",
      "Epoch 259/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0686 - val_loss: 0.2579\n",
      "Epoch 260/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0815 - val_loss: 0.2518\n",
      "Epoch 261/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0954 - val_loss: 0.2513\n",
      "Epoch 262/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0560 - val_loss: 0.2464\n",
      "Epoch 263/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0549 - val_loss: 0.2454\n",
      "Epoch 264/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0810 - val_loss: 0.2603\n",
      "Epoch 265/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0785 - val_loss: 0.2510\n",
      "Epoch 266/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0697 - val_loss: 0.2786\n",
      "Epoch 267/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0580 - val_loss: 0.3026\n",
      "Epoch 268/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0582 - val_loss: 0.2662\n",
      "Epoch 269/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1039 - val_loss: 0.2614\n",
      "Epoch 270/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0660 - val_loss: 0.2567\n",
      "Epoch 271/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0675 - val_loss: 0.2398\n",
      "Epoch 272/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0957 - val_loss: 0.2720\n",
      "Epoch 273/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0601 - val_loss: 0.2717\n",
      "Epoch 274/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0663 - val_loss: 0.2242\n",
      "Epoch 275/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0659 - val_loss: 0.2541\n",
      "Epoch 276/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0551 - val_loss: 0.2430\n",
      "Epoch 277/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0766 - val_loss: 0.2644\n",
      "Epoch 278/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0831 - val_loss: 0.2829\n",
      "Epoch 279/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0573 - val_loss: 0.2316\n",
      "Epoch 280/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0892 - val_loss: 0.2858\n",
      "Epoch 281/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0570 - val_loss: 0.2769\n",
      "Epoch 282/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0621 - val_loss: 0.2450\n",
      "Epoch 283/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0633 - val_loss: 0.2369\n",
      "Epoch 284/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0516 - val_loss: 0.2535\n",
      "Epoch 285/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0622 - val_loss: 0.2358\n",
      "Epoch 286/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0632 - val_loss: 0.2268\n",
      "Epoch 287/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0720 - val_loss: 0.2283\n",
      "Epoch 288/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0520 - val_loss: 0.2401\n",
      "Epoch 289/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0716 - val_loss: 0.2677\n",
      "Epoch 290/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0887 - val_loss: 0.2682\n",
      "Epoch 291/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0689 - val_loss: 0.2578\n",
      "Epoch 292/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0660 - val_loss: 0.2550\n",
      "Epoch 293/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0955 - val_loss: 0.3216\n",
      "Epoch 294/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0757 - val_loss: 0.2797\n",
      "Epoch 295/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0577 - val_loss: 0.2866\n",
      "Epoch 296/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0483 - val_loss: 0.2562\n",
      "Epoch 297/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0664 - val_loss: 0.2366\n",
      "Epoch 298/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0627 - val_loss: 0.2426\n",
      "Epoch 299/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0521 - val_loss: 0.2398\n",
      "Epoch 300/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0465 - val_loss: 0.2418\n",
      "Epoch 301/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0544 - val_loss: 0.2363\n",
      "Epoch 302/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0567 - val_loss: 0.2621\n",
      "Epoch 303/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0563 - val_loss: 0.2744\n",
      "Epoch 304/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1591 - val_loss: 0.2521\n",
      "Epoch 305/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0674 - val_loss: 0.2463\n",
      "Epoch 306/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0554 - val_loss: 0.2550\n",
      "Epoch 307/500\n",
      "108/108 [==============================] - 1s 8ms/step - loss: 0.0603 - val_loss: 0.2309\n",
      "Epoch 308/500\n",
      "108/108 [==============================] - 1s 8ms/step - loss: 0.0548 - val_loss: 0.2457\n",
      "Epoch 309/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0682 - val_loss: 0.2720\n",
      "Epoch 310/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0576 - val_loss: 0.2495\n",
      "Epoch 311/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0614 - val_loss: 0.2252\n",
      "Epoch 312/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1567 - val_loss: 0.2341\n",
      "Epoch 313/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0657 - val_loss: 0.2475\n",
      "Epoch 314/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0530 - val_loss: 0.2639\n",
      "Epoch 315/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0482 - val_loss: 0.2358\n",
      "Epoch 316/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0808 - val_loss: 0.2783\n",
      "Epoch 317/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0626 - val_loss: 0.3424\n",
      "Epoch 318/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1046 - val_loss: 0.2746\n",
      "Epoch 319/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0547 - val_loss: 0.2485\n",
      "Epoch 320/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0651 - val_loss: 0.2400\n",
      "Epoch 321/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0789 - val_loss: 0.2440\n",
      "Epoch 322/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1333 - val_loss: 0.2706\n",
      "Epoch 323/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0509 - val_loss: 0.2741\n",
      "Epoch 324/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0617 - val_loss: 0.2451\n",
      "Epoch 325/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0668 - val_loss: 0.2549\n",
      "Epoch 326/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0571 - val_loss: 0.3274\n",
      "Epoch 327/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0825 - val_loss: 0.2643\n",
      "Epoch 328/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0584 - val_loss: 0.2518\n",
      "Epoch 329/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0573 - val_loss: 0.2706\n",
      "Epoch 330/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1209 - val_loss: 0.2725\n",
      "Epoch 331/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0562 - val_loss: 0.2664\n",
      "Epoch 332/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0755 - val_loss: 0.2843\n",
      "Epoch 333/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0581 - val_loss: 0.2546\n",
      "Epoch 334/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0492 - val_loss: 0.2659\n",
      "Epoch 335/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0538 - val_loss: 0.2584\n",
      "Epoch 336/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0633 - val_loss: 0.2725\n",
      "Epoch 337/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0461 - val_loss: 0.2768\n",
      "Epoch 338/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0506 - val_loss: 0.2695\n",
      "Epoch 339/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0517 - val_loss: 0.2589\n",
      "Epoch 340/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0531 - val_loss: 0.2652\n",
      "Epoch 341/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0537 - val_loss: 0.2820\n",
      "Epoch 342/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0553 - val_loss: 0.2562\n",
      "Epoch 343/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0731 - val_loss: 0.3314\n",
      "Epoch 344/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0681 - val_loss: 0.2955\n",
      "Epoch 345/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0649 - val_loss: 0.2274\n",
      "Epoch 346/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0636 - val_loss: 0.2460\n",
      "Epoch 347/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0539 - val_loss: 0.2547\n",
      "Epoch 348/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0839 - val_loss: 0.2366\n",
      "Epoch 349/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0545 - val_loss: 0.2564\n",
      "Epoch 350/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0629 - val_loss: 0.2327\n",
      "Epoch 351/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0542 - val_loss: 0.2385\n",
      "Epoch 352/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0666 - val_loss: 0.2284\n",
      "Epoch 353/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0901 - val_loss: 0.2398\n",
      "Epoch 354/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0937 - val_loss: 0.2580\n",
      "Epoch 355/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0561 - val_loss: 0.2365\n",
      "Epoch 356/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0513 - val_loss: 0.2399\n",
      "Epoch 357/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0490 - val_loss: 0.2325\n",
      "Epoch 358/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0582 - val_loss: 0.2305\n",
      "Epoch 359/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0567 - val_loss: 0.2348\n",
      "Epoch 360/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0662 - val_loss: 0.2409\n",
      "Epoch 361/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0636 - val_loss: 0.2423\n",
      "Epoch 362/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0563 - val_loss: 0.2635\n",
      "Epoch 363/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0674 - val_loss: 0.2458\n",
      "Epoch 364/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0473 - val_loss: 0.2836\n",
      "Epoch 365/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0578 - val_loss: 0.2600\n",
      "Epoch 366/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0422 - val_loss: 0.2630\n",
      "Epoch 367/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0450 - val_loss: 0.2254\n",
      "Epoch 368/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0650 - val_loss: 0.2519\n",
      "Epoch 369/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0556 - val_loss: 0.2469\n",
      "Epoch 370/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0476 - val_loss: 0.2708\n",
      "Epoch 371/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0409 - val_loss: 0.2573\n",
      "Epoch 372/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0483 - val_loss: 0.2378\n",
      "Epoch 373/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0496 - val_loss: 0.2693\n",
      "Epoch 374/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0528 - val_loss: 0.2671\n",
      "Epoch 375/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0530 - val_loss: 0.2374\n",
      "Epoch 376/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0619 - val_loss: 0.2380\n",
      "Epoch 377/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0576 - val_loss: 0.2474\n",
      "Epoch 378/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0591 - val_loss: 0.2661\n",
      "Epoch 379/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0550 - val_loss: 0.2641\n",
      "Epoch 380/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0478 - val_loss: 0.2329\n",
      "Epoch 381/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0406 - val_loss: 0.2246\n",
      "Epoch 382/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0456 - val_loss: 0.2315\n",
      "Epoch 383/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0438 - val_loss: 0.2399\n",
      "Epoch 384/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0465 - val_loss: 0.2470\n",
      "Epoch 385/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0479 - val_loss: 0.2324\n",
      "Epoch 386/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0543 - val_loss: 0.2554\n",
      "Epoch 387/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0451 - val_loss: 0.2305\n",
      "Epoch 388/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0761 - val_loss: 0.2456\n",
      "Epoch 389/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0535 - val_loss: 0.2483\n",
      "Epoch 390/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0418 - val_loss: 0.2471\n",
      "Epoch 391/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0720 - val_loss: 0.2247\n",
      "Epoch 392/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0805 - val_loss: 0.2348\n",
      "Epoch 393/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0596 - val_loss: 0.2238\n",
      "Epoch 394/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0521 - val_loss: 0.2114\n",
      "Epoch 395/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0421 - val_loss: 0.1993\n",
      "Epoch 396/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0583 - val_loss: 0.2323\n",
      "Epoch 397/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0409 - val_loss: 0.2247\n",
      "Epoch 398/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0443 - val_loss: 0.2211\n",
      "Epoch 399/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0444 - val_loss: 0.2380\n",
      "Epoch 400/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0399 - val_loss: 0.2285\n",
      "Epoch 401/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0490 - val_loss: 0.2630\n",
      "Epoch 402/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0412 - val_loss: 0.2605\n",
      "Epoch 403/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0404 - val_loss: 0.2301\n",
      "Epoch 404/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0600 - val_loss: 0.2469\n",
      "Epoch 405/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0443 - val_loss: 0.2586\n",
      "Epoch 406/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0418 - val_loss: 0.2365\n",
      "Epoch 407/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0414 - val_loss: 0.2479\n",
      "Epoch 408/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0563 - val_loss: 0.3388\n",
      "Epoch 409/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0555 - val_loss: 0.2318\n",
      "Epoch 410/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0427 - val_loss: 0.2583\n",
      "Epoch 411/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0517 - val_loss: 0.2738\n",
      "Epoch 412/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0402 - val_loss: 0.2221\n",
      "Epoch 413/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0353 - val_loss: 0.2483\n",
      "Epoch 414/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0447 - val_loss: 0.2510\n",
      "Epoch 415/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0557 - val_loss: 0.2601\n",
      "Epoch 416/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0602 - val_loss: 0.2730\n",
      "Epoch 417/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0511 - val_loss: 0.2457\n",
      "Epoch 418/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0443 - val_loss: 0.2352\n",
      "Epoch 419/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0394 - val_loss: 0.2396\n",
      "Epoch 420/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0466 - val_loss: 0.2795\n",
      "Epoch 421/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0435 - val_loss: 0.2580\n",
      "Epoch 422/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0552 - val_loss: 0.2164\n",
      "Epoch 423/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0489 - val_loss: 0.2427\n",
      "Epoch 424/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0455 - val_loss: 0.2268\n",
      "Epoch 425/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0486 - val_loss: 0.2145\n",
      "Epoch 426/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0430 - val_loss: 0.2583\n",
      "Epoch 427/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0396 - val_loss: 0.2357\n",
      "Epoch 428/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0501 - val_loss: 0.2679\n",
      "Epoch 429/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0475 - val_loss: 0.2728\n",
      "Epoch 430/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0388 - val_loss: 0.2556\n",
      "Epoch 431/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0513 - val_loss: 0.2579\n",
      "Epoch 432/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0464 - val_loss: 0.2632\n",
      "Epoch 433/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0401 - val_loss: 0.2611\n",
      "Epoch 434/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0807 - val_loss: 0.2462\n",
      "Epoch 435/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0625 - val_loss: 0.2832\n",
      "Epoch 436/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0443 - val_loss: 0.2408\n",
      "Epoch 437/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0340 - val_loss: 0.2306\n",
      "Epoch 438/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0387 - val_loss: 0.2481\n",
      "Epoch 439/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0390 - val_loss: 0.2600\n",
      "Epoch 440/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1396 - val_loss: 0.2412\n",
      "Epoch 441/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0759 - val_loss: 0.2419\n",
      "Epoch 442/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0439 - val_loss: 0.2594\n",
      "Epoch 443/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1449 - val_loss: 0.2421\n",
      "Epoch 444/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0611 - val_loss: 0.2653\n",
      "Epoch 445/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0678 - val_loss: 0.2484\n",
      "Epoch 446/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0635 - val_loss: 0.2347\n",
      "Epoch 447/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0459 - val_loss: 0.2539\n",
      "Epoch 448/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0470 - val_loss: 0.2610\n",
      "Epoch 449/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0358 - val_loss: 0.2504\n",
      "Epoch 450/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0424 - val_loss: 0.2456\n",
      "Epoch 451/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0707 - val_loss: 0.2343\n",
      "Epoch 452/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0427 - val_loss: 0.2323\n",
      "Epoch 453/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0438 - val_loss: 0.2267\n",
      "Epoch 454/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0576 - val_loss: 0.2636\n",
      "Epoch 455/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0442 - val_loss: 0.2785\n",
      "Epoch 456/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0511 - val_loss: 0.2533\n",
      "Epoch 457/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0460 - val_loss: 0.2697\n",
      "Epoch 458/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0399 - val_loss: 0.2637\n",
      "Epoch 459/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0424 - val_loss: 0.2498\n",
      "Epoch 460/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0449 - val_loss: 0.2344\n",
      "Epoch 461/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0353 - val_loss: 0.2231\n",
      "Epoch 462/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0317 - val_loss: 0.2538\n",
      "Epoch 463/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1147 - val_loss: 0.2333\n",
      "Epoch 464/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0585 - val_loss: 0.2592\n",
      "Epoch 465/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0616 - val_loss: 0.2522\n",
      "Epoch 466/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0479 - val_loss: 0.2288\n",
      "Epoch 467/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0421 - val_loss: 0.2615\n",
      "Epoch 468/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0963 - val_loss: 0.2514\n",
      "Epoch 469/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0424 - val_loss: 0.2398\n",
      "Epoch 470/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0468 - val_loss: 0.2460\n",
      "Epoch 471/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0362 - val_loss: 0.2675\n",
      "Epoch 472/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0503 - val_loss: 0.2478\n",
      "Epoch 473/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0430 - val_loss: 0.2315\n",
      "Epoch 474/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0530 - val_loss: 0.2581\n",
      "Epoch 475/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0430 - val_loss: 0.2320\n",
      "Epoch 476/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0528 - val_loss: 0.2276\n",
      "Epoch 477/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0397 - val_loss: 0.2294\n",
      "Epoch 478/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0630 - val_loss: 0.2783\n",
      "Epoch 479/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0765 - val_loss: 0.2757\n",
      "Epoch 480/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0548 - val_loss: 0.2527\n",
      "Epoch 481/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0507 - val_loss: 0.2514\n",
      "Epoch 482/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0356 - val_loss: 0.2634\n",
      "Epoch 483/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0469 - val_loss: 0.2524\n",
      "Epoch 484/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0389 - val_loss: 0.2526\n",
      "Epoch 485/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0448 - val_loss: 0.2595\n",
      "Epoch 486/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0449 - val_loss: 0.2768\n",
      "Epoch 487/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0392 - val_loss: 0.2824\n",
      "Epoch 488/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0403 - val_loss: 0.2498\n",
      "Epoch 489/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0326 - val_loss: 0.2689\n",
      "Epoch 490/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0506 - val_loss: 0.2843\n",
      "Epoch 491/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0320 - val_loss: 0.2620\n",
      "Epoch 492/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0460 - val_loss: 0.2406\n",
      "Epoch 493/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0338 - val_loss: 0.2534\n",
      "Epoch 494/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0457 - val_loss: 0.2640\n",
      "Epoch 495/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0475 - val_loss: 0.2398\n",
      "Epoch 496/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0569 - val_loss: 0.2278\n",
      "Epoch 497/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0537 - val_loss: 0.2654\n",
      "Epoch 498/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0512 - val_loss: 0.2233\n",
      "Epoch 499/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0429 - val_loss: 0.2471\n",
      "Epoch 500/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0360 - val_loss: 0.2591\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 24, 12)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 24, 24)       1800        input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "seq_self_attention_3 (SeqSelfAt (None, 24, 24)       577         bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 24, 24)       0           seq_self_attention_3[0][0]       \n",
      "                                                                 bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_3 (LayerNor (None, 24, 24)       48          add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 576)          0           layer_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 12)           6924        flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 12)           0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 588)          0           dropout_3[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            589         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 1)            2           dense_8[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 9,940\n",
      "Trainable params: 9,940\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Predict time:  0.3011934757232666\n",
      "RMSE:  23.29852426367362\n",
      "RMSE2:  17.136769127685508\n",
      "MAE:  16.027589001542044\n",
      "MAE2:  16.027589001542044\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'mse score')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "opt = optimizers.Adam(lr=0.001,decay = 0.0001)\n",
    "print('Train...')\n",
    "# model.compile(optimizer = opt , loss=\"mse\")\n",
    "model.compile(optimizer = \"adam\" , loss=\"mse\")\n",
    "history = model.fit([x_train,x_train], y_train, epochs = 500, batch_size=8, validation_split=0.1, shuffle=True)\n",
    "# history = model.fit(x_train, y_train, epochs = 500, batch_size=6, validation_split=0.1, shuffle=True)\n",
    "model.summary()\n",
    "#Save Model\n",
    "model.save('GRU_Single_Attention_model_Duke.h5')  # creates a HDF5 file \n",
    "del model\n",
    "\n",
    "custom_ob = {'LayerNormalization': LayerNormalization , 'SeqSelfAttention':SeqSelfAttention}\n",
    "model = load_model('GRU_Single_Attention_model_Duke.h5', custom_objects=custom_ob)\n",
    "t1 = time.time()\n",
    "# y_pred = model.predict([x_test,x_test])\n",
    "y_pred2 = model.predict(x_test)\n",
    "y_pred = model.predict(x_train)\n",
    "t2 = time.time()\n",
    "print('Predict time: ',t2-t1)\n",
    "y_pred = scaler.inverse_transform(y_pred)#Undo scaling\n",
    "rmse_lstm2 = np.sqrt(mean_squared_error(y_test, y_pred2))\n",
    "rmse_lstm = np.sqrt(mean_squared_error(y_train, y_pred))\n",
    "print('RMSE: ',rmse_lstm)\n",
    "print('RMSE2: ',rmse_lstm2)\n",
    "mae = mean_absolute_error(y_test, y_pred2)\n",
    "mae = mean_absolute_error(y_train, y_pred)\n",
    "print('MAE: ',mae)\n",
    "print('MAE2: ',mae)\n",
    "# r22 =  r2_score(y_test, y_pred2)\n",
    "# r2 =  r2_score(y_train, y_pred)\n",
    "# print('R-square: ',r2)\n",
    "# print('R-square2: ',r22)\n",
    "\n",
    "# n = len(y_test)\n",
    "# p = 12\n",
    "# Adj_r2 = 1-(1-r2)*(n-1)/(n-p-1)\n",
    "# Adj_r22 = 1-(1-r22)*(n-1)/(n-p-1)\n",
    "# print('Adj R-square: ',Adj_r2)\n",
    "# print('Adj R-square2: ',Adj_r22)\n",
    "\n",
    "plt.plot(history.history[\"loss\"],label=\"loss\")\n",
    "plt.plot(history.history[\"val_loss\"],label=\"val_loss\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"No. Of Epochs\")\n",
    "plt.ylabel(\"mse score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "579a4155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 108 samples, validate on 12 samples\n",
      "Epoch 1/200\n",
      "108/108 [==============================] - 2s 18ms/step - loss: 2.0323 - val_loss: 1.7447\n",
      "Epoch 2/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 1.9729 - val_loss: 1.6576\n",
      "Epoch 3/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 1.8922 - val_loss: 1.5725\n",
      "Epoch 4/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 1.8563 - val_loss: 1.5006\n",
      "Epoch 5/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 1.8037 - val_loss: 1.4421\n",
      "Epoch 6/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 1.7537 - val_loss: 1.3894\n",
      "Epoch 7/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 1.6985 - val_loss: 1.3462\n",
      "Epoch 8/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 1.6773 - val_loss: 1.3055\n",
      "Epoch 9/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 1.6239 - val_loss: 1.2643\n",
      "Epoch 10/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 1.5812 - val_loss: 1.2225\n",
      "Epoch 11/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 1.5222 - val_loss: 1.1841\n",
      "Epoch 12/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 1.5261 - val_loss: 1.1488\n",
      "Epoch 13/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 1.4720 - val_loss: 1.1206\n",
      "Epoch 14/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 1.4496 - val_loss: 1.0890\n",
      "Epoch 15/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 1.3967 - val_loss: 1.0647\n",
      "Epoch 16/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 1.3634 - val_loss: 1.0417\n",
      "Epoch 17/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 1.4041 - val_loss: 1.0162\n",
      "Epoch 18/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 1.3157 - val_loss: 0.9939\n",
      "Epoch 19/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 1.2931 - val_loss: 0.9763\n",
      "Epoch 20/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 1.2493 - val_loss: 0.9550\n",
      "Epoch 21/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 1.2798 - val_loss: 0.9248\n",
      "Epoch 22/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 1.2318 - val_loss: 0.9038\n",
      "Epoch 23/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.2066 - val_loss: 0.8963\n",
      "Epoch 24/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 1.1888 - val_loss: 0.8926\n",
      "Epoch 25/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 1.2050 - val_loss: 0.8657\n",
      "Epoch 26/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 1.1279 - val_loss: 0.8556\n",
      "Epoch 27/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 1.0898 - val_loss: 0.8414\n",
      "Epoch 28/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 1.1337 - val_loss: 0.8179\n",
      "Epoch 29/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 1.0802 - val_loss: 0.8289\n",
      "Epoch 30/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 1.0278 - val_loss: 0.8223\n",
      "Epoch 31/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 1.0336 - val_loss: 0.8107\n",
      "Epoch 32/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 1.0729 - val_loss: 0.7912\n",
      "Epoch 33/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.0223 - val_loss: 0.7774\n",
      "Epoch 34/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.0061 - val_loss: 0.7619\n",
      "Epoch 35/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 1.0119 - val_loss: 0.7570\n",
      "Epoch 36/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.9825 - val_loss: 0.7252\n",
      "Epoch 37/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.9768 - val_loss: 0.7216\n",
      "Epoch 38/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.9814 - val_loss: 0.7285\n",
      "Epoch 39/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.9708 - val_loss: 0.7367\n",
      "Epoch 40/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.9849 - val_loss: 0.7204\n",
      "Epoch 41/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.8960 - val_loss: 0.7166\n",
      "Epoch 42/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.9967 - val_loss: 0.7082\n",
      "Epoch 43/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.9381 - val_loss: 0.7207\n",
      "Epoch 44/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.9510 - val_loss: 0.7079\n",
      "Epoch 45/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.8792 - val_loss: 0.7176\n",
      "Epoch 46/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.9352 - val_loss: 0.6987\n",
      "Epoch 47/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.9227 - val_loss: 0.7039\n",
      "Epoch 48/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.8653 - val_loss: 0.6925\n",
      "Epoch 49/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.9214 - val_loss: 0.6974\n",
      "Epoch 50/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.9065 - val_loss: 0.6849\n",
      "Epoch 51/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.8979 - val_loss: 0.6825\n",
      "Epoch 52/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.8951 - val_loss: 0.6819\n",
      "Epoch 53/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.9393 - val_loss: 0.6875\n",
      "Epoch 54/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.8415 - val_loss: 0.6549\n",
      "Epoch 55/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.7912 - val_loss: 0.6593\n",
      "Epoch 56/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.7857 - val_loss: 0.6313\n",
      "Epoch 57/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.8385 - val_loss: 0.6311\n",
      "Epoch 58/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.8007 - val_loss: 0.6540\n",
      "Epoch 59/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.7526 - val_loss: 0.6600\n",
      "Epoch 60/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.7623 - val_loss: 0.6657\n",
      "Epoch 61/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.8235 - val_loss: 0.6701\n",
      "Epoch 62/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.7866 - val_loss: 0.6665\n",
      "Epoch 63/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.7600 - val_loss: 0.6861\n",
      "Epoch 64/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.7554 - val_loss: 0.6949\n",
      "Epoch 65/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.7609 - val_loss: 0.6893\n",
      "Epoch 66/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.6799 - val_loss: 0.6591\n",
      "Epoch 67/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.7050 - val_loss: 0.6578\n",
      "Epoch 68/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.7128 - val_loss: 0.6542\n",
      "Epoch 69/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.6827 - val_loss: 0.6767\n",
      "Epoch 70/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6821 - val_loss: 0.6595\n",
      "Epoch 71/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.7428 - val_loss: 0.6984\n",
      "Epoch 72/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.6275 - val_loss: 0.7002\n",
      "Epoch 73/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.6506 - val_loss: 0.6460\n",
      "Epoch 74/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.6614 - val_loss: 0.6759\n",
      "Epoch 75/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.7172 - val_loss: 0.6752\n",
      "Epoch 76/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.6314 - val_loss: 0.6553\n",
      "Epoch 77/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.6659 - val_loss: 0.6800\n",
      "Epoch 78/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.6458 - val_loss: 0.7027\n",
      "Epoch 79/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.5881 - val_loss: 0.7016\n",
      "Epoch 80/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.6032 - val_loss: 0.6614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.6214 - val_loss: 0.6378\n",
      "Epoch 82/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.5877 - val_loss: 0.6446\n",
      "Epoch 83/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.6273 - val_loss: 0.6650\n",
      "Epoch 84/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.5633 - val_loss: 0.6815\n",
      "Epoch 85/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.5247 - val_loss: 0.5910\n",
      "Epoch 86/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.6004 - val_loss: 0.6414\n",
      "Epoch 87/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.6244 - val_loss: 0.6460\n",
      "Epoch 88/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.5096 - val_loss: 0.6619\n",
      "Epoch 89/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.6078 - val_loss: 0.6417\n",
      "Epoch 90/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.5254 - val_loss: 0.6437\n",
      "Epoch 91/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.5658 - val_loss: 0.6224\n",
      "Epoch 92/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4664 - val_loss: 0.6000\n",
      "Epoch 93/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.5328 - val_loss: 0.5714\n",
      "Epoch 94/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.5462 - val_loss: 0.5981\n",
      "Epoch 95/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4629 - val_loss: 0.6090\n",
      "Epoch 96/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.5001 - val_loss: 0.6281\n",
      "Epoch 97/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.5278 - val_loss: 0.6441\n",
      "Epoch 98/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.5118 - val_loss: 0.6097\n",
      "Epoch 99/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4272 - val_loss: 0.5971\n",
      "Epoch 100/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.5519 - val_loss: 0.6140\n",
      "Epoch 101/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4974 - val_loss: 0.5753\n",
      "Epoch 102/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4368 - val_loss: 0.5764\n",
      "Epoch 103/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.5010 - val_loss: 0.5884\n",
      "Epoch 104/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4715 - val_loss: 0.5507\n",
      "Epoch 105/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4378 - val_loss: 0.5720\n",
      "Epoch 106/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.5011 - val_loss: 0.5692\n",
      "Epoch 107/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4752 - val_loss: 0.5689\n",
      "Epoch 108/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.5305 - val_loss: 0.6126\n",
      "Epoch 109/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.5081 - val_loss: 0.5392\n",
      "Epoch 110/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4147 - val_loss: 0.5395\n",
      "Epoch 111/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4560 - val_loss: 0.5106\n",
      "Epoch 112/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4415 - val_loss: 0.5426\n",
      "Epoch 113/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4412 - val_loss: 0.5491\n",
      "Epoch 114/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.5068 - val_loss: 0.6088\n",
      "Epoch 115/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4818 - val_loss: 0.5488\n",
      "Epoch 116/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4423 - val_loss: 0.4922\n",
      "Epoch 117/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4566 - val_loss: 0.5537\n",
      "Epoch 118/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4407 - val_loss: 0.5446\n",
      "Epoch 119/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.5014 - val_loss: 0.5184\n",
      "Epoch 120/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4338 - val_loss: 0.5961\n",
      "Epoch 121/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4683 - val_loss: 0.5526\n",
      "Epoch 122/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4045 - val_loss: 0.5413\n",
      "Epoch 123/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.4215 - val_loss: 0.4800\n",
      "Epoch 124/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4144 - val_loss: 0.4744\n",
      "Epoch 125/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4694 - val_loss: 0.4927\n",
      "Epoch 126/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4139 - val_loss: 0.5172\n",
      "Epoch 127/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4208 - val_loss: 0.5316\n",
      "Epoch 128/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4236 - val_loss: 0.5196\n",
      "Epoch 129/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4439 - val_loss: 0.5186\n",
      "Epoch 130/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4176 - val_loss: 0.5729\n",
      "Epoch 131/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4026 - val_loss: 0.5333\n",
      "Epoch 132/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4063 - val_loss: 0.6541\n",
      "Epoch 133/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4257 - val_loss: 0.5508\n",
      "Epoch 134/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4295 - val_loss: 0.5679\n",
      "Epoch 135/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4988 - val_loss: 0.5772\n",
      "Epoch 136/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4137 - val_loss: 0.6061\n",
      "Epoch 137/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3695 - val_loss: 0.6020\n",
      "Epoch 138/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3485 - val_loss: 0.4987\n",
      "Epoch 139/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3929 - val_loss: 0.4780\n",
      "Epoch 140/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4524 - val_loss: 0.4666\n",
      "Epoch 141/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4363 - val_loss: 0.5463\n",
      "Epoch 142/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4408 - val_loss: 0.5854\n",
      "Epoch 143/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4255 - val_loss: 0.4809\n",
      "Epoch 144/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4247 - val_loss: 0.4510\n",
      "Epoch 145/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3945 - val_loss: 0.4310\n",
      "Epoch 146/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4288 - val_loss: 0.4853\n",
      "Epoch 147/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4047 - val_loss: 0.4760\n",
      "Epoch 148/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3405 - val_loss: 0.4772\n",
      "Epoch 149/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3755 - val_loss: 0.6133\n",
      "Epoch 150/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3776 - val_loss: 0.5138\n",
      "Epoch 151/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3654 - val_loss: 0.4670\n",
      "Epoch 152/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3746 - val_loss: 0.4882\n",
      "Epoch 153/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3791 - val_loss: 0.4640\n",
      "Epoch 154/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.4018 - val_loss: 0.4827\n",
      "Epoch 155/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3675 - val_loss: 0.5451\n",
      "Epoch 156/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3387 - val_loss: 0.5981\n",
      "Epoch 157/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3750 - val_loss: 0.5738\n",
      "Epoch 158/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3458 - val_loss: 0.5038\n",
      "Epoch 159/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3727 - val_loss: 0.4694\n",
      "Epoch 160/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3348 - val_loss: 0.5144\n",
      "Epoch 161/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3683 - val_loss: 0.4558\n",
      "Epoch 162/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3768 - val_loss: 0.5411\n",
      "Epoch 163/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.4385 - val_loss: 0.4761\n",
      "Epoch 164/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.3610 - val_loss: 0.4725\n",
      "Epoch 165/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3541 - val_loss: 0.4305\n",
      "Epoch 166/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3712 - val_loss: 0.5301\n",
      "Epoch 167/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3296 - val_loss: 0.5737\n",
      "Epoch 168/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3868 - val_loss: 0.5213\n",
      "Epoch 169/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3723 - val_loss: 0.4387\n",
      "Epoch 170/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3775 - val_loss: 0.5874\n",
      "Epoch 171/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3684 - val_loss: 0.5033\n",
      "Epoch 172/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3949 - val_loss: 0.4916\n",
      "Epoch 173/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3441 - val_loss: 0.4828\n",
      "Epoch 174/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3908 - val_loss: 0.4592\n",
      "Epoch 175/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3481 - val_loss: 0.4961\n",
      "Epoch 176/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3631 - val_loss: 0.4574\n",
      "Epoch 177/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3496 - val_loss: 0.5489\n",
      "Epoch 178/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3086 - val_loss: 0.5659\n",
      "Epoch 179/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3828 - val_loss: 0.5007\n",
      "Epoch 180/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3413 - val_loss: 0.5467\n",
      "Epoch 181/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3657 - val_loss: 0.5256\n",
      "Epoch 182/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3273 - val_loss: 0.6232\n",
      "Epoch 183/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3320 - val_loss: 0.6289\n",
      "Epoch 184/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3955 - val_loss: 0.5770\n",
      "Epoch 185/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3452 - val_loss: 0.4678\n",
      "Epoch 186/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.3510 - val_loss: 0.5502\n",
      "Epoch 187/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3302 - val_loss: 0.5136\n",
      "Epoch 188/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3464 - val_loss: 0.4707\n",
      "Epoch 189/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2795 - val_loss: 0.4876\n",
      "Epoch 190/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3005 - val_loss: 0.5122\n",
      "Epoch 191/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3292 - val_loss: 0.4814\n",
      "Epoch 192/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3313 - val_loss: 0.5391\n",
      "Epoch 193/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3717 - val_loss: 0.5773\n",
      "Epoch 194/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3685 - val_loss: 0.6494\n",
      "Epoch 195/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3004 - val_loss: 0.4991\n",
      "Epoch 196/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3057 - val_loss: 0.5622\n",
      "Epoch 197/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3315 - val_loss: 0.7999\n",
      "Epoch 198/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3513 - val_loss: 0.5989\n",
      "Epoch 199/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3161 - val_loss: 0.5469\n",
      "Epoch 200/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3518 - val_loss: 0.6360\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_8 (Bidirection (None, 24, 12)            684       \n",
      "_________________________________________________________________\n",
      "layer_normalization_6 (Layer (None, 24, 12)            24        \n",
      "_________________________________________________________________\n",
      "bidirectional_9 (Bidirection (None, 12)                684       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 1,405\n",
      "Trainable params: 1,405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Saved\n",
      "Predict time:  0.38796520233154297\n",
      "RMSE:  20.164063698473417\n",
      "RMSE2:  17.434528209449088\n",
      "MAE:  15.15715168657757\n",
      "MAE2:  15.15715168657757\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'mse score')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "init = glorot_normal(seed=None) # 給 LSTM\n",
    "init_d = RandomUniform(minval=-0.05, maxval=0.05) # 給 Dense layer\n",
    "nadam = optimizers.Nadam(lr=0.0015,clipvalue=0.5)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(GRU(6, kernel_initializer=init ,return_sequences = True,kernel_regularizer=regularizers.l2(0.01)\n",
    "                             ,recurrent_regularizer = regularizers.l2(0.01) ,input_shape=(x_train.shape[1],x_train.shape[2]))))\n",
    "model.add(LayerNormalization())\n",
    "model.add(Bidirectional(GRU(6,kernel_initializer=init,kernel_regularizer=regularizers.l2(0.01),recurrent_regularizer = regularizers.l2(0.01))))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(units=1, kernel_initializer=init_d))\n",
    "model.compile(optimizer = nadam , loss=\"mse\")\n",
    "history = model.fit(x_train, y_train, epochs=200, batch_size=24, validation_split=0.1, shuffle=True)\n",
    "#model summary\n",
    "model.summary()\n",
    "#Save Model\n",
    "model.save('GRU_model_DUKE.h5')  # creates a HDF5 file \n",
    "print('Model Saved')\n",
    "del model  # deletes the existing model\n",
    "\n",
    "custom_ob = {'LayerNormalization': LayerNormalization , 'SeqSelfAttention':SeqSelfAttention}\n",
    "model = load_model('GRU_model_DUKE.h5', custom_objects=custom_ob)\n",
    "t1 = time.time()\n",
    "# y_pred = model.predict([x_test,x_test])\n",
    "y_pred2 = model.predict(x_test)\n",
    "y_pred = model.predict(x_train)\n",
    "t2 = time.time()\n",
    "print('Predict time: ',t2-t1)\n",
    "y_pred = scaler.inverse_transform(y_pred)#Undo scaling\n",
    "rmse_lstm2 = np.sqrt(mean_squared_error(y_test, y_pred2))\n",
    "rmse_lstm = np.sqrt(mean_squared_error(y_train, y_pred))\n",
    "print('RMSE: ',rmse_lstm)\n",
    "print('RMSE2: ',rmse_lstm2)\n",
    "mae = mean_absolute_error(y_test, y_pred2)\n",
    "mae = mean_absolute_error(y_train, y_pred)\n",
    "print('MAE: ',mae)\n",
    "print('MAE2: ',mae)\n",
    "# r22 =  r2_score(y_test, y_pred2)\n",
    "# r2 =  r2_score(y_train, y_pred)\n",
    "# print('R-square: ',r2)\n",
    "# print('R-square2: ',r22)\n",
    "\n",
    "# n = len(y_test)\n",
    "# p = 12\n",
    "# Adj_r2 = 1-(1-r2)*(n-1)/(n-p-1)\n",
    "# Adj_r22 = 1-(1-r22)*(n-1)/(n-p-1)\n",
    "# print('Adj R-square: ',Adj_r2)\n",
    "# print('Adj R-square2: ',Adj_r22)\n",
    "\n",
    "plt.plot(history.history[\"loss\"],label=\"loss\")\n",
    "plt.plot(history.history[\"val_loss\"],label=\"val_loss\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"No. Of Epochs\")\n",
    "plt.ylabel(\"mse score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58e662f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef9587e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
