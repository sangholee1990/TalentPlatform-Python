{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85563f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from tcn import TCN\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error, r2_score,mean_absolute_error\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential , load_model , Model\n",
    "from keras.layers import Dense, Dropout , LSTM , Bidirectional ,GRU ,Flatten,Add,BatchNormalization\n",
    "from keras_layer_normalization import LayerNormalization\n",
    "from keras.initializers import  glorot_normal, RandomUniform\n",
    "from keras import optimizers,Input\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "661234cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:32: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160, 13) (144, 13) (40, 13)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d0c24c3db954daab59870162b2cb5da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:44: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a24d6329b2f4e46ae9b57b618e7c3c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:\n",
      "(120, 24, 12) (120,)\n",
      "Test size:\n",
      "(16, 24, 12) (16,)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"station_bike _Soho.csv\")\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "df = df.set_index(\"timestamp\")\n",
    "#df.head()\n",
    "\n",
    "df[\"hour\"] = df.index.hour\n",
    "df[\"day_of_month\"] = df.index.day\n",
    "df[\"day_of_week\"]  = df.index.dayofweek\n",
    "df[\"month\"] = df.index.month\n",
    "\n",
    "training_data_len = math.ceil(len(df) * 0.9) # taking 90% of data to train and 10% of data to test\n",
    "testing_data_len = len(df) - training_data_len\n",
    "\n",
    "time_steps = 24\n",
    "train, test = df.iloc[0:training_data_len], df.iloc[(training_data_len-time_steps):len(df)]\n",
    "print(df.shape, train.shape, test.shape)\n",
    "train_trans = train[['t1','t2', 'hum', 'wind_speed']].to_numpy()\n",
    "test_trans = test[['t1','t2', 'hum', 'wind_speed']].to_numpy()\n",
    "\n",
    "scaler = RobustScaler() # Handles outliers\n",
    "#scaler = MinMaxScaler(feature_range=(0, 1)) # scale to (0,1)\n",
    "train.loc[:, ['t1','t2','hum', 'wind_speed']]=scaler.fit_transform(train_trans)\n",
    "test.loc[:, ['t1','t2', 'hum', 'wind_speed']]=scaler.fit_transform(test_trans)\n",
    "\n",
    "train['cnt'] = scaler.fit_transform(train[['cnt']])\n",
    "test['cnt'] = scaler.fit_transform(test[['cnt']])\n",
    "\n",
    "#Split the data into x_train and y_train data sets\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in tqdm(range(len(train) - time_steps)):\n",
    "    x_train.append(train.drop(columns='cnt').iloc[i:i + time_steps].to_numpy())\n",
    "    y_train.append(train.loc[:,'cnt'].iloc[i + time_steps])\n",
    "\n",
    "#Convert x_train and y_train to numpy arrays\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "#Create the x_test and y_test data sets\n",
    "x_test = []\n",
    "y_test = df.loc[:,'cnt'].iloc[training_data_len:len(df)]\n",
    "\n",
    "for i in tqdm(range(len(test) - time_steps)):\n",
    "    x_test.append(test.drop(columns='cnt').iloc[i:i + time_steps].to_numpy())\n",
    "    # y_test.append(test.loc[:,'cnt'].iloc[i + time_steps])\n",
    "\n",
    "#Convert x_test and y_test to numpy arrays\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# All 12 columns of the data\n",
    "print('Train size:')\n",
    "print(x_train.shape, y_train.shape)\n",
    "print('Test size:')\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b2b3d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 24, 12)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 24, 24)       1800        input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "seq_self_attention_3 (SeqSelfAt (None, 24, 24)       577         bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 24, 24)       0           seq_self_attention_3[0][0]       \n",
      "                                                                 bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_3 (LayerNor (None, 24, 24)       48          add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 576)          0           layer_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 12)           6924        flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 12)           0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 588)          0           dropout_3[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            589         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 1)            2           dense_8[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 9,940\n",
      "Trainable params: 9,940\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Concatenate\n",
    "init = glorot_normal(seed=None) # 給 GRU\n",
    "init_d = RandomUniform(minval=-0.05, maxval=0.05) # 給 Dense layer\n",
    "\n",
    "def Encoder(layer):\n",
    "    shortcut = layer\n",
    "    layer = SeqSelfAttention(\n",
    "                     attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n",
    "                     kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     bias_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     attention_regularizer_weight=0.0001)(layer)\n",
    "    layer = Add()([layer,shortcut])\n",
    "    layer = LayerNormalization()(layer)\n",
    "    layer = Flatten()(layer)\n",
    "    \n",
    "    shortcut2 = layer\n",
    "    layer = Dense(12,kernel_initializer=init_d)(layer)\n",
    "    layer = Dropout(0.15)(layer)\n",
    "    layer = Concatenate()([layer,shortcut2])\n",
    "    output = Dense(1,kernel_initializer=init_d)(layer)\n",
    "    return output\n",
    "\n",
    "def Decoder(layer):\n",
    "    shortcut = layer\n",
    "    layer = SeqSelfAttention(\n",
    "                     attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n",
    "                     kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     bias_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     attention_regularizer_weight=0.0001)(layer)\n",
    "    layer = Add()([layer,shortcut])\n",
    "    layer = LayerNormalization()(layer)\n",
    "    layer = SeqSelfAttention(\n",
    "                     attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n",
    "                     kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     bias_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     attention_regularizer_weight=0.0001)(layer)\n",
    "    layer = LayerNormalization()(layer)\n",
    "    \n",
    "    layer = Flatten()(layer)\n",
    "    shortcut2 = layer\n",
    "    layer = Dense(10,kernel_initializer=init_d)(layer)\n",
    "    #layer = Dropout(0.2)(layer)\n",
    "    layer = Concatenate()([layer,shortcut2])\n",
    "    output = Dense(1,kernel_initializer=init_d)(layer)\n",
    "    return output\n",
    "\n",
    "def Bi_GRU(layer,unit):\n",
    "    output = Bidirectional(GRU(unit, dropout=0.1, recurrent_dropout=0.1, return_sequences=True,\n",
    "                            kernel_initializer=init))(layer)\n",
    "    return output\n",
    "\n",
    "#start = Input(shape = (x_train.shape[1],x_train.shape[2]))\n",
    "start = Input(shape = (x_train.shape[1:]))\n",
    "start2 = Input(shape = (x_train.shape[1:]))\n",
    "x = Bi_GRU(start,12)\n",
    "x = Encoder(x)\n",
    "\n",
    "# y = Bi_GRU(start2,8)\n",
    "# y = Decoder(y)\n",
    "\n",
    "#Merge = Add()([x,x])\n",
    "Last = Dense(1)(x)\n",
    "model = Model([start,start2] , Last)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa68264b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 108 samples, validate on 12 samples\n",
      "Epoch 1/500\n",
      "108/108 [==============================] - 2s 19ms/step - loss: 0.7637 - val_loss: 0.6613\n",
      "Epoch 2/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.6699 - val_loss: 0.5960\n",
      "Epoch 3/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.6143 - val_loss: 0.5504\n",
      "Epoch 4/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.5430 - val_loss: 0.6947\n",
      "Epoch 5/500\n",
      "108/108 [==============================] - 1s 8ms/step - loss: 0.5333 - val_loss: 0.5294\n",
      "Epoch 6/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.5252 - val_loss: 0.4377\n",
      "Epoch 7/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.5338 - val_loss: 0.4992\n",
      "Epoch 8/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.4817 - val_loss: 0.4998\n",
      "Epoch 9/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.4492 - val_loss: 0.3942\n",
      "Epoch 10/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.4257 - val_loss: 0.4787\n",
      "Epoch 11/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.4192 - val_loss: 0.3602\n",
      "Epoch 12/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.3664 - val_loss: 0.2828\n",
      "Epoch 13/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.3780 - val_loss: 0.3119\n",
      "Epoch 14/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.3274 - val_loss: 0.4146\n",
      "Epoch 15/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.3499 - val_loss: 0.2856\n",
      "Epoch 16/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.3124 - val_loss: 0.2133\n",
      "Epoch 17/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.2737 - val_loss: 0.2647\n",
      "Epoch 18/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.2672 - val_loss: 0.3412\n",
      "Epoch 19/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.2345 - val_loss: 0.2919\n",
      "Epoch 20/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.2795 - val_loss: 0.2863\n",
      "Epoch 21/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.2567 - val_loss: 0.2964\n",
      "Epoch 22/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.2195 - val_loss: 0.2808\n",
      "Epoch 23/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.2425 - val_loss: 0.2040\n",
      "Epoch 24/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1853 - val_loss: 0.1797\n",
      "Epoch 25/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.2514 - val_loss: 0.2834\n",
      "Epoch 26/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.2097 - val_loss: 0.2097\n",
      "Epoch 27/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.2304 - val_loss: 0.1582\n",
      "Epoch 28/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1552 - val_loss: 0.2434\n",
      "Epoch 29/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.2158 - val_loss: 0.1567\n",
      "Epoch 30/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1574 - val_loss: 0.2636\n",
      "Epoch 31/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1355 - val_loss: 0.2075\n",
      "Epoch 32/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1796 - val_loss: 0.2162\n",
      "Epoch 33/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1644 - val_loss: 0.1959\n",
      "Epoch 34/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1646 - val_loss: 0.2195\n",
      "Epoch 35/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1300 - val_loss: 0.2110\n",
      "Epoch 36/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1180 - val_loss: 0.1785\n",
      "Epoch 37/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1379 - val_loss: 0.2021\n",
      "Epoch 38/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1002 - val_loss: 0.1765\n",
      "Epoch 39/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1304 - val_loss: 0.2038\n",
      "Epoch 40/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1382 - val_loss: 0.1762\n",
      "Epoch 41/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1333 - val_loss: 0.1879\n",
      "Epoch 42/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1182 - val_loss: 0.1536\n",
      "Epoch 43/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1314 - val_loss: 0.1788\n",
      "Epoch 44/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1343 - val_loss: 0.1866\n",
      "Epoch 45/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1239 - val_loss: 0.2572\n",
      "Epoch 46/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0945 - val_loss: 0.2530\n",
      "Epoch 47/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1222 - val_loss: 0.2585\n",
      "Epoch 48/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1382 - val_loss: 0.1509\n",
      "Epoch 49/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1256 - val_loss: 0.1335\n",
      "Epoch 50/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0858 - val_loss: 0.1708\n",
      "Epoch 51/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1101 - val_loss: 0.2740\n",
      "Epoch 52/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1192 - val_loss: 0.1941\n",
      "Epoch 53/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1429 - val_loss: 0.1642\n",
      "Epoch 54/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1299 - val_loss: 0.2035\n",
      "Epoch 55/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1123 - val_loss: 0.2684\n",
      "Epoch 56/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1057 - val_loss: 0.2144\n",
      "Epoch 57/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0958 - val_loss: 0.1313\n",
      "Epoch 58/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0838 - val_loss: 0.2697\n",
      "Epoch 59/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1094 - val_loss: 0.2712\n",
      "Epoch 60/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1201 - val_loss: 0.2572\n",
      "Epoch 61/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1154 - val_loss: 0.2468\n",
      "Epoch 62/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0919 - val_loss: 0.1988\n",
      "Epoch 63/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1048 - val_loss: 0.1912\n",
      "Epoch 64/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0810 - val_loss: 0.1855\n",
      "Epoch 65/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0863 - val_loss: 0.2308\n",
      "Epoch 66/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0928 - val_loss: 0.1472\n",
      "Epoch 67/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1016 - val_loss: 0.1834\n",
      "Epoch 68/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0659 - val_loss: 0.2907\n",
      "Epoch 69/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.1075 - val_loss: 0.2407\n",
      "Epoch 70/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0676 - val_loss: 0.1907\n",
      "Epoch 71/500\n",
      "108/108 [==============================] - 1s 8ms/step - loss: 0.0632 - val_loss: 0.2110\n",
      "Epoch 72/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1126 - val_loss: 0.1553\n",
      "Epoch 73/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0904 - val_loss: 0.1663\n",
      "Epoch 74/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0787 - val_loss: 0.2061\n",
      "Epoch 75/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0722 - val_loss: 0.2270\n",
      "Epoch 76/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0846 - val_loss: 0.2644\n",
      "Epoch 77/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0869 - val_loss: 0.1547\n",
      "Epoch 78/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1266 - val_loss: 0.1736\n",
      "Epoch 79/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0823 - val_loss: 0.1670\n",
      "Epoch 80/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1232 - val_loss: 0.3147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0748 - val_loss: 0.2804\n",
      "Epoch 82/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0577 - val_loss: 0.1848\n",
      "Epoch 83/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0778 - val_loss: 0.1558\n",
      "Epoch 84/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0718 - val_loss: 0.2228\n",
      "Epoch 85/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0694 - val_loss: 0.1480\n",
      "Epoch 86/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0737 - val_loss: 0.1690\n",
      "Epoch 87/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0844 - val_loss: 0.2636\n",
      "Epoch 88/500\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.049 - 1s 7ms/step - loss: 0.0511 - val_loss: 0.1760\n",
      "Epoch 89/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0825 - val_loss: 0.2321\n",
      "Epoch 90/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0713 - val_loss: 0.2046\n",
      "Epoch 91/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0779 - val_loss: 0.2501\n",
      "Epoch 92/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0692 - val_loss: 0.2711\n",
      "Epoch 93/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0840 - val_loss: 0.2114\n",
      "Epoch 94/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0759 - val_loss: 0.2789\n",
      "Epoch 95/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1047 - val_loss: 0.1826\n",
      "Epoch 96/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0747 - val_loss: 0.2169\n",
      "Epoch 97/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0704 - val_loss: 0.2157\n",
      "Epoch 98/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0680 - val_loss: 0.2613\n",
      "Epoch 99/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0957 - val_loss: 0.1865\n",
      "Epoch 100/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0639 - val_loss: 0.2385\n",
      "Epoch 101/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0603 - val_loss: 0.2228\n",
      "Epoch 102/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0625 - val_loss: 0.3119\n",
      "Epoch 103/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0823 - val_loss: 0.3294\n",
      "Epoch 104/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0881 - val_loss: 0.2130\n",
      "Epoch 105/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0662 - val_loss: 0.2866\n",
      "Epoch 106/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0658 - val_loss: 0.2938\n",
      "Epoch 107/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0636 - val_loss: 0.3109\n",
      "Epoch 108/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0760 - val_loss: 0.2576\n",
      "Epoch 109/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0909 - val_loss: 0.2476\n",
      "Epoch 110/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0590 - val_loss: 0.2002\n",
      "Epoch 111/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0695 - val_loss: 0.2564\n",
      "Epoch 112/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0537 - val_loss: 0.2377\n",
      "Epoch 113/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0764 - val_loss: 0.3017\n",
      "Epoch 114/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0701 - val_loss: 0.2803\n",
      "Epoch 115/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0650 - val_loss: 0.1912\n",
      "Epoch 116/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0960 - val_loss: 0.2743\n",
      "Epoch 117/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0814 - val_loss: 0.3252\n",
      "Epoch 118/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0853 - val_loss: 0.2993\n",
      "Epoch 119/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0548 - val_loss: 0.2954\n",
      "Epoch 120/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0480 - val_loss: 0.1981\n",
      "Epoch 121/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0668 - val_loss: 0.1946\n",
      "Epoch 122/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0512 - val_loss: 0.2050\n",
      "Epoch 123/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0749 - val_loss: 0.2156\n",
      "Epoch 124/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0412 - val_loss: 0.2033\n",
      "Epoch 125/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0815 - val_loss: 0.2481\n",
      "Epoch 126/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0553 - val_loss: 0.1991\n",
      "Epoch 127/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0604 - val_loss: 0.2422\n",
      "Epoch 128/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0700 - val_loss: 0.2033\n",
      "Epoch 129/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0682 - val_loss: 0.2385\n",
      "Epoch 130/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0483 - val_loss: 0.2092\n",
      "Epoch 131/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1013 - val_loss: 0.2541\n",
      "Epoch 132/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0893 - val_loss: 0.1826\n",
      "Epoch 133/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0802 - val_loss: 0.1582\n",
      "Epoch 134/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0505 - val_loss: 0.2325\n",
      "Epoch 135/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0729 - val_loss: 0.2442\n",
      "Epoch 136/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0486 - val_loss: 0.1819\n",
      "Epoch 137/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0577 - val_loss: 0.2189\n",
      "Epoch 138/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0744 - val_loss: 0.2477\n",
      "Epoch 139/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0567 - val_loss: 0.1787\n",
      "Epoch 140/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0602 - val_loss: 0.1883\n",
      "Epoch 141/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0518 - val_loss: 0.2834\n",
      "Epoch 142/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0720 - val_loss: 0.2018\n",
      "Epoch 143/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0448 - val_loss: 0.1999\n",
      "Epoch 144/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0809 - val_loss: 0.2363\n",
      "Epoch 145/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0467 - val_loss: 0.2197\n",
      "Epoch 146/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0597 - val_loss: 0.2889\n",
      "Epoch 147/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0573 - val_loss: 0.2479\n",
      "Epoch 148/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0510 - val_loss: 0.2304\n",
      "Epoch 149/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0814 - val_loss: 0.2635\n",
      "Epoch 150/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0484 - val_loss: 0.2261\n",
      "Epoch 151/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0573 - val_loss: 0.2261\n",
      "Epoch 152/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0797 - val_loss: 0.1864\n",
      "Epoch 153/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0601 - val_loss: 0.2239\n",
      "Epoch 154/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0445 - val_loss: 0.2182\n",
      "Epoch 155/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0598 - val_loss: 0.1960\n",
      "Epoch 156/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0519 - val_loss: 0.2351\n",
      "Epoch 157/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0564 - val_loss: 0.2032\n",
      "Epoch 158/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0516 - val_loss: 0.1908\n",
      "Epoch 159/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0720 - val_loss: 0.2123\n",
      "Epoch 160/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0861 - val_loss: 0.1924\n",
      "Epoch 161/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0574 - val_loss: 0.3091\n",
      "Epoch 162/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0433 - val_loss: 0.3128\n",
      "Epoch 163/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0682 - val_loss: 0.2330\n",
      "Epoch 164/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0402 - val_loss: 0.2436\n",
      "Epoch 165/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0589 - val_loss: 0.2868\n",
      "Epoch 166/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0551 - val_loss: 0.2445\n",
      "Epoch 167/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0442 - val_loss: 0.2803\n",
      "Epoch 168/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0430 - val_loss: 0.2218\n",
      "Epoch 169/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0650 - val_loss: 0.2299\n",
      "Epoch 170/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0570 - val_loss: 0.3050\n",
      "Epoch 171/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0487 - val_loss: 0.2559\n",
      "Epoch 172/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0715 - val_loss: 0.4028\n",
      "Epoch 173/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0599 - val_loss: 0.1970\n",
      "Epoch 174/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0507 - val_loss: 0.2409\n",
      "Epoch 175/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0516 - val_loss: 0.2204\n",
      "Epoch 176/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0618 - val_loss: 0.2099\n",
      "Epoch 177/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0781 - val_loss: 0.1735\n",
      "Epoch 178/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.1012 - val_loss: 0.3009\n",
      "Epoch 179/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0634 - val_loss: 0.2387\n",
      "Epoch 180/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0448 - val_loss: 0.1695\n",
      "Epoch 181/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0434 - val_loss: 0.2152\n",
      "Epoch 182/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0763 - val_loss: 0.2657\n",
      "Epoch 183/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0757 - val_loss: 0.2157\n",
      "Epoch 184/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0632 - val_loss: 0.2271\n",
      "Epoch 185/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0567 - val_loss: 0.2338\n",
      "Epoch 186/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0785 - val_loss: 0.4713\n",
      "Epoch 187/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0486 - val_loss: 0.3143\n",
      "Epoch 188/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0807 - val_loss: 0.2906\n",
      "Epoch 189/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0503 - val_loss: 0.3447\n",
      "Epoch 190/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0447 - val_loss: 0.2509\n",
      "Epoch 191/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0569 - val_loss: 0.3128\n",
      "Epoch 192/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0479 - val_loss: 0.2887\n",
      "Epoch 193/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0488 - val_loss: 0.2787\n",
      "Epoch 194/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0465 - val_loss: 0.3228\n",
      "Epoch 195/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0370 - val_loss: 0.2718\n",
      "Epoch 196/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0495 - val_loss: 0.2944\n",
      "Epoch 197/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0422 - val_loss: 0.2233\n",
      "Epoch 198/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0509 - val_loss: 0.2081\n",
      "Epoch 199/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0398 - val_loss: 0.2580\n",
      "Epoch 200/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0398 - val_loss: 0.2548\n",
      "Epoch 201/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0350 - val_loss: 0.2283\n",
      "Epoch 202/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0401 - val_loss: 0.2177\n",
      "Epoch 203/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0653 - val_loss: 0.1967\n",
      "Epoch 204/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0437 - val_loss: 0.2113\n",
      "Epoch 205/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0464 - val_loss: 0.2205\n",
      "Epoch 206/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0459 - val_loss: 0.2255\n",
      "Epoch 207/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0444 - val_loss: 0.2184\n",
      "Epoch 208/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0444 - val_loss: 0.2412\n",
      "Epoch 209/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0727 - val_loss: 0.2601\n",
      "Epoch 210/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0400 - val_loss: 0.2050\n",
      "Epoch 211/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0452 - val_loss: 0.1925\n",
      "Epoch 212/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0469 - val_loss: 0.2427\n",
      "Epoch 213/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0506 - val_loss: 0.2467\n",
      "Epoch 214/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0449 - val_loss: 0.2171\n",
      "Epoch 215/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0320 - val_loss: 0.2310\n",
      "Epoch 216/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0514 - val_loss: 0.2033\n",
      "Epoch 217/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0427 - val_loss: 0.1944\n",
      "Epoch 218/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0367 - val_loss: 0.2362\n",
      "Epoch 219/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0324 - val_loss: 0.2115\n",
      "Epoch 220/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0393 - val_loss: 0.1711\n",
      "Epoch 221/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0377 - val_loss: 0.2556\n",
      "Epoch 222/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0412 - val_loss: 0.2155\n",
      "Epoch 223/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0709 - val_loss: 0.2621\n",
      "Epoch 224/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0567 - val_loss: 0.2658\n",
      "Epoch 225/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0673 - val_loss: 0.2971\n",
      "Epoch 226/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0380 - val_loss: 0.2365\n",
      "Epoch 227/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0348 - val_loss: 0.2704\n",
      "Epoch 228/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0804 - val_loss: 0.2518\n",
      "Epoch 229/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0424 - val_loss: 0.2208\n",
      "Epoch 230/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0495 - val_loss: 0.2091\n",
      "Epoch 231/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0358 - val_loss: 0.2229\n",
      "Epoch 232/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0454 - val_loss: 0.2254\n",
      "Epoch 233/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0445 - val_loss: 0.2605\n",
      "Epoch 234/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0560 - val_loss: 0.2516\n",
      "Epoch 235/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0626 - val_loss: 0.2965\n",
      "Epoch 236/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0549 - val_loss: 0.2984\n",
      "Epoch 237/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0365 - val_loss: 0.2472\n",
      "Epoch 238/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0310 - val_loss: 0.1676\n",
      "Epoch 239/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0550 - val_loss: 0.2507\n",
      "Epoch 240/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0538 - val_loss: 0.2204\n",
      "Epoch 241/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0471 - val_loss: 0.3239\n",
      "Epoch 242/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0394 - val_loss: 0.2414\n",
      "Epoch 243/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0397 - val_loss: 0.2443\n",
      "Epoch 244/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0411 - val_loss: 0.2053\n",
      "Epoch 245/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0436 - val_loss: 0.2190\n",
      "Epoch 246/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0419 - val_loss: 0.2112\n",
      "Epoch 247/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0381 - val_loss: 0.2744\n",
      "Epoch 248/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0284 - val_loss: 0.2164\n",
      "Epoch 249/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0656 - val_loss: 0.2169\n",
      "Epoch 250/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0392 - val_loss: 0.2644\n",
      "Epoch 251/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0437 - val_loss: 0.2442\n",
      "Epoch 252/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0407 - val_loss: 0.3132\n",
      "Epoch 253/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0418 - val_loss: 0.2058\n",
      "Epoch 254/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0368 - val_loss: 0.2084\n",
      "Epoch 255/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0713 - val_loss: 0.2445\n",
      "Epoch 256/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0289 - val_loss: 0.1853\n",
      "Epoch 257/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0344 - val_loss: 0.2200\n",
      "Epoch 258/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0368 - val_loss: 0.1962\n",
      "Epoch 259/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0360 - val_loss: 0.2690\n",
      "Epoch 260/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0355 - val_loss: 0.2807\n",
      "Epoch 261/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0604 - val_loss: 0.2088\n",
      "Epoch 262/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0405 - val_loss: 0.1644\n",
      "Epoch 263/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0384 - val_loss: 0.1785\n",
      "Epoch 264/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0550 - val_loss: 0.2628\n",
      "Epoch 265/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0337 - val_loss: 0.2266\n",
      "Epoch 266/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0475 - val_loss: 0.2906\n",
      "Epoch 267/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0578 - val_loss: 0.2206\n",
      "Epoch 268/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0472 - val_loss: 0.2955\n",
      "Epoch 269/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0418 - val_loss: 0.2291\n",
      "Epoch 270/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0474 - val_loss: 0.2724\n",
      "Epoch 271/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0456 - val_loss: 0.2040\n",
      "Epoch 272/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0391 - val_loss: 0.2254\n",
      "Epoch 273/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0620 - val_loss: 0.3487\n",
      "Epoch 274/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0522 - val_loss: 0.1847\n",
      "Epoch 275/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0515 - val_loss: 0.1502\n",
      "Epoch 276/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0363 - val_loss: 0.2193\n",
      "Epoch 277/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0321 - val_loss: 0.3403\n",
      "Epoch 278/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0513 - val_loss: 0.2631\n",
      "Epoch 279/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0504 - val_loss: 0.2350\n",
      "Epoch 280/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0350 - val_loss: 0.2266\n",
      "Epoch 281/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0331 - val_loss: 0.1748\n",
      "Epoch 282/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0280 - val_loss: 0.1753\n",
      "Epoch 283/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0438 - val_loss: 0.2624\n",
      "Epoch 284/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0385 - val_loss: 0.1551\n",
      "Epoch 285/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0388 - val_loss: 0.1877\n",
      "Epoch 286/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0305 - val_loss: 0.1606\n",
      "Epoch 287/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0243 - val_loss: 0.2083\n",
      "Epoch 288/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0434 - val_loss: 0.2061\n",
      "Epoch 289/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0267 - val_loss: 0.1583\n",
      "Epoch 290/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0343 - val_loss: 0.2055\n",
      "Epoch 291/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0450 - val_loss: 0.2115\n",
      "Epoch 292/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0384 - val_loss: 0.1981\n",
      "Epoch 293/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0334 - val_loss: 0.1687\n",
      "Epoch 294/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0335 - val_loss: 0.1930\n",
      "Epoch 295/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0359 - val_loss: 0.1719\n",
      "Epoch 296/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0399 - val_loss: 0.1865\n",
      "Epoch 297/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0307 - val_loss: 0.1754\n",
      "Epoch 298/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0238 - val_loss: 0.2255\n",
      "Epoch 299/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0305 - val_loss: 0.1782\n",
      "Epoch 300/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0372 - val_loss: 0.1889\n",
      "Epoch 301/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0366 - val_loss: 0.1720\n",
      "Epoch 302/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0238 - val_loss: 0.1899\n",
      "Epoch 303/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0747 - val_loss: 0.2244\n",
      "Epoch 304/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0357 - val_loss: 0.2324\n",
      "Epoch 305/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0327 - val_loss: 0.1868\n",
      "Epoch 306/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0287 - val_loss: 0.2539\n",
      "Epoch 307/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0411 - val_loss: 0.1823\n",
      "Epoch 308/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0319 - val_loss: 0.2021\n",
      "Epoch 309/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0305 - val_loss: 0.2971\n",
      "Epoch 310/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0340 - val_loss: 0.1885\n",
      "Epoch 311/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0318 - val_loss: 0.2682\n",
      "Epoch 312/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0357 - val_loss: 0.2889\n",
      "Epoch 313/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0300 - val_loss: 0.1861\n",
      "Epoch 314/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0334 - val_loss: 0.1991\n",
      "Epoch 315/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0282 - val_loss: 0.1685\n",
      "Epoch 316/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0324 - val_loss: 0.1719\n",
      "Epoch 317/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0213 - val_loss: 0.2877\n",
      "Epoch 318/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0389 - val_loss: 0.2144\n",
      "Epoch 319/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0307 - val_loss: 0.1354\n",
      "Epoch 320/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0263 - val_loss: 0.2055\n",
      "Epoch 321/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0294 - val_loss: 0.2273\n",
      "Epoch 322/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0251 - val_loss: 0.1946\n",
      "Epoch 323/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0397 - val_loss: 0.1741\n",
      "Epoch 324/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0368 - val_loss: 0.1824\n",
      "Epoch 325/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0485 - val_loss: 0.2599\n",
      "Epoch 326/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0442 - val_loss: 0.1983\n",
      "Epoch 327/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0281 - val_loss: 0.1739\n",
      "Epoch 328/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0276 - val_loss: 0.2518\n",
      "Epoch 329/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0534 - val_loss: 0.2517\n",
      "Epoch 330/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0409 - val_loss: 0.2120\n",
      "Epoch 331/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0246 - val_loss: 0.2004\n",
      "Epoch 332/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0267 - val_loss: 0.2429\n",
      "Epoch 333/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0310 - val_loss: 0.2654\n",
      "Epoch 334/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0415 - val_loss: 0.2359\n",
      "Epoch 335/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0269 - val_loss: 0.2265\n",
      "Epoch 336/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0353 - val_loss: 0.2862\n",
      "Epoch 337/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0338 - val_loss: 0.1691\n",
      "Epoch 338/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0397 - val_loss: 0.1828\n",
      "Epoch 339/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0389 - val_loss: 0.2196\n",
      "Epoch 340/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0250 - val_loss: 0.2143\n",
      "Epoch 341/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0223 - val_loss: 0.2213\n",
      "Epoch 342/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0357 - val_loss: 0.1717\n",
      "Epoch 343/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0320 - val_loss: 0.2308\n",
      "Epoch 344/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0261 - val_loss: 0.1912\n",
      "Epoch 345/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0709 - val_loss: 0.1738\n",
      "Epoch 346/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0408 - val_loss: 0.2357\n",
      "Epoch 347/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0348 - val_loss: 0.2067\n",
      "Epoch 348/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0262 - val_loss: 0.2442\n",
      "Epoch 349/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0453 - val_loss: 0.2277\n",
      "Epoch 350/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0320 - val_loss: 0.1919\n",
      "Epoch 351/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0321 - val_loss: 0.2646\n",
      "Epoch 352/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0269 - val_loss: 0.2163\n",
      "Epoch 353/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0285 - val_loss: 0.1813\n",
      "Epoch 354/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0356 - val_loss: 0.1906\n",
      "Epoch 355/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0240 - val_loss: 0.1760\n",
      "Epoch 356/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0340 - val_loss: 0.1875\n",
      "Epoch 357/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0290 - val_loss: 0.1912\n",
      "Epoch 358/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0477 - val_loss: 0.1997\n",
      "Epoch 359/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0339 - val_loss: 0.2000\n",
      "Epoch 360/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0330 - val_loss: 0.1673\n",
      "Epoch 361/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0294 - val_loss: 0.1630\n",
      "Epoch 362/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0301 - val_loss: 0.1739\n",
      "Epoch 363/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0257 - val_loss: 0.1814\n",
      "Epoch 364/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0209 - val_loss: 0.1631\n",
      "Epoch 365/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0187 - val_loss: 0.2220\n",
      "Epoch 366/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0226 - val_loss: 0.1686\n",
      "Epoch 367/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0306 - val_loss: 0.1616\n",
      "Epoch 368/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0346 - val_loss: 0.1897\n",
      "Epoch 369/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0315 - val_loss: 0.2423\n",
      "Epoch 370/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0338 - val_loss: 0.2719\n",
      "Epoch 371/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0345 - val_loss: 0.1895\n",
      "Epoch 372/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0348 - val_loss: 0.1791\n",
      "Epoch 373/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0238 - val_loss: 0.1935\n",
      "Epoch 374/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0327 - val_loss: 0.2015\n",
      "Epoch 375/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0254 - val_loss: 0.1600\n",
      "Epoch 376/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0197 - val_loss: 0.1890\n",
      "Epoch 377/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0253 - val_loss: 0.1992\n",
      "Epoch 378/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0203 - val_loss: 0.1593\n",
      "Epoch 379/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0256 - val_loss: 0.2089\n",
      "Epoch 380/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0216 - val_loss: 0.3247\n",
      "Epoch 381/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0346 - val_loss: 0.2577\n",
      "Epoch 382/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0192 - val_loss: 0.1837\n",
      "Epoch 383/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0409 - val_loss: 0.2572\n",
      "Epoch 384/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0297 - val_loss: 0.1595\n",
      "Epoch 385/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0248 - val_loss: 0.2048\n",
      "Epoch 386/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0436 - val_loss: 0.1942\n",
      "Epoch 387/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0251 - val_loss: 0.1629\n",
      "Epoch 388/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0501 - val_loss: 0.1484\n",
      "Epoch 389/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0258 - val_loss: 0.1475\n",
      "Epoch 390/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0536 - val_loss: 0.1712\n",
      "Epoch 391/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0279 - val_loss: 0.2105\n",
      "Epoch 392/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0305 - val_loss: 0.1832\n",
      "Epoch 393/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0351 - val_loss: 0.1712\n",
      "Epoch 394/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0281 - val_loss: 0.2040\n",
      "Epoch 395/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0268 - val_loss: 0.2018\n",
      "Epoch 396/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0372 - val_loss: 0.1906\n",
      "Epoch 397/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0207 - val_loss: 0.2554\n",
      "Epoch 398/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0272 - val_loss: 0.1759\n",
      "Epoch 399/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0243 - val_loss: 0.1717\n",
      "Epoch 400/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0222 - val_loss: 0.1934\n",
      "Epoch 401/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0247 - val_loss: 0.1949\n",
      "Epoch 402/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0392 - val_loss: 0.1569\n",
      "Epoch 403/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0250 - val_loss: 0.2363\n",
      "Epoch 404/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0234 - val_loss: 0.1795\n",
      "Epoch 405/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0361 - val_loss: 0.2663\n",
      "Epoch 406/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0222 - val_loss: 0.2345\n",
      "Epoch 407/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0214 - val_loss: 0.2301\n",
      "Epoch 408/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0199 - val_loss: 0.2008\n",
      "Epoch 409/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0192 - val_loss: 0.1744\n",
      "Epoch 410/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0206 - val_loss: 0.1628\n",
      "Epoch 411/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0317 - val_loss: 0.1429\n",
      "Epoch 412/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0366 - val_loss: 0.2077\n",
      "Epoch 413/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0241 - val_loss: 0.2013\n",
      "Epoch 414/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0209 - val_loss: 0.1702\n",
      "Epoch 415/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0212 - val_loss: 0.2048\n",
      "Epoch 416/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0286 - val_loss: 0.1488\n",
      "Epoch 417/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0235 - val_loss: 0.2141\n",
      "Epoch 418/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0315 - val_loss: 0.2468\n",
      "Epoch 419/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0301 - val_loss: 0.1768\n",
      "Epoch 420/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0269 - val_loss: 0.1523\n",
      "Epoch 421/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0303 - val_loss: 0.1642\n",
      "Epoch 422/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0230 - val_loss: 0.1654\n",
      "Epoch 423/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0203 - val_loss: 0.1816\n",
      "Epoch 424/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0253 - val_loss: 0.1958\n",
      "Epoch 425/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0330 - val_loss: 0.1338\n",
      "Epoch 426/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0271 - val_loss: 0.1967\n",
      "Epoch 427/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0349 - val_loss: 0.2359\n",
      "Epoch 428/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0222 - val_loss: 0.1705\n",
      "Epoch 429/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0203 - val_loss: 0.1784\n",
      "Epoch 430/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0272 - val_loss: 0.2054\n",
      "Epoch 431/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0230 - val_loss: 0.1639\n",
      "Epoch 432/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0214 - val_loss: 0.1544\n",
      "Epoch 433/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0260 - val_loss: 0.1599\n",
      "Epoch 434/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0313 - val_loss: 0.1716\n",
      "Epoch 435/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0174 - val_loss: 0.2032\n",
      "Epoch 436/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0220 - val_loss: 0.1723\n",
      "Epoch 437/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0281 - val_loss: 0.1744\n",
      "Epoch 438/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0238 - val_loss: 0.1634\n",
      "Epoch 439/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0246 - val_loss: 0.1669\n",
      "Epoch 440/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0236 - val_loss: 0.2010\n",
      "Epoch 441/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0224 - val_loss: 0.1427\n",
      "Epoch 442/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0206 - val_loss: 0.1630\n",
      "Epoch 443/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0270 - val_loss: 0.1707\n",
      "Epoch 444/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0301 - val_loss: 0.2031\n",
      "Epoch 445/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0294 - val_loss: 0.1819\n",
      "Epoch 446/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0347 - val_loss: 0.1697\n",
      "Epoch 447/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0257 - val_loss: 0.1399\n",
      "Epoch 448/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0248 - val_loss: 0.1729\n",
      "Epoch 449/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0309 - val_loss: 0.2048\n",
      "Epoch 450/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0229 - val_loss: 0.2064\n",
      "Epoch 451/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0156 - val_loss: 0.1673\n",
      "Epoch 452/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0302 - val_loss: 0.1918\n",
      "Epoch 453/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0157 - val_loss: 0.1684\n",
      "Epoch 454/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0205 - val_loss: 0.1574\n",
      "Epoch 455/500\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.0581 - val_loss: 0.1733\n",
      "Epoch 456/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0303 - val_loss: 0.1803\n",
      "Epoch 457/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0172 - val_loss: 0.1851\n",
      "Epoch 458/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0413 - val_loss: 0.2493\n",
      "Epoch 459/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0561 - val_loss: 0.1787\n",
      "Epoch 460/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0265 - val_loss: 0.1921\n",
      "Epoch 461/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0476 - val_loss: 0.1810\n",
      "Epoch 462/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0223 - val_loss: 0.1830\n",
      "Epoch 463/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0309 - val_loss: 0.2032\n",
      "Epoch 464/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0361 - val_loss: 0.1649\n",
      "Epoch 465/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0302 - val_loss: 0.1667\n",
      "Epoch 466/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0218 - val_loss: 0.2038\n",
      "Epoch 467/500\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.0228 - val_loss: 0.3137\n",
      "Epoch 468/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0324 - val_loss: 0.1736\n",
      "Epoch 469/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0286 - val_loss: 0.2017\n",
      "Epoch 470/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0247 - val_loss: 0.1748\n",
      "Epoch 471/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0305 - val_loss: 0.2263\n",
      "Epoch 472/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0196 - val_loss: 0.1991\n",
      "Epoch 473/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0272 - val_loss: 0.1768\n",
      "Epoch 474/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0199 - val_loss: 0.2143\n",
      "Epoch 475/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0338 - val_loss: 0.1755\n",
      "Epoch 476/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0183 - val_loss: 0.1852\n",
      "Epoch 477/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0274 - val_loss: 0.1613\n",
      "Epoch 478/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0245 - val_loss: 0.1462\n",
      "Epoch 479/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0208 - val_loss: 0.1566\n",
      "Epoch 480/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0252 - val_loss: 0.1769\n",
      "Epoch 481/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0272 - val_loss: 0.1647\n",
      "Epoch 482/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0239 - val_loss: 0.1423\n",
      "Epoch 483/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0220 - val_loss: 0.1516\n",
      "Epoch 484/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0176 - val_loss: 0.1982\n",
      "Epoch 485/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0210 - val_loss: 0.1626\n",
      "Epoch 486/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0480 - val_loss: 0.1964\n",
      "Epoch 487/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0187 - val_loss: 0.1527\n",
      "Epoch 488/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0151 - val_loss: 0.1877\n",
      "Epoch 489/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0194 - val_loss: 0.1834\n",
      "Epoch 490/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0227 - val_loss: 0.1808\n",
      "Epoch 491/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0215 - val_loss: 0.1589\n",
      "Epoch 492/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0240 - val_loss: 0.1688\n",
      "Epoch 493/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0218 - val_loss: 0.1560\n",
      "Epoch 494/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0192 - val_loss: 0.1288\n",
      "Epoch 495/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0180 - val_loss: 0.1532\n",
      "Epoch 496/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0369 - val_loss: 0.1672\n",
      "Epoch 497/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0208 - val_loss: 0.1423\n",
      "Epoch 498/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0259 - val_loss: 0.1483\n",
      "Epoch 499/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0263 - val_loss: 0.1475\n",
      "Epoch 500/500\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.0226 - val_loss: 0.1685\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 24, 12)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 24, 24)       1800        input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "seq_self_attention_3 (SeqSelfAt (None, 24, 24)       577         bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 24, 24)       0           seq_self_attention_3[0][0]       \n",
      "                                                                 bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_3 (LayerNor (None, 24, 24)       48          add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 576)          0           layer_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 12)           6924        flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 12)           0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 588)          0           dropout_3[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            589         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 1)            2           dense_8[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 9,940\n",
      "Trainable params: 9,940\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Predict time:  0.27513813972473145\n",
      "RMSE:  18.60344187315346\n",
      "RMSE2:  15.785144189220999\n",
      "MAE:  13.15352331002553\n",
      "MAE2:  13.15352331002553\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'mse score')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "opt = optimizers.Adam(lr=0.001,decay = 0.0001)\n",
    "print('Train...')\n",
    "# model.compile(optimizer = opt , loss=\"mse\")\n",
    "model.compile(optimizer = \"adam\" , loss=\"mse\")\n",
    "history = model.fit([x_train,x_train], y_train, epochs = 500, batch_size=8, validation_split=0.1, shuffle=True)\n",
    "# history = model.fit(x_train, y_train, epochs = 500, batch_size=6, validation_split=0.1, shuffle=True)\n",
    "model.summary()\n",
    "#Save Model\n",
    "model.save('GRU_Single_Attention_model_Soho.h5')  # creates a HDF5 file \n",
    "del model\n",
    "\n",
    "custom_ob = {'LayerNormalization': LayerNormalization , 'SeqSelfAttention':SeqSelfAttention}\n",
    "model = load_model('GRU_Single_Attention_model_Soho.h5', custom_objects=custom_ob)\n",
    "t1 = time.time()\n",
    "# y_pred = model.predict([x_test,x_test])\n",
    "y_pred2 = model.predict(x_test)\n",
    "y_pred = model.predict(x_train)\n",
    "t2 = time.time()\n",
    "print('Predict time: ',t2-t1)\n",
    "y_pred = scaler.inverse_transform(y_pred)#Undo scaling\n",
    "rmse_lstm2 = np.sqrt(mean_squared_error(y_test, y_pred2))\n",
    "rmse_lstm = np.sqrt(mean_squared_error(y_train, y_pred))\n",
    "print('RMSE: ',rmse_lstm)\n",
    "print('RMSE2: ',rmse_lstm2)\n",
    "mae = mean_absolute_error(y_test, y_pred2)\n",
    "mae = mean_absolute_error(y_train, y_pred)\n",
    "print('MAE: ',mae)\n",
    "print('MAE2: ',mae)\n",
    "# r22 =  r2_score(y_test, y_pred2)\n",
    "# r2 =  r2_score(y_train, y_pred)\n",
    "# print('R-square: ',r2)\n",
    "# print('R-square2: ',r22)\n",
    "\n",
    "# n = len(y_test)\n",
    "# p = 12\n",
    "# Adj_r2 = 1-(1-r2)*(n-1)/(n-p-1)\n",
    "# Adj_r22 = 1-(1-r22)*(n-1)/(n-p-1)\n",
    "# print('Adj R-square: ',Adj_r2)\n",
    "# print('Adj R-square2: ',Adj_r22)\n",
    "\n",
    "plt.plot(history.history[\"loss\"],label=\"loss\")\n",
    "plt.plot(history.history[\"val_loss\"],label=\"val_loss\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"No. Of Epochs\")\n",
    "plt.ylabel(\"mse score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d34b2744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 108 samples, validate on 12 samples\n",
      "Epoch 1/200\n",
      "108/108 [==============================] - 2s 22ms/step - loss: 1.3543 - val_loss: 1.4991\n",
      "Epoch 2/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.3140 - val_loss: 1.4301\n",
      "Epoch 3/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.2927 - val_loss: 1.3809\n",
      "Epoch 4/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.2374 - val_loss: 1.3246\n",
      "Epoch 5/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.2098 - val_loss: 1.2778\n",
      "Epoch 6/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.1638 - val_loss: 1.2292\n",
      "Epoch 7/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.1217 - val_loss: 1.1859\n",
      "Epoch 8/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.0923 - val_loss: 1.1507\n",
      "Epoch 9/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.0565 - val_loss: 1.1106\n",
      "Epoch 10/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.0150 - val_loss: 1.0703\n",
      "Epoch 11/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.9823 - val_loss: 1.0338\n",
      "Epoch 12/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.9456 - val_loss: 0.9994\n",
      "Epoch 13/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.9026 - val_loss: 0.9619\n",
      "Epoch 14/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.8503 - val_loss: 0.9260\n",
      "Epoch 15/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.8354 - val_loss: 0.8969\n",
      "Epoch 16/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.8037 - val_loss: 0.8666\n",
      "Epoch 17/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.7853 - val_loss: 0.8426\n",
      "Epoch 18/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.7702 - val_loss: 0.8205\n",
      "Epoch 19/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.7227 - val_loss: 0.7958\n",
      "Epoch 20/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.7074 - val_loss: 0.7746\n",
      "Epoch 21/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.7029 - val_loss: 0.7550\n",
      "Epoch 22/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.6756 - val_loss: 0.7373\n",
      "Epoch 23/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6703 - val_loss: 0.7222\n",
      "Epoch 24/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6454 - val_loss: 0.7081\n",
      "Epoch 25/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6223 - val_loss: 0.6940\n",
      "Epoch 26/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.6239 - val_loss: 0.6815\n",
      "Epoch 27/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.5948 - val_loss: 0.6674\n",
      "Epoch 28/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.5868 - val_loss: 0.6536\n",
      "Epoch 29/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.5770 - val_loss: 0.6409\n",
      "Epoch 30/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.5581 - val_loss: 0.6306\n",
      "Epoch 31/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.5474 - val_loss: 0.6211\n",
      "Epoch 32/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.5366 - val_loss: 0.6105\n",
      "Epoch 33/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.5292 - val_loss: 0.6006\n",
      "Epoch 34/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5257 - val_loss: 0.5931\n",
      "Epoch 35/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5147 - val_loss: 0.5851\n",
      "Epoch 36/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5010 - val_loss: 0.5770\n",
      "Epoch 37/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.4914 - val_loss: 0.5690\n",
      "Epoch 38/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.4720 - val_loss: 0.5623\n",
      "Epoch 39/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4741 - val_loss: 0.5561\n",
      "Epoch 40/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.4848 - val_loss: 0.5473\n",
      "Epoch 41/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4747 - val_loss: 0.5405\n",
      "Epoch 42/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4629 - val_loss: 0.5323\n",
      "Epoch 43/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4667 - val_loss: 0.5251\n",
      "Epoch 44/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4559 - val_loss: 0.5189\n",
      "Epoch 45/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.4386 - val_loss: 0.5143\n",
      "Epoch 46/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4531 - val_loss: 0.5112\n",
      "Epoch 47/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.4386 - val_loss: 0.5080\n",
      "Epoch 48/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4273 - val_loss: 0.5031\n",
      "Epoch 49/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.4294 - val_loss: 0.4975\n",
      "Epoch 50/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4186 - val_loss: 0.4918\n",
      "Epoch 51/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.4045 - val_loss: 0.4894\n",
      "Epoch 52/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4229 - val_loss: 0.4835\n",
      "Epoch 53/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.4216 - val_loss: 0.4799\n",
      "Epoch 54/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3897 - val_loss: 0.4786\n",
      "Epoch 55/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4014 - val_loss: 0.4784\n",
      "Epoch 56/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3896 - val_loss: 0.4723\n",
      "Epoch 57/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.3796 - val_loss: 0.4667\n",
      "Epoch 58/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3877 - val_loss: 0.4664\n",
      "Epoch 59/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.3989 - val_loss: 0.4644\n",
      "Epoch 60/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3879 - val_loss: 0.4606\n",
      "Epoch 61/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.3843 - val_loss: 0.4585\n",
      "Epoch 62/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3764 - val_loss: 0.4590\n",
      "Epoch 63/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3725 - val_loss: 0.4587\n",
      "Epoch 64/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3865 - val_loss: 0.4552\n",
      "Epoch 65/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.3593 - val_loss: 0.4559\n",
      "Epoch 66/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3897 - val_loss: 0.4513\n",
      "Epoch 67/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3553 - val_loss: 0.4515\n",
      "Epoch 68/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.3607 - val_loss: 0.4485\n",
      "Epoch 69/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3348 - val_loss: 0.4446\n",
      "Epoch 70/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3427 - val_loss: 0.4453\n",
      "Epoch 71/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3591 - val_loss: 0.4443\n",
      "Epoch 72/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.3365 - val_loss: 0.4351\n",
      "Epoch 73/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3417 - val_loss: 0.4372\n",
      "Epoch 74/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.3447 - val_loss: 0.4347\n",
      "Epoch 75/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3329 - val_loss: 0.4322\n",
      "Epoch 76/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.3355 - val_loss: 0.4312\n",
      "Epoch 77/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3166 - val_loss: 0.4355\n",
      "Epoch 78/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3332 - val_loss: 0.4288\n",
      "Epoch 79/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.3385 - val_loss: 0.4252\n",
      "Epoch 80/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3174 - val_loss: 0.4409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.3197 - val_loss: 0.4173\n",
      "Epoch 82/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3233 - val_loss: 0.4086\n",
      "Epoch 83/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.3109 - val_loss: 0.4217\n",
      "Epoch 84/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2996 - val_loss: 0.4218\n",
      "Epoch 85/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3123 - val_loss: 0.4462\n",
      "Epoch 86/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.3183 - val_loss: 0.4257\n",
      "Epoch 87/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2883 - val_loss: 0.4110\n",
      "Epoch 88/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3035 - val_loss: 0.4051\n",
      "Epoch 89/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3103 - val_loss: 0.4103\n",
      "Epoch 90/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2768 - val_loss: 0.4105\n",
      "Epoch 91/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2878 - val_loss: 0.4121\n",
      "Epoch 92/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2932 - val_loss: 0.3769\n",
      "Epoch 93/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2834 - val_loss: 0.3808\n",
      "Epoch 94/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2610 - val_loss: 0.3853\n",
      "Epoch 95/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.3049 - val_loss: 0.4078\n",
      "Epoch 96/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.2814 - val_loss: 0.5227\n",
      "Epoch 97/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2674 - val_loss: 0.4510\n",
      "Epoch 98/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2764 - val_loss: 0.4326\n",
      "Epoch 99/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2546 - val_loss: 0.4024\n",
      "Epoch 100/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2869 - val_loss: 0.3760\n",
      "Epoch 101/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.2540 - val_loss: 0.4199\n",
      "Epoch 102/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.2574 - val_loss: 0.3790\n",
      "Epoch 103/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2587 - val_loss: 0.3830\n",
      "Epoch 104/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.2651 - val_loss: 0.4362\n",
      "Epoch 105/200\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.2702 - val_loss: 0.3991\n",
      "Epoch 106/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2556 - val_loss: 0.3848\n",
      "Epoch 107/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2537 - val_loss: 0.3559\n",
      "Epoch 108/200\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.2633 - val_loss: 0.4553\n",
      "Epoch 109/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2554 - val_loss: 0.4274\n",
      "Epoch 110/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2162 - val_loss: 0.3873\n",
      "Epoch 111/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2517 - val_loss: 0.4106\n",
      "Epoch 112/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2576 - val_loss: 0.4061\n",
      "Epoch 113/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2511 - val_loss: 0.4933\n",
      "Epoch 114/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2542 - val_loss: 0.4722\n",
      "Epoch 115/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2444 - val_loss: 0.3886\n",
      "Epoch 116/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2271 - val_loss: 0.3868\n",
      "Epoch 117/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2447 - val_loss: 0.5435\n",
      "Epoch 118/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2385 - val_loss: 0.3685\n",
      "Epoch 119/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2161 - val_loss: 0.3688\n",
      "Epoch 120/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2232 - val_loss: 0.4184\n",
      "Epoch 121/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2241 - val_loss: 0.4117\n",
      "Epoch 122/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2168 - val_loss: 0.4278\n",
      "Epoch 123/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2273 - val_loss: 0.3852\n",
      "Epoch 124/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2251 - val_loss: 0.4151\n",
      "Epoch 125/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2286 - val_loss: 0.5760\n",
      "Epoch 126/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2258 - val_loss: 0.3909\n",
      "Epoch 127/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2247 - val_loss: 0.3912\n",
      "Epoch 128/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2174 - val_loss: 0.3131\n",
      "Epoch 129/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2453 - val_loss: 0.4555\n",
      "Epoch 130/200\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.212 - 0s 4ms/step - loss: 0.2317 - val_loss: 0.3638\n",
      "Epoch 131/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.2010 - val_loss: 0.3405\n",
      "Epoch 132/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2461 - val_loss: 0.4109\n",
      "Epoch 133/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2087 - val_loss: 0.3646\n",
      "Epoch 134/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2031 - val_loss: 0.3522\n",
      "Epoch 135/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2125 - val_loss: 0.3721\n",
      "Epoch 136/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2016 - val_loss: 0.3014\n",
      "Epoch 137/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2175 - val_loss: 0.4099\n",
      "Epoch 138/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2194 - val_loss: 0.3502\n",
      "Epoch 139/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.1997 - val_loss: 0.3395\n",
      "Epoch 140/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2214 - val_loss: 0.3350\n",
      "Epoch 141/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2221 - val_loss: 0.3026\n",
      "Epoch 142/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2179 - val_loss: 0.4720\n",
      "Epoch 143/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2117 - val_loss: 0.3871\n",
      "Epoch 144/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2168 - val_loss: 0.3176\n",
      "Epoch 145/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2197 - val_loss: 0.3227\n",
      "Epoch 146/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2111 - val_loss: 0.3466\n",
      "Epoch 147/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.1828 - val_loss: 0.3972\n",
      "Epoch 148/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.1969 - val_loss: 0.3760\n",
      "Epoch 149/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.1823 - val_loss: 0.4572\n",
      "Epoch 150/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2048 - val_loss: 0.4446\n",
      "Epoch 151/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.2243 - val_loss: 0.4010\n",
      "Epoch 152/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.2061 - val_loss: 0.4735\n",
      "Epoch 153/200\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.1829 - val_loss: 0.5033\n",
      "Epoch 154/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.2107 - val_loss: 0.3404\n",
      "Epoch 155/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.1924 - val_loss: 0.4803\n",
      "Epoch 156/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.2017 - val_loss: 0.3596\n",
      "Epoch 157/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.1728 - val_loss: 0.3334\n",
      "Epoch 158/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.1841 - val_loss: 0.4566\n",
      "Epoch 159/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.1773 - val_loss: 0.3275\n",
      "Epoch 160/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 0s 3ms/step - loss: 0.1890 - val_loss: 0.3580\n",
      "Epoch 161/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.2099 - val_loss: 0.3765\n",
      "Epoch 162/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.1817 - val_loss: 0.3787\n",
      "Epoch 163/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.1821 - val_loss: 0.4049\n",
      "Epoch 164/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.1904 - val_loss: 0.3700\n",
      "Epoch 165/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.1890 - val_loss: 0.4445\n",
      "Epoch 166/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.1925 - val_loss: 0.3572\n",
      "Epoch 167/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.1802 - val_loss: 0.3945\n",
      "Epoch 168/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.1714 - val_loss: 0.4874\n",
      "Epoch 169/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.1879 - val_loss: 0.3403\n",
      "Epoch 170/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.1871 - val_loss: 0.4444\n",
      "Epoch 171/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.1869 - val_loss: 0.3668\n",
      "Epoch 172/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.1848 - val_loss: 0.4314\n",
      "Epoch 173/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.1907 - val_loss: 0.3232\n",
      "Epoch 174/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.1572 - val_loss: 0.2920\n",
      "Epoch 175/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.1838 - val_loss: 0.3141\n",
      "Epoch 176/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.1940 - val_loss: 0.4010\n",
      "Epoch 177/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.1815 - val_loss: 0.4754\n",
      "Epoch 178/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.1851 - val_loss: 0.3284\n",
      "Epoch 179/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.2028 - val_loss: 0.3972\n",
      "Epoch 180/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.1651 - val_loss: 0.3101\n",
      "Epoch 181/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.1965 - val_loss: 0.3473\n",
      "Epoch 182/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.1633 - val_loss: 0.3548\n",
      "Epoch 183/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.1799 - val_loss: 0.3251\n",
      "Epoch 184/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.1915 - val_loss: 0.3322\n",
      "Epoch 185/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.1844 - val_loss: 0.3726\n",
      "Epoch 186/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.1715 - val_loss: 0.3203\n",
      "Epoch 187/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.2062 - val_loss: 0.3659\n",
      "Epoch 188/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.1815 - val_loss: 0.3521\n",
      "Epoch 189/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.1723 - val_loss: 0.3292\n",
      "Epoch 190/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.1958 - val_loss: 0.3515\n",
      "Epoch 191/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.1850 - val_loss: 0.3145\n",
      "Epoch 192/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.1731 - val_loss: 0.4313\n",
      "Epoch 193/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.1577 - val_loss: 0.3728\n",
      "Epoch 194/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.1663 - val_loss: 0.4338\n",
      "Epoch 195/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.1566 - val_loss: 0.3431\n",
      "Epoch 196/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.1672 - val_loss: 0.3362\n",
      "Epoch 197/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.1897 - val_loss: 0.3621\n",
      "Epoch 198/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.1552 - val_loss: 0.3517\n",
      "Epoch 199/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.1811 - val_loss: 0.3885\n",
      "Epoch 200/200\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 0.1580 - val_loss: 0.2512\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_10 (Bidirectio (None, 24, 12)            684       \n",
      "_________________________________________________________________\n",
      "layer_normalization_7 (Layer (None, 24, 12)            24        \n",
      "_________________________________________________________________\n",
      "bidirectional_11 (Bidirectio (None, 12)                684       \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 1,405\n",
      "Trainable params: 1,405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Saved\n",
      "Predict time:  0.3445103168487549\n",
      "RMSE:  17.347479913418166\n",
      "RMSE2:  15.898797267850798\n",
      "MAE:  13.99752211271088\n",
      "MAE2:  13.99752211271088\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'mse score')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "init = glorot_normal(seed=None) # 給 LSTM\n",
    "init_d = RandomUniform(minval=-0.05, maxval=0.05) # 給 Dense layer\n",
    "nadam = optimizers.Nadam(lr=0.0015,clipvalue=0.5)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(GRU(6, kernel_initializer=init ,return_sequences = True,kernel_regularizer=regularizers.l2(0.01)\n",
    "                             ,recurrent_regularizer = regularizers.l2(0.01) ,input_shape=(x_train.shape[1],x_train.shape[2]))))\n",
    "model.add(LayerNormalization())\n",
    "model.add(Bidirectional(GRU(6,kernel_initializer=init,kernel_regularizer=regularizers.l2(0.01),recurrent_regularizer = regularizers.l2(0.01))))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Dense(units=1, kernel_initializer=init_d))\n",
    "model.compile(optimizer = nadam , loss=\"mse\")\n",
    "history = model.fit(x_train, y_train, epochs=200, batch_size=24, validation_split=0.1, shuffle=True)\n",
    "#model summary\n",
    "model.summary()\n",
    "#Save Model\n",
    "model.save('GRU_model_soho.h5')  # creates a HDF5 file \n",
    "print('Model Saved')\n",
    "del model  # deletes the existing model\n",
    "\n",
    "custom_ob = {'LayerNormalization': LayerNormalization , 'SeqSelfAttention':SeqSelfAttention}\n",
    "model = load_model('GRU_model_soho.h5', custom_objects=custom_ob)\n",
    "t1 = time.time()\n",
    "# y_pred = model.predict([x_test,x_test])\n",
    "y_pred2 = model.predict(x_test)\n",
    "y_pred = model.predict(x_train)\n",
    "t2 = time.time()\n",
    "print('Predict time: ',t2-t1)\n",
    "y_pred = scaler.inverse_transform(y_pred)#Undo scaling\n",
    "rmse_lstm2 = np.sqrt(mean_squared_error(y_test, y_pred2))\n",
    "rmse_lstm = np.sqrt(mean_squared_error(y_train, y_pred))\n",
    "print('RMSE: ',rmse_lstm)\n",
    "print('RMSE2: ',rmse_lstm2)\n",
    "mae = mean_absolute_error(y_test, y_pred2)\n",
    "mae = mean_absolute_error(y_train, y_pred)\n",
    "print('MAE: ',mae)\n",
    "print('MAE2: ',mae)\n",
    "# r22 =  r2_score(y_test, y_pred2)\n",
    "# r2 =  r2_score(y_train, y_pred)\n",
    "# print('R-square: ',r2)\n",
    "# print('R-square2: ',r22)\n",
    "\n",
    "# n = len(y_test)\n",
    "# p = 12\n",
    "# Adj_r2 = 1-(1-r2)*(n-1)/(n-p-1)\n",
    "# Adj_r22 = 1-(1-r22)*(n-1)/(n-p-1)\n",
    "# print('Adj R-square: ',Adj_r2)\n",
    "# print('Adj R-square2: ',Adj_r22)\n",
    "\n",
    "plt.plot(history.history[\"loss\"],label=\"loss\")\n",
    "plt.plot(history.history[\"val_loss\"],label=\"val_loss\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"No. Of Epochs\")\n",
    "plt.ylabel(\"mse score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c0a454",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c20abe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
