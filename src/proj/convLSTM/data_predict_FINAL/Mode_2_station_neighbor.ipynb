{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6e738fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from tcn import TCN\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error, r2_score,mean_absolute_error\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential , load_model , Model\n",
    "from keras.layers import Dense, Dropout , LSTM , Bidirectional ,GRU ,Flatten,Add,BatchNormalization\n",
    "from keras_layer_normalization import LayerNormalization\n",
    "from keras.initializers import  glorot_normal, RandomUniform\n",
    "from keras import optimizers,Input\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02678187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(735, 16) (662, 16) (97, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:32: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c567f5c351c4e2bab6ec89292e0e4e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/638 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:44: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cd932e5ecfb45b6ac1e5ac830570712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:\n",
      "(638, 24, 15) (638,)\n",
      "Test size:\n",
      "(73, 24, 15) (73,)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"Mode2_bike_neighbor.csv\")\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "df = df.set_index(\"timestamp\")\n",
    "#df.head()\n",
    "\n",
    "df[\"hour\"] = df.index.hour\n",
    "df[\"day_of_month\"] = df.index.day\n",
    "df[\"day_of_week\"]  = df.index.dayofweek\n",
    "df[\"month\"] = df.index.month\n",
    "\n",
    "training_data_len = math.ceil(len(df) * 0.9) # taking 90% of data to train and 10% of data to test\n",
    "testing_data_len = len(df) - training_data_len\n",
    "\n",
    "time_steps = 24\n",
    "train, test = df.iloc[0:training_data_len], df.iloc[(training_data_len-time_steps):len(df)]\n",
    "print(df.shape, train.shape, test.shape)\n",
    "train_trans = train[['t1','t2', 'hum', 'wind_speed']].to_numpy()\n",
    "test_trans = test[['t1','t2', 'hum', 'wind_speed']].to_numpy()\n",
    "\n",
    "scaler = RobustScaler() # Handles outliers\n",
    "#scaler = MinMaxScaler(feature_range=(0, 1)) # scale to (0,1)\n",
    "train.loc[:, ['t1','t2','hum', 'wind_speed']]=scaler.fit_transform(train_trans)\n",
    "test.loc[:, ['t1','t2', 'hum', 'wind_speed']]=scaler.fit_transform(test_trans)\n",
    "\n",
    "train['cnt'] = scaler.fit_transform(train[['cnt']])\n",
    "test['cnt'] = scaler.fit_transform(test[['cnt']])\n",
    "\n",
    "#Split the data into x_train and y_train data sets\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in tqdm(range(len(train) - time_steps)):\n",
    "    x_train.append(train.drop(columns='cnt').iloc[i:i + time_steps].to_numpy())\n",
    "    y_train.append(train.loc[:,'cnt'].iloc[i + time_steps])\n",
    "\n",
    "#Convert x_train and y_train to numpy arrays\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "#Create the x_test and y_test data sets\n",
    "x_test = []\n",
    "y_test = df.loc[:,'cnt'].iloc[training_data_len:len(df)]\n",
    "\n",
    "for i in tqdm(range(len(test) - time_steps)):\n",
    "    x_test.append(test.drop(columns='cnt').iloc[i:i + time_steps].to_numpy())\n",
    "    # y_test.append(test.loc[:,'cnt'].iloc[i + time_steps])\n",
    "\n",
    "#Convert x_test and y_test to numpy arrays\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# All 12 columns of the data\n",
    "print('Train size:')\n",
    "print(x_train.shape, y_train.shape)\n",
    "print('Test size:')\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "314982aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 24, 15)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 24, 32)       3072        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 24, 15)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "seq_self_attention_2 (SeqSelfAt (None, 24, 32)       1025        bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 24, 64)       9216        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 24, 32)       0           seq_self_attention_2[0][0]       \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "seq_self_attention_1 (SeqSelfAt (None, 24, 64)       4097        bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_2 (LayerNor (None, 24, 32)       64          add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 24, 64)       0           seq_self_attention_1[0][0]       \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "seq_self_attention_3 (SeqSelfAt (None, 24, 32)       1025        layer_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 24, 64)       128         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_3 (LayerNor (None, 24, 32)       64          seq_self_attention_3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 1536)         0           layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 768)          0           layer_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           24592       flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           12304       flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 16)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 16)           0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1552)         0           dropout_1[0][0]                  \n",
      "                                                                 flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 784)          0           dropout_2[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            1553        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            785         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 1)            0           dense_2[0][0]                    \n",
      "                                                                 dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            2           add_3[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 57,927\n",
      "Trainable params: 57,927\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Concatenate\n",
    "init = glorot_normal(seed=None) # 給 GRU\n",
    "init_d = RandomUniform(minval=-0.05, maxval=0.05) # 給 Dense layer\n",
    "\n",
    "def Encoder(layer):\n",
    "    shortcut = layer\n",
    "    layer = SeqSelfAttention(\n",
    "                     attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n",
    "                     kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     bias_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     attention_regularizer_weight=0.0001)(layer)\n",
    "    layer = Add()([layer,shortcut])\n",
    "    layer = LayerNormalization()(layer)\n",
    "    layer = Flatten()(layer)\n",
    "    \n",
    "    shortcut2 = layer\n",
    "    layer = Dense(16,kernel_initializer=init_d)(layer)\n",
    "    layer = Dropout(0.5)(layer)\n",
    "    layer = Concatenate()([layer,shortcut2])\n",
    "    output = Dense(1,kernel_initializer=init_d)(layer)\n",
    "    return output\n",
    "\n",
    "def Decoder(layer):\n",
    "    shortcut = layer\n",
    "    layer = SeqSelfAttention(\n",
    "                     attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n",
    "                     kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     bias_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     attention_regularizer_weight=0.0001)(layer)\n",
    "    layer = Add()([layer,shortcut])\n",
    "    layer = LayerNormalization()(layer)\n",
    "    layer = SeqSelfAttention(\n",
    "                     attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n",
    "                     kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     bias_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                     attention_regularizer_weight=0.0001)(layer)\n",
    "    layer = LayerNormalization()(layer)\n",
    "    \n",
    "    layer = Flatten()(layer)\n",
    "    shortcut2 = layer\n",
    "    layer = Dense(16,kernel_initializer=init_d)(layer)\n",
    "    layer = Dropout(0.25)(layer)\n",
    "    layer = Concatenate()([layer,shortcut2])\n",
    "    output = Dense(1,kernel_initializer=init_d)(layer)\n",
    "    return output\n",
    "\n",
    "def Bi_GRU(layer,unit):\n",
    "    output = Bidirectional(GRU(unit, dropout=0.25, recurrent_dropout=0.25, return_sequences=True,\n",
    "                            kernel_initializer=init))(layer)\n",
    "    return output\n",
    "\n",
    "#start = Input(shape = (x_train.shape[1],x_train.shape[2]))\n",
    "start = Input(shape = (x_train.shape[1:]))\n",
    "start2 = Input(shape = (x_train.shape[1:]))\n",
    "x = Bi_GRU(start,32)\n",
    "x = Encoder(x)\n",
    "\n",
    "y = Bi_GRU(start2,16)\n",
    "y = Decoder(y)\n",
    "\n",
    "Merge = Add()([x,y])\n",
    "Last = Dense(1)(Merge)\n",
    "model = Model([start,start2] , Last)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fa435e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 574 samples, validate on 64 samples\n",
      "Epoch 1/500\n",
      "574/574 [==============================] - 4s 7ms/step - loss: 2.7518 - val_loss: 6.7304\n",
      "Epoch 2/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 2.0012 - val_loss: 2.5922\n",
      "Epoch 3/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.9105 - val_loss: 3.7887\n",
      "Epoch 4/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.9059 - val_loss: 1.9763\n",
      "Epoch 5/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.8086 - val_loss: 3.0070\n",
      "Epoch 6/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.7480 - val_loss: 2.2661\n",
      "Epoch 7/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.6892 - val_loss: 1.9823\n",
      "Epoch 8/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.6537 - val_loss: 2.9850\n",
      "Epoch 9/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.5819 - val_loss: 3.0344\n",
      "Epoch 10/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.5509 - val_loss: 2.3262\n",
      "Epoch 11/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.5584 - val_loss: 2.0304\n",
      "Epoch 12/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.5371 - val_loss: 2.7007\n",
      "Epoch 13/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.4807 - val_loss: 2.7953\n",
      "Epoch 14/500\n",
      "574/574 [==============================] - 1s 3ms/step - loss: 1.4277 - val_loss: 2.9919\n",
      "Epoch 15/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.4398 - val_loss: 3.2986\n",
      "Epoch 16/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.4347 - val_loss: 3.4949\n",
      "Epoch 17/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.3822 - val_loss: 2.7322\n",
      "Epoch 18/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.3440 - val_loss: 3.3059\n",
      "Epoch 19/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.3158 - val_loss: 3.2392\n",
      "Epoch 20/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.3354 - val_loss: 2.5721\n",
      "Epoch 21/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.3043 - val_loss: 1.6784\n",
      "Epoch 22/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.2812 - val_loss: 2.3895\n",
      "Epoch 23/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.2454 - val_loss: 1.8176\n",
      "Epoch 24/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.2598 - val_loss: 1.9928\n",
      "Epoch 25/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.1962 - val_loss: 2.1072\n",
      "Epoch 26/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.1876 - val_loss: 3.9596\n",
      "Epoch 27/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.2071 - val_loss: 2.0260\n",
      "Epoch 28/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.1640 - val_loss: 3.5030\n",
      "Epoch 29/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.1640 - val_loss: 4.4772\n",
      "Epoch 30/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.1273 - val_loss: 5.1169\n",
      "Epoch 31/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.1389 - val_loss: 2.8402\n",
      "Epoch 32/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.0832 - val_loss: 4.7729\n",
      "Epoch 33/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.1383 - val_loss: 3.9919\n",
      "Epoch 34/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.0832 - val_loss: 3.2395\n",
      "Epoch 35/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.0395 - val_loss: 2.9800\n",
      "Epoch 36/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.0417 - val_loss: 3.1550\n",
      "Epoch 37/500\n",
      "574/574 [==============================] - 1s 3ms/step - loss: 1.0582 - val_loss: 4.1095\n",
      "Epoch 38/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.0408 - val_loss: 3.9892\n",
      "Epoch 39/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.0422 - val_loss: 2.7117\n",
      "Epoch 40/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 1.0143 - val_loss: 2.7985\n",
      "Epoch 41/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.9768 - val_loss: 3.0482\n",
      "Epoch 42/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.9959 - val_loss: 1.6242\n",
      "Epoch 43/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.9694 - val_loss: 2.1465\n",
      "Epoch 44/500\n",
      "574/574 [==============================] - 1s 3ms/step - loss: 0.9544 - val_loss: 1.5700\n",
      "Epoch 45/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.9504 - val_loss: 1.2746\n",
      "Epoch 46/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.9324 - val_loss: 2.2657\n",
      "Epoch 47/500\n",
      "574/574 [==============================] - 1s 3ms/step - loss: 0.9521 - val_loss: 2.5829\n",
      "Epoch 48/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.9383 - val_loss: 1.7430\n",
      "Epoch 49/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.8990 - val_loss: 2.3267\n",
      "Epoch 50/500\n",
      "574/574 [==============================] - 1s 3ms/step - loss: 0.9033 - val_loss: 2.5007\n",
      "Epoch 51/500\n",
      "574/574 [==============================] - 1s 3ms/step - loss: 0.8898 - val_loss: 2.7915\n",
      "Epoch 52/500\n",
      "574/574 [==============================] - 1s 3ms/step - loss: 0.8864 - val_loss: 1.8507\n",
      "Epoch 53/500\n",
      "574/574 [==============================] - 1s 3ms/step - loss: 0.8514 - val_loss: 2.0684\n",
      "Epoch 54/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.8571 - val_loss: 2.2174\n",
      "Epoch 55/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.8729 - val_loss: 1.7936\n",
      "Epoch 56/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.8827 - val_loss: 2.1732\n",
      "Epoch 57/500\n",
      "574/574 [==============================] - 1s 3ms/step - loss: 0.8613 - val_loss: 1.5949\n",
      "Epoch 58/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.8449 - val_loss: 1.3771\n",
      "Epoch 59/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.8111 - val_loss: 2.0518\n",
      "Epoch 60/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7944 - val_loss: 2.6947\n",
      "Epoch 61/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.8050 - val_loss: 1.2546\n",
      "Epoch 62/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.8094 - val_loss: 1.7573\n",
      "Epoch 63/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.8241 - val_loss: 2.1436\n",
      "Epoch 64/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7877 - val_loss: 2.0582\n",
      "Epoch 65/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7778 - val_loss: 1.5299\n",
      "Epoch 66/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7617 - val_loss: 2.1948\n",
      "Epoch 67/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7861 - val_loss: 3.2569\n",
      "Epoch 68/500\n",
      "574/574 [==============================] - 1s 3ms/step - loss: 0.7681 - val_loss: 2.2465\n",
      "Epoch 69/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7500 - val_loss: 2.9554\n",
      "Epoch 70/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7624 - val_loss: 2.7057\n",
      "Epoch 71/500\n",
      "574/574 [==============================] - 1s 3ms/step - loss: 0.7478 - val_loss: 2.2429\n",
      "Epoch 72/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7489 - val_loss: 3.2256\n",
      "Epoch 73/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7312 - val_loss: 2.5075\n",
      "Epoch 74/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7396 - val_loss: 3.3179\n",
      "Epoch 75/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7075 - val_loss: 2.3650\n",
      "Epoch 76/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7087 - val_loss: 1.8849\n",
      "Epoch 77/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7043 - val_loss: 2.8057\n",
      "Epoch 78/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7176 - val_loss: 2.5833\n",
      "Epoch 79/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6992 - val_loss: 2.7845\n",
      "Epoch 80/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7032 - val_loss: 3.0450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.7054 - val_loss: 2.7745\n",
      "Epoch 82/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6733 - val_loss: 2.3700\n",
      "Epoch 83/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6786 - val_loss: 3.4513\n",
      "Epoch 84/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6905 - val_loss: 3.4240\n",
      "Epoch 85/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6719 - val_loss: 2.9689\n",
      "Epoch 86/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6755 - val_loss: 2.7325\n",
      "Epoch 87/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6867 - val_loss: 3.9516\n",
      "Epoch 88/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6674 - val_loss: 4.0855\n",
      "Epoch 89/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6720 - val_loss: 2.8588\n",
      "Epoch 90/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6601 - val_loss: 3.1833\n",
      "Epoch 91/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6408 - val_loss: 3.5126\n",
      "Epoch 92/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6536 - val_loss: 3.2429\n",
      "Epoch 93/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6097 - val_loss: 3.1142\n",
      "Epoch 94/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6423 - val_loss: 2.3618\n",
      "Epoch 95/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5966 - val_loss: 2.5051\n",
      "Epoch 96/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6653 - val_loss: 2.6613\n",
      "Epoch 97/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5905 - val_loss: 2.1744\n",
      "Epoch 98/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6164 - val_loss: 3.8014\n",
      "Epoch 99/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5883 - val_loss: 2.0664\n",
      "Epoch 100/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5949 - val_loss: 2.9199\n",
      "Epoch 101/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5924 - val_loss: 2.6017\n",
      "Epoch 102/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6152 - val_loss: 2.2110\n",
      "Epoch 103/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5963 - val_loss: 1.9441\n",
      "Epoch 104/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5720 - val_loss: 1.8318\n",
      "Epoch 105/500\n",
      "574/574 [==============================] - 1s 3ms/step - loss: 0.5659 - val_loss: 1.9322\n",
      "Epoch 106/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5922 - val_loss: 1.7203\n",
      "Epoch 107/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5585 - val_loss: 1.5613\n",
      "Epoch 108/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5352 - val_loss: 1.2112\n",
      "Epoch 109/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5667 - val_loss: 1.2166\n",
      "Epoch 110/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5592 - val_loss: 1.7924\n",
      "Epoch 111/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5601 - val_loss: 1.3398\n",
      "Epoch 112/500\n",
      "574/574 [==============================] - 1s 3ms/step - loss: 0.5351 - val_loss: 1.4594\n",
      "Epoch 113/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5198 - val_loss: 1.3389\n",
      "Epoch 114/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.6004 - val_loss: 1.1331\n",
      "Epoch 115/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5268 - val_loss: 1.4315\n",
      "Epoch 116/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5266 - val_loss: 1.3515\n",
      "Epoch 117/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5172 - val_loss: 1.2716\n",
      "Epoch 118/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5206 - val_loss: 1.4458\n",
      "Epoch 119/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4966 - val_loss: 2.1244\n",
      "Epoch 120/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5131 - val_loss: 1.7198\n",
      "Epoch 121/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5101 - val_loss: 1.8424\n",
      "Epoch 122/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5127 - val_loss: 1.7877\n",
      "Epoch 123/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5338 - val_loss: 1.3617\n",
      "Epoch 124/500\n",
      "574/574 [==============================] - 1s 3ms/step - loss: 0.5052 - val_loss: 1.7251\n",
      "Epoch 125/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5069 - val_loss: 1.7269\n",
      "Epoch 126/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4887 - val_loss: 1.2239\n",
      "Epoch 127/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5237 - val_loss: 1.2402\n",
      "Epoch 128/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4946 - val_loss: 1.2612\n",
      "Epoch 129/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4834 - val_loss: 1.3891\n",
      "Epoch 130/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4551 - val_loss: 1.4717\n",
      "Epoch 131/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5026 - val_loss: 1.4939\n",
      "Epoch 132/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4694 - val_loss: 1.6291\n",
      "Epoch 133/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4912 - val_loss: 1.6115\n",
      "Epoch 134/500\n",
      "574/574 [==============================] - 1s 3ms/step - loss: 0.4835 - val_loss: 1.1383\n",
      "Epoch 135/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4532 - val_loss: 1.3438\n",
      "Epoch 136/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.5009 - val_loss: 1.4746\n",
      "Epoch 137/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4758 - val_loss: 1.3435\n",
      "Epoch 138/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4929 - val_loss: 1.5649\n",
      "Epoch 139/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4656 - val_loss: 1.5764\n",
      "Epoch 140/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4647 - val_loss: 1.8292\n",
      "Epoch 141/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4771 - val_loss: 1.4048\n",
      "Epoch 142/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4674 - val_loss: 1.7544\n",
      "Epoch 143/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4518 - val_loss: 1.4927\n",
      "Epoch 144/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4649 - val_loss: 1.1799\n",
      "Epoch 145/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4411 - val_loss: 1.5223\n",
      "Epoch 146/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4554 - val_loss: 1.3275\n",
      "Epoch 147/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4495 - val_loss: 1.3626\n",
      "Epoch 148/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4422 - val_loss: 1.4762\n",
      "Epoch 149/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4490 - val_loss: 1.4891\n",
      "Epoch 150/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4504 - val_loss: 1.8060\n",
      "Epoch 151/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4420 - val_loss: 1.4051\n",
      "Epoch 152/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4266 - val_loss: 1.5469\n",
      "Epoch 153/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4184 - val_loss: 2.1496\n",
      "Epoch 154/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4229 - val_loss: 2.0999\n",
      "Epoch 155/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4345 - val_loss: 2.0992\n",
      "Epoch 156/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4186 - val_loss: 1.5020\n",
      "Epoch 157/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4077 - val_loss: 2.1563\n",
      "Epoch 158/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4048 - val_loss: 1.6401\n",
      "Epoch 159/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4113 - val_loss: 2.4122\n",
      "Epoch 160/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4339 - val_loss: 2.1134\n",
      "Epoch 161/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4075 - val_loss: 1.8309\n",
      "Epoch 162/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4247 - val_loss: 1.9639\n",
      "Epoch 163/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4051 - val_loss: 1.3669\n",
      "Epoch 164/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3897 - val_loss: 1.2315\n",
      "Epoch 165/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4276 - val_loss: 1.1202\n",
      "Epoch 166/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3921 - val_loss: 1.4739\n",
      "Epoch 167/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4015 - val_loss: 1.2472\n",
      "Epoch 168/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4082 - val_loss: 1.2663\n",
      "Epoch 169/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4202 - val_loss: 1.3411\n",
      "Epoch 170/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3852 - val_loss: 1.1831\n",
      "Epoch 171/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3722 - val_loss: 1.7457\n",
      "Epoch 172/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3865 - val_loss: 1.1885\n",
      "Epoch 173/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4006 - val_loss: 1.0862\n",
      "Epoch 174/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3748 - val_loss: 1.0756\n",
      "Epoch 175/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.4056 - val_loss: 1.0639\n",
      "Epoch 176/500\n",
      "574/574 [==============================] - 1s 3ms/step - loss: 0.3704 - val_loss: 1.0730\n",
      "Epoch 177/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3757 - val_loss: 1.1067\n",
      "Epoch 178/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3709 - val_loss: 0.9822\n",
      "Epoch 179/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3623 - val_loss: 1.0059\n",
      "Epoch 180/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3617 - val_loss: 0.8674\n",
      "Epoch 181/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3395 - val_loss: 1.0806\n",
      "Epoch 182/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3781 - val_loss: 1.0065\n",
      "Epoch 183/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3518 - val_loss: 0.9126\n",
      "Epoch 184/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3657 - val_loss: 0.7789\n",
      "Epoch 185/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3637 - val_loss: 0.8584\n",
      "Epoch 186/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3358 - val_loss: 1.0124\n",
      "Epoch 187/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3724 - val_loss: 1.0694\n",
      "Epoch 188/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3414 - val_loss: 0.9859\n",
      "Epoch 189/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3368 - val_loss: 1.2644\n",
      "Epoch 190/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3434 - val_loss: 0.9280\n",
      "Epoch 191/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3371 - val_loss: 1.1217\n",
      "Epoch 192/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3319 - val_loss: 1.0876\n",
      "Epoch 193/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3329 - val_loss: 1.0960\n",
      "Epoch 194/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3448 - val_loss: 1.0345\n",
      "Epoch 195/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3360 - val_loss: 0.9975\n",
      "Epoch 196/500\n",
      "574/574 [==============================] - 1s 3ms/step - loss: 0.3416 - val_loss: 0.9854\n",
      "Epoch 197/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3245 - val_loss: 0.9248\n",
      "Epoch 198/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3313 - val_loss: 1.0507\n",
      "Epoch 199/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3134 - val_loss: 1.0831\n",
      "Epoch 200/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3512 - val_loss: 1.2586\n",
      "Epoch 201/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3338 - val_loss: 0.9389\n",
      "Epoch 202/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3129 - val_loss: 0.9201\n",
      "Epoch 203/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3004 - val_loss: 1.0394\n",
      "Epoch 204/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3101 - val_loss: 0.9446\n",
      "Epoch 205/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3241 - val_loss: 0.9414\n",
      "Epoch 206/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3158 - val_loss: 0.8131\n",
      "Epoch 207/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3344 - val_loss: 0.7948\n",
      "Epoch 208/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3103 - val_loss: 0.8402\n",
      "Epoch 209/500\n",
      "574/574 [==============================] - 1s 3ms/step - loss: 0.3030 - val_loss: 0.9274\n",
      "Epoch 210/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3267 - val_loss: 0.9195\n",
      "Epoch 211/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3014 - val_loss: 0.9060\n",
      "Epoch 212/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2974 - val_loss: 0.9789\n",
      "Epoch 213/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3023 - val_loss: 0.8823\n",
      "Epoch 214/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3263 - val_loss: 0.9347\n",
      "Epoch 215/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3127 - val_loss: 0.9535\n",
      "Epoch 216/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2917 - val_loss: 1.0679\n",
      "Epoch 217/500\n",
      "574/574 [==============================] - 1s 3ms/step - loss: 0.2933 - val_loss: 0.9334\n",
      "Epoch 218/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3099 - val_loss: 0.9565\n",
      "Epoch 219/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3068 - val_loss: 0.9899\n",
      "Epoch 220/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3043 - val_loss: 1.1678\n",
      "Epoch 221/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2858 - val_loss: 1.2419\n",
      "Epoch 222/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2999 - val_loss: 1.2651\n",
      "Epoch 223/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2859 - val_loss: 1.1708\n",
      "Epoch 224/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2861 - val_loss: 1.5741\n",
      "Epoch 225/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2954 - val_loss: 1.3687\n",
      "Epoch 226/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.3007 - val_loss: 1.2152\n",
      "Epoch 227/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2847 - val_loss: 1.1975\n",
      "Epoch 228/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2946 - val_loss: 1.1819\n",
      "Epoch 229/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2962 - val_loss: 1.2148\n",
      "Epoch 230/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2797 - val_loss: 1.2181\n",
      "Epoch 231/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2923 - val_loss: 1.1152\n",
      "Epoch 232/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2922 - val_loss: 0.9653\n",
      "Epoch 233/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2706 - val_loss: 0.8940\n",
      "Epoch 234/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2779 - val_loss: 0.9235\n",
      "Epoch 235/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2704 - val_loss: 0.8526\n",
      "Epoch 236/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2705 - val_loss: 0.9390\n",
      "Epoch 237/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2732 - val_loss: 0.8755\n",
      "Epoch 238/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2633 - val_loss: 0.8725\n",
      "Epoch 239/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2610 - val_loss: 0.8484\n",
      "Epoch 240/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2524 - val_loss: 0.8448\n",
      "Epoch 241/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2649 - val_loss: 0.8111\n",
      "Epoch 242/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2586 - val_loss: 0.7375\n",
      "Epoch 243/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2483 - val_loss: 0.7612\n",
      "Epoch 244/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2539 - val_loss: 0.7465\n",
      "Epoch 245/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2629 - val_loss: 0.7671\n",
      "Epoch 246/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2664 - val_loss: 0.7045\n",
      "Epoch 247/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2629 - val_loss: 0.7829\n",
      "Epoch 248/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2448 - val_loss: 0.8228\n",
      "Epoch 249/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2406 - val_loss: 0.9535\n",
      "Epoch 250/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2475 - val_loss: 0.8163\n",
      "Epoch 251/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2430 - val_loss: 0.8150\n",
      "Epoch 252/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2699 - val_loss: 0.7866\n",
      "Epoch 253/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2420 - val_loss: 0.7453\n",
      "Epoch 254/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2639 - val_loss: 0.7069\n",
      "Epoch 255/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2652 - val_loss: 0.7090\n",
      "Epoch 256/500\n",
      "574/574 [==============================] - 1s 3ms/step - loss: 0.2788 - val_loss: 0.7121\n",
      "Epoch 257/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2491 - val_loss: 0.7716\n",
      "Epoch 258/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2647 - val_loss: 0.7895\n",
      "Epoch 259/500\n",
      "574/574 [==============================] - 1s 3ms/step - loss: 0.2537 - val_loss: 0.7166\n",
      "Epoch 260/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2495 - val_loss: 0.7252\n",
      "Epoch 261/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2409 - val_loss: 0.7646\n",
      "Epoch 262/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2475 - val_loss: 0.8358\n",
      "Epoch 263/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2490 - val_loss: 0.7807\n",
      "Epoch 264/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2371 - val_loss: 0.8328\n",
      "Epoch 265/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2452 - val_loss: 0.9446\n",
      "Epoch 266/500\n",
      "574/574 [==============================] - 1s 3ms/step - loss: 0.2221 - val_loss: 0.8736\n",
      "Epoch 267/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2376 - val_loss: 0.7880\n",
      "Epoch 268/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2405 - val_loss: 0.7246\n",
      "Epoch 269/500\n",
      "574/574 [==============================] - 1s 3ms/step - loss: 0.2485 - val_loss: 0.6655\n",
      "Epoch 270/500\n",
      "574/574 [==============================] - 1s 3ms/step - loss: 0.2288 - val_loss: 0.6960\n",
      "Epoch 271/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2321 - val_loss: 0.7287\n",
      "Epoch 272/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2498 - val_loss: 0.8169\n",
      "Epoch 273/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2191 - val_loss: 0.9022\n",
      "Epoch 274/500\n",
      "574/574 [==============================] - 1s 3ms/step - loss: 0.2162 - val_loss: 0.7746\n",
      "Epoch 275/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2371 - val_loss: 0.7593\n",
      "Epoch 276/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2433 - val_loss: 0.7247\n",
      "Epoch 277/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2290 - val_loss: 0.6306\n",
      "Epoch 278/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2253 - val_loss: 0.7051\n",
      "Epoch 279/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2385 - val_loss: 0.6746\n",
      "Epoch 280/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2309 - val_loss: 0.8168\n",
      "Epoch 281/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2041 - val_loss: 0.6978\n",
      "Epoch 282/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2322 - val_loss: 0.7948\n",
      "Epoch 283/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2254 - val_loss: 0.6866\n",
      "Epoch 284/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2111 - val_loss: 0.7151\n",
      "Epoch 285/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2292 - val_loss: 0.7000\n",
      "Epoch 286/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2254 - val_loss: 0.7191\n",
      "Epoch 287/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2415 - val_loss: 0.6313\n",
      "Epoch 288/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2444 - val_loss: 0.5993\n",
      "Epoch 289/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2277 - val_loss: 0.8185\n",
      "Epoch 290/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2230 - val_loss: 0.8450\n",
      "Epoch 291/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2184 - val_loss: 0.6644\n",
      "Epoch 292/500\n",
      "574/574 [==============================] - 1s 3ms/step - loss: 0.2217 - val_loss: 0.7852\n",
      "Epoch 293/500\n",
      "574/574 [==============================] - 1s 3ms/step - loss: 0.2160 - val_loss: 0.7712\n",
      "Epoch 294/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2122 - val_loss: 0.7767\n",
      "Epoch 295/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2271 - val_loss: 0.7523\n",
      "Epoch 296/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2279 - val_loss: 0.9142\n",
      "Epoch 297/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2231 - val_loss: 0.6912\n",
      "Epoch 298/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2258 - val_loss: 0.7818\n",
      "Epoch 299/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2116 - val_loss: 0.7563\n",
      "Epoch 300/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2232 - val_loss: 0.7979\n",
      "Epoch 301/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2176 - val_loss: 0.8568\n",
      "Epoch 302/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2326 - val_loss: 0.8087\n",
      "Epoch 303/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2121 - val_loss: 0.7385\n",
      "Epoch 304/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2143 - val_loss: 0.8424\n",
      "Epoch 305/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2009 - val_loss: 0.8472\n",
      "Epoch 306/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1984 - val_loss: 0.7013\n",
      "Epoch 307/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1900 - val_loss: 0.6718\n",
      "Epoch 308/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2227 - val_loss: 0.6974\n",
      "Epoch 309/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2068 - val_loss: 0.8267\n",
      "Epoch 310/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2315 - val_loss: 0.7939\n",
      "Epoch 311/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2089 - val_loss: 0.7784\n",
      "Epoch 312/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1970 - val_loss: 0.6843\n",
      "Epoch 313/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2027 - val_loss: 0.6663\n",
      "Epoch 314/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1982 - val_loss: 0.7056\n",
      "Epoch 315/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2077 - val_loss: 0.6834\n",
      "Epoch 316/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2011 - val_loss: 0.6354\n",
      "Epoch 317/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1964 - val_loss: 0.8224\n",
      "Epoch 318/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2032 - val_loss: 0.7358\n",
      "Epoch 319/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1883 - val_loss: 0.7199\n",
      "Epoch 320/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1878 - val_loss: 0.6834\n",
      "Epoch 321/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1979 - val_loss: 0.6773\n",
      "Epoch 322/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2103 - val_loss: 0.6367\n",
      "Epoch 323/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2090 - val_loss: 0.6500\n",
      "Epoch 324/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2050 - val_loss: 0.6534\n",
      "Epoch 325/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2144 - val_loss: 0.7185\n",
      "Epoch 326/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2106 - val_loss: 0.6898\n",
      "Epoch 327/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2003 - val_loss: 0.6761\n",
      "Epoch 328/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2024 - val_loss: 0.6224\n",
      "Epoch 329/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2028 - val_loss: 0.6616\n",
      "Epoch 330/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2065 - val_loss: 0.6046\n",
      "Epoch 331/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1963 - val_loss: 0.7259\n",
      "Epoch 332/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1857 - val_loss: 0.9956\n",
      "Epoch 333/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1991 - val_loss: 0.7255\n",
      "Epoch 334/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1869 - val_loss: 0.6633\n",
      "Epoch 335/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1940 - val_loss: 0.6219\n",
      "Epoch 336/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2004 - val_loss: 0.7584\n",
      "Epoch 337/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1709 - val_loss: 0.8388\n",
      "Epoch 338/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1805 - val_loss: 0.7675\n",
      "Epoch 339/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.2082 - val_loss: 0.7763\n",
      "Epoch 340/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1900 - val_loss: 0.6421\n",
      "Epoch 341/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1915 - val_loss: 0.7356\n",
      "Epoch 342/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1994 - val_loss: 0.6297\n",
      "Epoch 343/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1793 - val_loss: 0.7376\n",
      "Epoch 344/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1862 - val_loss: 0.8414\n",
      "Epoch 345/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1980 - val_loss: 0.9156\n",
      "Epoch 346/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1987 - val_loss: 0.7753\n",
      "Epoch 347/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1909 - val_loss: 0.7160\n",
      "Epoch 348/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1778 - val_loss: 0.7561\n",
      "Epoch 349/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1884 - val_loss: 0.6899\n",
      "Epoch 350/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1711 - val_loss: 0.7083\n",
      "Epoch 351/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1742 - val_loss: 0.6531\n",
      "Epoch 352/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1868 - val_loss: 0.6362\n",
      "Epoch 353/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1722 - val_loss: 0.6757\n",
      "Epoch 354/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1792 - val_loss: 0.6796\n",
      "Epoch 355/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1747 - val_loss: 0.6965\n",
      "Epoch 356/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1893 - val_loss: 0.6172\n",
      "Epoch 357/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1690 - val_loss: 0.5559\n",
      "Epoch 358/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1884 - val_loss: 0.5338\n",
      "Epoch 359/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1839 - val_loss: 0.5807\n",
      "Epoch 360/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1681 - val_loss: 0.6065\n",
      "Epoch 361/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1597 - val_loss: 0.8064\n",
      "Epoch 362/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1651 - val_loss: 0.9283\n",
      "Epoch 363/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1910 - val_loss: 0.6096\n",
      "Epoch 364/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1817 - val_loss: 0.7195\n",
      "Epoch 365/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1656 - val_loss: 0.7342\n",
      "Epoch 366/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1814 - val_loss: 0.6636\n",
      "Epoch 367/500\n",
      "574/574 [==============================] - 1s 3ms/step - loss: 0.1845 - val_loss: 0.6442\n",
      "Epoch 368/500\n",
      "574/574 [==============================] - 1s 3ms/step - loss: 0.1900 - val_loss: 0.8167\n",
      "Epoch 369/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1711 - val_loss: 0.7504\n",
      "Epoch 370/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1683 - val_loss: 0.7108\n",
      "Epoch 371/500\n",
      "574/574 [==============================] - 1s 3ms/step - loss: 0.1750 - val_loss: 0.7041\n",
      "Epoch 372/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1750 - val_loss: 0.6641\n",
      "Epoch 373/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1759 - val_loss: 0.6866\n",
      "Epoch 374/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1717 - val_loss: 0.8340\n",
      "Epoch 375/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1691 - val_loss: 0.6572\n",
      "Epoch 376/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1683 - val_loss: 0.6260\n",
      "Epoch 377/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1868 - val_loss: 0.5811\n",
      "Epoch 378/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1827 - val_loss: 0.5356\n",
      "Epoch 379/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1636 - val_loss: 0.5057\n",
      "Epoch 380/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1593 - val_loss: 0.7477\n",
      "Epoch 381/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1742 - val_loss: 0.7666\n",
      "Epoch 382/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1805 - val_loss: 0.6132\n",
      "Epoch 383/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1799 - val_loss: 0.5509\n",
      "Epoch 384/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1732 - val_loss: 0.7918\n",
      "Epoch 385/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1961 - val_loss: 0.5693\n",
      "Epoch 386/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1907 - val_loss: 0.6338\n",
      "Epoch 387/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1589 - val_loss: 0.6408\n",
      "Epoch 388/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1727 - val_loss: 0.8295\n",
      "Epoch 389/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1719 - val_loss: 0.6996\n",
      "Epoch 390/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1646 - val_loss: 0.7158\n",
      "Epoch 391/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1661 - val_loss: 0.6226\n",
      "Epoch 392/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1701 - val_loss: 0.6654\n",
      "Epoch 393/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1600 - val_loss: 0.6910\n",
      "Epoch 394/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1525 - val_loss: 0.7062\n",
      "Epoch 395/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1551 - val_loss: 0.7357\n",
      "Epoch 396/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1728 - val_loss: 0.5436\n",
      "Epoch 397/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1572 - val_loss: 0.9496\n",
      "Epoch 398/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1722 - val_loss: 0.6345\n",
      "Epoch 399/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1767 - val_loss: 0.8043\n",
      "Epoch 400/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1688 - val_loss: 0.5725\n",
      "Epoch 401/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1806 - val_loss: 0.6897\n",
      "Epoch 402/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1502 - val_loss: 0.6951\n",
      "Epoch 403/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1550 - val_loss: 0.5739\n",
      "Epoch 404/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1589 - val_loss: 0.6076\n",
      "Epoch 405/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1677 - val_loss: 0.6745\n",
      "Epoch 406/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1708 - val_loss: 0.5879\n",
      "Epoch 407/500\n",
      "574/574 [==============================] - 1s 3ms/step - loss: 0.1495 - val_loss: 0.6205\n",
      "Epoch 408/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1657 - val_loss: 0.5527\n",
      "Epoch 409/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1783 - val_loss: 0.8437\n",
      "Epoch 410/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1716 - val_loss: 0.6702\n",
      "Epoch 411/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1531 - val_loss: 0.5890\n",
      "Epoch 412/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1596 - val_loss: 0.5546\n",
      "Epoch 413/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1582 - val_loss: 0.6220\n",
      "Epoch 414/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1632 - val_loss: 0.5079\n",
      "Epoch 415/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1611 - val_loss: 0.5136\n",
      "Epoch 416/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1590 - val_loss: 0.6031\n",
      "Epoch 417/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1718 - val_loss: 0.5678\n",
      "Epoch 418/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1624 - val_loss: 0.6828\n",
      "Epoch 419/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1535 - val_loss: 0.7382\n",
      "Epoch 420/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1618 - val_loss: 0.8109\n",
      "Epoch 421/500\n",
      "574/574 [==============================] - 1s 3ms/step - loss: 0.1778 - val_loss: 0.6106\n",
      "Epoch 422/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1672 - val_loss: 0.6131\n",
      "Epoch 423/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1513 - val_loss: 0.4551\n",
      "Epoch 424/500\n",
      "574/574 [==============================] - 1s 3ms/step - loss: 0.1628 - val_loss: 0.5695\n",
      "Epoch 425/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1661 - val_loss: 0.6574\n",
      "Epoch 426/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1715 - val_loss: 0.6454\n",
      "Epoch 427/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1657 - val_loss: 0.9667\n",
      "Epoch 428/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1779 - val_loss: 0.7387\n",
      "Epoch 429/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1682 - val_loss: 0.7289\n",
      "Epoch 430/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1609 - val_loss: 0.7855\n",
      "Epoch 431/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1514 - val_loss: 0.7263\n",
      "Epoch 432/500\n",
      "574/574 [==============================] - 1s 3ms/step - loss: 0.1598 - val_loss: 0.6925\n",
      "Epoch 433/500\n",
      "574/574 [==============================] - 1s 3ms/step - loss: 0.1618 - val_loss: 0.5891\n",
      "Epoch 434/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1606 - val_loss: 0.7323\n",
      "Epoch 435/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1655 - val_loss: 0.5874\n",
      "Epoch 436/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1540 - val_loss: 0.5437\n",
      "Epoch 437/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1625 - val_loss: 0.6442\n",
      "Epoch 438/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1589 - val_loss: 0.4814\n",
      "Epoch 439/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1560 - val_loss: 0.5687\n",
      "Epoch 440/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1537 - val_loss: 0.6279\n",
      "Epoch 441/500\n",
      "574/574 [==============================] - 1s 3ms/step - loss: 0.1454 - val_loss: 0.8331\n",
      "Epoch 442/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1702 - val_loss: 0.8043\n",
      "Epoch 443/500\n",
      "574/574 [==============================] - 1s 3ms/step - loss: 0.1555 - val_loss: 0.6165\n",
      "Epoch 444/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1496 - val_loss: 0.4977\n",
      "Epoch 445/500\n",
      "574/574 [==============================] - 1s 3ms/step - loss: 0.1465 - val_loss: 0.6211\n",
      "Epoch 446/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1518 - val_loss: 0.9059\n",
      "Epoch 447/500\n",
      "574/574 [==============================] - 1s 3ms/step - loss: 0.1751 - val_loss: 0.9863\n",
      "Epoch 448/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1517 - val_loss: 0.8174\n",
      "Epoch 449/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1597 - val_loss: 0.6376\n",
      "Epoch 450/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1523 - val_loss: 0.7164\n",
      "Epoch 451/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1508 - val_loss: 0.6717\n",
      "Epoch 452/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1469 - val_loss: 1.0156\n",
      "Epoch 453/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1515 - val_loss: 1.0193\n",
      "Epoch 454/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1526 - val_loss: 1.0107\n",
      "Epoch 455/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1521 - val_loss: 0.8437\n",
      "Epoch 456/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1387 - val_loss: 0.8773\n",
      "Epoch 457/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1471 - val_loss: 0.6354\n",
      "Epoch 458/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1404 - val_loss: 0.5152\n",
      "Epoch 459/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1535 - val_loss: 0.6767\n",
      "Epoch 460/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1568 - val_loss: 0.5817\n",
      "Epoch 461/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1618 - val_loss: 0.4622\n",
      "Epoch 462/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1420 - val_loss: 0.5108\n",
      "Epoch 463/500\n",
      "574/574 [==============================] - 1s 3ms/step - loss: 0.1514 - val_loss: 0.4374\n",
      "Epoch 464/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1490 - val_loss: 0.5788\n",
      "Epoch 465/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1511 - val_loss: 0.4767\n",
      "Epoch 466/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1388 - val_loss: 0.5378\n",
      "Epoch 467/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1465 - val_loss: 0.4899\n",
      "Epoch 468/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1470 - val_loss: 0.4380\n",
      "Epoch 469/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1715 - val_loss: 0.6024\n",
      "Epoch 470/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1356 - val_loss: 0.5567\n",
      "Epoch 471/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1470 - val_loss: 0.5394\n",
      "Epoch 472/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1490 - val_loss: 0.4664\n",
      "Epoch 473/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1448 - val_loss: 0.6029\n",
      "Epoch 474/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1515 - val_loss: 0.5278\n",
      "Epoch 475/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1519 - val_loss: 0.6382\n",
      "Epoch 476/500\n",
      "574/574 [==============================] - 1s 3ms/step - loss: 0.1519 - val_loss: 0.4953\n",
      "Epoch 477/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1558 - val_loss: 0.7236\n",
      "Epoch 478/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1397 - val_loss: 0.7481\n",
      "Epoch 479/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1430 - val_loss: 0.6909\n",
      "Epoch 480/500\n",
      "574/574 [==============================] - 1s 3ms/step - loss: 0.1485 - val_loss: 0.6539\n",
      "Epoch 481/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1450 - val_loss: 0.7116\n",
      "Epoch 482/500\n",
      "574/574 [==============================] - 1s 3ms/step - loss: 0.1412 - val_loss: 0.6235\n",
      "Epoch 483/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1446 - val_loss: 0.5823\n",
      "Epoch 484/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1469 - val_loss: 0.4650\n",
      "Epoch 485/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1615 - val_loss: 0.4908\n",
      "Epoch 486/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1397 - val_loss: 0.5961\n",
      "Epoch 487/500\n",
      "574/574 [==============================] - 1s 3ms/step - loss: 0.1573 - val_loss: 0.6038\n",
      "Epoch 488/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1469 - val_loss: 0.5467\n",
      "Epoch 489/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1471 - val_loss: 0.4459\n",
      "Epoch 490/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1515 - val_loss: 0.5637\n",
      "Epoch 491/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1467 - val_loss: 0.5316\n",
      "Epoch 492/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1380 - val_loss: 0.6766\n",
      "Epoch 493/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1417 - val_loss: 0.5745\n",
      "Epoch 494/500\n",
      "574/574 [==============================] - 1s 3ms/step - loss: 0.1380 - val_loss: 0.7365\n",
      "Epoch 495/500\n",
      "574/574 [==============================] - 1s 3ms/step - loss: 0.1564 - val_loss: 0.6042\n",
      "Epoch 496/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1518 - val_loss: 0.6214\n",
      "Epoch 497/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1417 - val_loss: 0.9059\n",
      "Epoch 498/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1412 - val_loss: 0.7350\n",
      "Epoch 499/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1388 - val_loss: 0.6092\n",
      "Epoch 500/500\n",
      "574/574 [==============================] - 2s 3ms/step - loss: 0.1407 - val_loss: 0.4997\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 24, 15)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 24, 32)       3072        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 24, 15)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "seq_self_attention_2 (SeqSelfAt (None, 24, 32)       1025        bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 24, 64)       9216        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 24, 32)       0           seq_self_attention_2[0][0]       \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "seq_self_attention_1 (SeqSelfAt (None, 24, 64)       4097        bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_2 (LayerNor (None, 24, 32)       64          add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 24, 64)       0           seq_self_attention_1[0][0]       \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "seq_self_attention_3 (SeqSelfAt (None, 24, 32)       1025        layer_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 24, 64)       128         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_3 (LayerNor (None, 24, 32)       64          seq_self_attention_3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 1536)         0           layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 768)          0           layer_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           24592       flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           12304       flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 16)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 16)           0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1552)         0           dropout_1[0][0]                  \n",
      "                                                                 flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 784)          0           dropout_2[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            1553        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            785         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 1)            0           dense_2[0][0]                    \n",
      "                                                                 dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            2           add_3[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 57,927\n",
      "Trainable params: 57,927\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict time:  0.5053224563598633\n",
      "RMSE2:  9.243014380770873\n",
      "MAE2:  6.646703983236053\n",
      "R-square2:  -0.6148710470090661\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'mse score')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEJCAYAAACe4zzCAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAABIRUlEQVR4nO2dd3hcxdX/P6PVqnfZlovcey8I000xnVBCrwmkGBJqEiCUvEDelx8hQAiBkGAg9B5DqCZgwHQwuFfcm9wkF1m97O78/ph7tXdXu9JK2lVZn8/z6Ln3zi0zV5a/c+6ZM2eU1hpBEAQh/kjo7AYIgiAIsUEEXhAEIU4RgRcEQYhTROAFQRDiFBF4QRCEOEUEXhAEIU6JmcArpUYqpRY7fsqVUjfEqj5BEAQhENURcfBKKRewDThEa7055hUKgiAIJHZQPdOB9S2Je48ePfSgQYM6pkWCIAhxwIIFC3ZrrXuGOtdRAn8h8HJLFw0aNIj58+d3QHMEQRDiA6VUWMM55oOsSqkk4Azg32HOz1BKzVdKzS8tLY11cwRBEA4YOiKK5hRgodZ6V6iTWuvHtdZFWuuinj1DfmUIgiAIbaAjBP4iInDPCIIgCNElpj54pVQ6cAJwZSzrEQSh+9LQ0EBxcTG1tbWd3ZQuTUpKCoWFhbjd7ojvianAa62rgPxY1iEIQvemuLiYzMxMBg0ahFKqs5vTJdFas2fPHoqLixk8eHDE98lMVkEQOpXa2lry8/NF3JtBKUV+fn6rv3JE4AVB6HRE3FumLb+j+BD4z+6DdR91disEQeimZGRkdHYTYkJ8CPwXD8KGTzu7FYIgCF2K+BB4lQCytqwgCO1Ea81NN93EuHHjGD9+PK+++ioAO3bsYNq0aUyaNIlx48bxxRdf4PV6ufzyyxuv/etf/9rJrW9KR6UqiC1KicALgtBu3njjDRYvXsySJUvYvXs3Bx98MNOmTeOll17ipJNO4vbbb8fr9VJdXc3ixYvZtm0by5cvB6CsrKxzGx+C+BB4FCACLwjdnT++s4KV28uj+swxfbO48/SxEV375ZdfctFFF+FyuSgoKODoo4/m+++/5+CDD+ZnP/sZDQ0NnHXWWUyaNIkhQ4awYcMGrr32Wk477TROPPHEqLY7GsSJi0YseEEQYse0adP4/PPP6devH5dffjnPPfccubm5LFmyhGOOOYbHHnuMX/ziF53dzCbEhwWvxIIXhHggUks7Vhx11FHMnDmTn/70p+zdu5fPP/+c+++/n82bN1NYWMgvf/lL6urqWLhwIaeeeipJSUmcc845jBw5kksvvbRT2x6K+BB4FGhfZzdCEIRuzo9//GO++eYbJk6ciFKK++67j969e/Pss89y//3343a7ycjI4LnnnmPbtm1cccUV+HxGe/70pz91cuub0iErOkVKUVGRblM++D8PgnHnwmkPRL1NgiDEllWrVjF69OjObka3INTvSim1QGtdFOr6+PDByyCrIAhCE+JD4CUOXhAEoQlxIvDigxcEQQgmPgReXDSCIAhNiA+Blzh4QRCEJsSJwCcgFrwgCEIg8SHwEgcvCILQhPgQeKXEgBcEoUNoLnf8pk2bGDduXAe2pnniRODFRSMIghBMfAi8uGgEQWgjt9xyC48++mjj8V133cXdd9/N9OnTmTJlCuPHj+ett95q9XNra2u54oorGD9+PJMnT2bu3LkArFixgqlTpzJp0iQmTJjA2rVrqaqq4rTTTmPixImMGzeuMQ99e4lpLhqlVA7wJDAOY2L/TGv9TfQrQqJoBCEeeP8W2Lksus/sPR5OuTfs6QsuuIAbbriBq6++GoDXXnuNDz74gOuuu46srCx2797NoYceyhlnnNGqdVEfffRRlFIsW7aMH374gRNPPJE1a9bw2GOPcf3113PJJZdQX1+P1+tl9uzZ9O3bl/feew+A/fv3t++dLWJtwf8N+K/WehQwEVgVm2okDl4QhLYxefJkSkpK2L59O0uWLCE3N5fevXtz2223MWHCBI4//ni2bdvGrl27WvXcL7/8sjHD5KhRoxg4cCBr1qzhsMMO45577uHPf/4zmzdvJjU1lfHjxzNnzhx+//vf88UXX5CdnR2Vd4uZBa+UygamAZcDaK3rgfrYVCapCgQhLmjG0o4l5513HrNmzWLnzp1ccMEFvPjii5SWlrJgwQLcbjeDBg2itrY2KnVdfPHFHHLIIbz33nuceuqpzJw5k+OOO46FCxcye/Zs/vCHPzB9+nTuuOOOdtcVSwt+MFAKPK2UWqSUelIplR6TmiRVgSAI7eCCCy7glVdeYdasWZx33nns37+fXr164Xa7mTt3Lps3b271M4866ihefPFFANasWcOWLVsYOXIkGzZsYMiQIVx33XWceeaZLF26lO3bt5OWlsall17KTTfdxMKFC6PyXrH0wScCU4BrtdbzlFJ/A24B/sd5kVJqBjADYMCAAW2sSlw0giC0nbFjx1JRUUG/fv3o06cPl1xyCaeffjrjx4+nqKiIUaNGtfqZv/71r/nVr37F+PHjSUxM5JlnniE5OZnXXnuN559/Hrfb3egK+v7777nppptISEjA7Xbzz3/+MyrvFbN88Eqp3sC3WutB1vFRwC1a69PC3dPmfPCPHAS9J8B5T7extYIgdBaSDz5yukw+eK31TmCrUmqkVTQdWBmTyiQOXhAEoQmxXrLvWuBFpVQSsAG4IjbViA9eEISOY9myZVx22WUBZcnJycybN6+TWhSamAq81noxEPLTIapINklBEDqQ8ePHs3jx4s5uRovEz0xWcdEIQrelK60N3VVpy+8oPgRe4uAFoduSkpLCnj17ROSbQWvNnj17SElJadV9sfbBdwziohGEbkthYSHFxcWUlpZ2dlO6NCkpKRQWFrbqnvgQeHHRCEK3xe12M3jw4M5uRlwSJy4axIIXBEEIIk4EXuLgBUEQgokPgZc4eEEQhCbEh8DLIKsgCEIT4kPgZZBVEAShCfEh8BIHLwiC0IQ4EXjxwQuCIAQTHwIvLhpBEIQmxIfAyyCrIAhCE+JE4CUOXhAEIZj4EHjEghcEQQgmPgReXDSCIAhNiA+Bl0FWQRCEJsSHwIsFLwiC0IQ4EniJgxcEQXASHwIvLhpBEIQmxIfARytVQUNN+58hCILQRYipwCulNimllimlFiul5sewova7aNZ+BP+vN2z9LjptEgRB6GQ6woI/Vms9SWtdFLsqouCiWTfHbItj1w8JgiB0JHHioolCFI23wWxd7va3RxAEoQsQa4HXwIdKqQVKqRkxqyUaqQq89WbrSmp3cwRBELoCiTF+/pFa621KqV7AHKXUD1rrz50XWMI/A2DAgAFtrCYKPnix4AVBiDNiasFrrbdZ2xLgP8DUENc8rrUu0loX9ezZs20VRcVFIxa8IAjxRcwEXimVrpTKtPeBE4HlMaqN6LloxIIXBCE+iKWLpgD4j1LKruclrfV/Y1KTSmj/PCfbRZMgAi8IQnwQM4HXWm8AJsbq+QFEIw7etuBNhyQIgtDtiY8wSaDdJrzPYz1GctoIghAfxIfAR3OQddbPoXhB+9skCILQycSJwEcxDt5TAxs/a3eTBEEQOpv4EPhoxsGDX+wFQRC6MfEh8NF00QB46tr3LEEQhC5AfAh8NOPgg/cFQRC6KfEh8NHIBy8uGkEQ4ow4Efgo++DFRSMIQhwQHwIvLhpBEIQmxIfARzMfPIgFLwhCXBAnAh+FOHif+OAFQYgv4kPgibIFLwIvCEIcEB8CHw0XjfMLQFw0giDEAfEh8NEYZHUiFrwgCHFAfAh8VCx4ByLwgiDEAXEk8FFM8+sRgRcEofsTHwLfXhdNsPXvFR+8IAjdn/gQ+Pa6aJwRNCCDrIIgxAURCbxSaqBS6nhrP9VeTLvLoBLa56LxBQm8+OAFQYgDWhR4pdQvgVnATKuoEHgzhm1qA+100QQLugi8IAhxQCQW/NXAEUA5gNZ6LdArlo1qNe120XgCj2WQVRCEOCASga/TWjcqnlIqkVaYy0opl1JqkVLq3bY0MLJK2pmqoImLps50GI8Uwfyn2tW0NlO9FxpqOqduQRDigkgE/jOl1G1AqlLqBODfwDutqON6YFVbGhc57QyTDOWi8TbAnrXw7m/a17S2ct9geP7szqlbEIS4IBKB/z1QCiwDrgRmA3+I5OFKqULgNODJtjYwIpRq30TWYBcNQENVOx7YTmrLzXbL153XBkEQuj2JzZ1USrmAFVrrUcATbXj+Q8DNQIyjbto5yBrsogGodwi8zwcJHRhRumed2brTOq5OQRDijmZVS2vtBVYrpQa09sFKqR8BJVrrBS1cN0MpNV8pNb+0tLS11VgPaceSfaVrYNvCpuX11f79yp1te3ZbsQU+d3DH1isIQlzRrAVvkQusUEp9BzSatVrrM1q47wjgDKXUqUAKkKWUekFrfanzIq3148DjAEVFRW1T6fakKnj04NDl9RX+/eCJULFm3yazzenfsfUKghBXRCLw/9OWB2utbwVuBVBKHQPcGCzu0SWKycYgyEUTwkcfSzy1ZqviY6KxIAidQ4sCr7X+TClVANim7nda65LYNquVRDubJEBdpX/f543us1vC7lA6umMRBCGuiGQm6/nAd8B5wPnAPKXUua2pRGv9qdb6R21rYgREY8m+YDrTgrc7FBF4QRDaQSQumtuBg22rXSnVE/gIk76gi6CMW6N8O2T1jc4j650WfAcLre3zD1Xv4pchOQNGn96xbRIEodsRiZM3IcglsyfC+zoOpcz2wdHRe2aABd/Bg6y2sIeKz3/zKng1hkMZgiDEDZFY8P9VSn0AvGwdXwC8H7smtQUV/UfWiw9eEITuTSSDrDcppc4GjrSKHtda/ye2zWolsYg26UwXjfjgBUGIAi0KvFJqMDBba/2GdZyqlBqktd4U68ZFjIqFBd+Zg6zN+OAFQRAiJBLT99+AcxaR1yrrQsSbwIuLRhCE9hOJwCc60wVb+0mxa1IbiIUFL3HwgiB0cyIR+FKlVGNaAqXUmcDu2DWpDYgPXhAEoQmRRNFcBbyolPo7xheyFfhJTFvVauLMRdNcHLwgCEKEtGj6aq3Xa60PBcYAo7XWh2ut18W+aa0gEhfNpq9gx5LIn9kVfPC15XBXNsx7vGPrFwQhLogkVcH1SqksTCbJh5RSC5VSJ8a+aa0hAoF/5lSYOS3yRwYIfAf54Ct2mfVgGwW+zGy/eqhj6hcEIa6IxHn9M611OXAikA9cBtwb01a1lmgMsgb78TsjXfBfRpjZuMH1iatGEIQ2EIkP3lbPU4HntNYrlIpF2Eo7aKk5dRXNnwdQrsCc8p3loqneDSVB9YnAC4LQBiKx4BcopT7ECPwHSqlMAuPiuwAtCHxFBCsyJTj6usTUwIW4O1pgbddMY/0dHKYpCEJcEIkF/3NgErBBa12tlMoHrohpq1pLSxZ8+faWn5Hg8u+7U8BT4z+OtsD7vFC9BzJ6+cuay2ff1tWqBEE4oIkkisantV6otS6zjvdorZfGvGWtoaU4eKcF/8aM0Nc4BT4xJfBctC3oOXfAA8Oheq+jjmY6EXHRCILQBrpW2t8204IFX+XIdrz0Vdi7McQjHL8KV9BE3fYKbNkW2L3Wf7zqbbN1umKaG8gVF40gCG0gPgS+JRdGTVng8ZoPQlzk6CQaBd4qa6/APzQe/l5k9vduMIIfXGdzOee9ddBQE/1lCQVBiGsiEnil1JFKqSus/Z5WhsmuQ0sCHDxoWV7c9BqnHz8ly2zdaZE9vzU8PNlx4BDsUIt7ONm3SSx5QRBaRSQTne4Efg/cahW5gRdi2ahW05JlG2zBe+pDXOQQ+LR8s3VbvvhYCatT1FvqREpXgxaBFwQhciKx4H8MnIGZyYrWejuQGctGtZqWhC/YgvfUNr1GhRL4NOObj9Ugp9MtE85F03uC2e5eKxa8IAitIhKBr9daayx/glIqPZIHK6VSlFLfKaWWKKVWKKX+2J6GNktLAtzEgq9r/npb4LP6mvj4WK3J6hxYbTLIanU4mX0guz/sWScWvCAIrSISgX9NKTUTyFFK/RL4CHgigvvqgOO01hMxcfQnK6UObXNLm6OlQdZwFnyAayeEBZ+WbwTeUx/GrdNOnKIe3Em5U802JRuSs0z6YgmXFAShFUQSB/8AMAt4HRgJ3KG1fiSC+7TW2k6q7rZ+YhMG4nRd+HxGOGffBKVrTFnt/sDrbQveKfBOF01qrtm604zAf/so3N0zBu22BH7m0fDUyYHnEpPNNiULXG4zs9YnE54EQYicSAZZ04FPtNY3YSz3VKWUO5KHK6VcSqnFQAkwR2s9rz2NDYvTsn3sCFj0Anz3OHzziBHxmjJ/RAyYsEMIcnk4BN7+InCnBk6AijZ2OoQdi00OGieJDgs+Mdl0SuKiEQShFUTiovkcSFZK9QP+i8km+UwkD9dae7XWk4BCYKpSalzwNUqpGUqp+Uqp+aWlpRE3PLAih2VbshJWv2/2cweZRGO+Bsgo8F9jW/BOy99pwdspBHqNCcxR0152BE0Abi40stGCzzZx+d56GWQVBKFVRCLwSmtdDZwN/FNrfR4wtjWVWGkO5gInhzj3uNa6SGtd1LNnG90gwcJnZ4L0evxpCnIH+c83+uDDWPCjfgQX/xsOuSq6Aj/zqMDjUG4X23K30yWEs+D3bY5euwRBiEsiEnil1GHAJcB7VlmLfgtrQlSOtZ8KnAD80MZ2Nk+w68JOFOaphQor0VjOAMf5EBa8E6VgxImQkBAo8NGeSepraDoAnGF1cm6HwLuSrc7AYfH/bUJ02yIIQtwRicDfgJnk9B8rF/wQjDXeEn2AuUqppcD3GB/8u21uaXMEC3WdNbbrrfdb8AEumhAWfLiMlE4ffLSzOnobAhOOAaRb7iE7XUJKNiQmmU4p+D1l0FUQhGZo0f+gtf4M+MxxvAG4LoL7lgKTW7ouKgSHD9ouGk+dP1WwMzWvHfIYLkzSiTM80ueN7qCrt8GkDXaS3tO/TcuH/OGWBV/XtIOprzAdgCAIQghaFHilVBFwGzDIeb3Wuuv4CIJdJ/WWBe+pNRZ8crY/rtwuh6aDrL/4GCodmSch0IXi8wCOTJM1ZbB9EQw9tm3tXvB004HX9B7+7c0bzH5ikrVWa5AFX7tfBF4QhLBEMoL4InATsIwut5KTobS8moDh2cZB1noo32b8707r1xMmTLKwqOnDG6r9+8FfCi9fCFu+gVu3QXJG6xu+NUTUqG3BO2ffNlrwQQJfUxY4tiAIguAgEoEv1Vq/HfOWtINv1+3idOdogj2BqKEGti2AMWcFCXwoCz6CioIFdut31nOiOMPUdiXV7POXJSZbFnxQPcETuARBEBxEIvB3KqWeBD7GpB8AQGv9Rsxa1UoSE8JEt5SsMiJYeDCM/hEsfglyB8PSV4xbJ1yYZDiCXST2/cF5ZFa+DX0mQu7A1seuD54Gw46HE/7PX+ZKMhZ8KBeNIAhCGCKJorkCK5cMcLr186MYtqnVuFUYgd9r+bDzhpj0Az//EHqOMGXe+siiYq78AqZeafbDiXVwMrLXfw4LnrHqaWWisqQMuPR1KBjjL0tMNtb70lcDrw0OsRQEQXAQiQV/sNZ6ZMxb0g7cyhc6y41tYbsda6zaE4g8teFnsjrpMwG2W/O6wrlivI5IG6/HHNtlrc1EGWpilR0yOe+xwHKx4AVBaIZILPivlVJjWr6s80gMZ8E3XuAQeFssPcFhh824aOzQSFvga8sDwycD0v4GTaJqrQXvCpHmx05bEExwGmRBEAQHkQj8ocBipdRqpdRSpdQya/JSl+GD5JOavyCxHRY8+K1q+4vg3v7w/I/9550i3jhL1hO4bY5hJzSty0nwIuA2dRUtP1sQhAOWSAT+ZGA4cCJ+//vpsWxUa1mUeih/73FH+AucFrC9722IfJDVFl1nh7D5S/++00UTPEu2JQs+ZwCc/5z/OJQFH07g60XgBUEITyQzWbt8VqukxATqPc0ItMsh8LaABg+yNmfBK6sf9HmgIcRyf04rvTEE07bgWxD4nqMgyZHKOKEVLhqx4AVBaIZILPguj9ulqG8uGtEpkLY13Jr0u04LPtTAZoAFb7tofP57gimc2rQ9wXU5CeuiqQxdLgiCQJwIfFKii3pfMxa40wdvW8htctF4wgh8Mz74UC6aop+ZdVadz26sK0Sum3AWfL0IvCAI4YkPgXclUOcNJ9Aq0K9t76/7OHDx7WYHWe0omnAWfHODrCEEPsHlF/Zg6zxkO8K0rau4aL76GzwxvbNbIQhCEFFczaLzSE5MoC54zlJiivGHJyYHiqYtqJ/eA+sPcdwQgcDrMALvFPFIBlm9Df52hHO/OKncGbq8q7hodq+BnUvN7ODmOkpBEDqUuBD4pMQQFnxShhHb4EFLp6DuXB5ZBS26aEL54JsJk/TWmwyRAC7r2b/42CQuC8WwEyAps2nUTFeJovE2mHdqqIak9M5ujSAIFnHjomkyyJqSZbbB6Qic7hqn+DYbReOY6ORMD9A4YBvCgm9uopO3Adzpgc8oLILDrw1df05/uK24aWrguororzLVFuxOzZkgTRCETic+BD6Ui8ZOu2sv32cTTuAjjYN3zh61BTekD94S+FA+eG+9PzQyEheNjStosNXnCRxH6CzsLxgReEHoUsSPwAe7aGyBb2LBOwQ1kiX7IFDgnfnhk62vhNr9UGWtzBQcB+8N4aJJzgR3WuCzIyFUNE1XiKQRgReELkncCHx9OAs+mFAzRYHmLXjHRCenvz0502z/+3u4f4jZDx5ktS34k+6BW4vhzEdh8qV+gW+NBR9K4LtCwjFx0QhClyQ+BN6VQEOwD965BquTcIIakQXfEGiR2wJv89B4v48+2Ac/eJq5fvKlJiqncZA1XIcTAjt2Hvzums5KGVyxy2xLV8Oe9WZfBF4QuhTxIfCJCajgfMFhLfhWWMw2tsC/einsWesvt100NmVbjOCBWdv1rmxYPsscJ6YGXutqg8Cf+xQMPtrs2x1Y9d6W79v4BVSWRl5PSxQvgL+MgEUvwqNTocJa2FwEXhC6FDETeKVUf6XUXKXUSqXUCqXU9bGqKzkxgYRggQ9rwYcR1EiiaMAssm2TktX0Wtti37PObFe+ZbaJwSkJ3IHbSEjLM18A4F+cuyWB1xqe/RE8fXLk9bRElbUw+Vu/DiyX9MWC0KWIZRy8B/id1nqhUioTWKCUmqO1XhntikJa8MHuE5uwghqBiwYCI29C1VFXbrbB67c60yWAv6MJvq4lbNdPum3B72n++uAOJxqES4EsFrwgdCliZsFrrXdorRda+xXAKqBfLOpKciXgImiUNVz4YJtcNA4L3pk8zJ3a9NpwVmzwAKnLkROnNYw+HUaeCqc9YLJctiTwnhDZL9tLuN+tCLwgdCk6ZCarUmoQMBmYF4vnu10hXDT1VTDxIrP4tZPgZF6FU6H4OziiGQ+S8x6nIIfqLMJFtQRb8PaXRCQLgjhJzoCLXjb7qXlQ04KLxhn1Ey0aakKXi8ALQpci5gKvlMoAXgdu0FqXhzg/A5gBMGDAgDbV0ScnhSqCLOSMAvjxY00vDva1958Kv5jTfAVOF43XmaAsRObHcFEt4dICt9aCd5KW10kWfJhnbvoC1nwII06Mfp2CILSamEbRKKXcGHF/UWv9RqhrtNaPa62LtNZFPXuGiXxpgYmFOaxkCO+OuBtu3QZXfg6Dj4rsZmfoYThCCTmETu0bzkUT3LE4Fx5pK2n5sPYj+OIv4a+JxUxXW+CDUycAvHRe9OsTBKFNxDKKRgH/AlZprR+MVT0A6cmJjO6TxTVLh7BwV0NTt0xzZBe2fE242aYqxK8v0kFTVxtdNE7yh0JDFXz8v+GFPBYCb69q5ZbEYoLQlYmlBX8EcBlwnFJqsfVzaqwqu/P0sQB8tHJX627MjmDcN5Sl3lx5JPSZZLb9Dmr7MwrG+fcrS0JfEysXjXK1LoZfEIQOJ5ZRNF9qrZXWeoLWepL1MztW9U0dnMe4flksLW7l1P1IXDThhDyc6yYSBh8Fv1kB489t+zPyhvr3wwm80wX078sje67PC1/+FWqbDJkYPLVm0FhyvwtClyYuZrLaTCzMYcnWMjze4MQ0zZCW3/I1zbloblxnhLotROIeao5BR0BmH7NfudPMVv0hqA91WvAr/hPZjNZV78BHd8EndweWN9TCI0Wweja4g6KChp9ktsGzewVB6DTiSuCnjehJRZ2Hr9e3EFli87vVkVmh4QQ+wQUZPY3IdoY/OikdfjnX7FfshBfPhVcuClzpKdgHv/Gzlp9rhztuWxA4aLxvo0nVsHeDlXrB8bvrMRymzgg9LiEIQqcQV/8bjx7Rk8zkRN5duj2yGzJ7R3ZdOFeMne8mwQW3b2+arz01L7Lnt4f0noAyLpq9G02Z02oPFvilr8JXDwdO2ArGvmfbfHjaMWzidPcEL4Xo8/qXSXSy6UuTk2fbgohfSRCE6BBXAp/idnHCmAL+u3wn9Z5WuGlaIiHo1zTmTPjxTBh3TmB5sNvCzhcTS1yJRuQrtvsFt74ZC37thzDnf6CkmYwRzkVSShzuJ6dP3h1kwfs8foF3rjK15gOz3RDBl4MgCFElrgQe4MzJ/Siv9fDa/K2xqyQxFSZe2NS9E5wxMhL/fjTIGWAyWdrukXrHoiS2RT3j08B7nNcEEy6W3zlLNzj1gvb6OzhnpyIDsYLQacSdwE8b3oOigbnM/Hw9Otx6pVd/D9e0w2XgCuOTD7bgO1Lg9212CHyV/5w98zYrKBy0wbpGa/j+X4ELkIebHeucpZuYGijeyZn+Ds75BdAV1owVhAOUuBN4pRRnTu7H1r01bNxdFfqiniOgx7C2VxIuYVkTC74DfPAAuQNhf7H/uMHx3rY1HWxx2/lk9hfDe7+Fx47wXxtW4B0WvDuFRhfNhAvh6Fv8dXSFdWIFQYg/gQc4ZoQZ/Lzy+QUs3xalJe3u2g+DrPQH4VIOJ2cEHoeayh8Lcgaa1aaqd5vj+hACHzwAbAu8cwGT2v1QttX46UPhdN04k6cdeYNZRNzOrulMRtYVXDT/OBy++ltnt0IQOpy4FPj+eWlcUNSftSWVnD/zG7bsacbf3Brs/O/hZnCm5JitLX7BohorsvoGHgf44MNY8HYnYC+3Z5eteqdp+oS1H5nnBFjwTheNtbXfOxazZ9tDyQqYc0dnt0IQOpy4FHiAu388jqcvP5jaBi+vfL8lOg+1BTyswFsWu+0LT0yGkafBGY9Ep/6w7Qr6UgiIoqk1LqVgS9q2snevcZRVQ+Wupi6oF8+B/97qX5oPzAzg4E7DtuBDCXxzYZmxRNxFwgFM3Aq825XAsaN6MW1ET/6zaBteXxQG+1JzzTaci8Zews+2gF1JcNFLMOUn7a+7OYJnj377T38aYm9901z0YMR89zpY+Jw/zn/OHfDVQ/7VopzMf8pY9zZ5Q+D85+Dwa6HHCFNmC/6WeXB3byjf4V+k3BMmh3yscU76EoQDjLgVeJtzDypkx/5aht42m9e+b2foZKg1WAPOW5a0PSEo2MKNFcFLB+5ebVINfPy/xpoP1Y6GapNywFNrYvoB1n1ktqk5ISqxOsgka5whf6gR+RPv9s8TsAeZv37YCPraD/zC3tBJbpv6is6pVxC6AHEv8MePLmDKgBxcCYp73l9FnacdroIkKx1BfZjonGBXSVuWB2wLoTqeeTNNnvg1H0JGiBm7DTVmlmn+MJNmwEm498voba4HyB3U9HxwmKinzi/sDVEaB2ktTgs+3EpUghCnxL3Ap7hdvPHrI3j68oMpq27grcURpjEIRaPAh/nszxsSeNxRAp+USZNFw32Wi6ZyZ9OkZmn5RnCLv4cBh/rfy6a2DK76Eo66MbC8YCyc/yyc/Gd/kjMntivIjn331Pot+M4aeHX+W1Xs7Jw2CEInEfcCb3PU8B5MKMzm5llLueTJb9lX1YaVlJIsV0g4C3f0GXDesyaVAXSciyYhoambxokt8D96CPpOMYJettWs59prDLjTAq/PHw69x8NhVweWDzrSWO6HXhU6/NH23dvrxH7zqMlgCZ1nPTsteOdcAUE4ADhgBF4pxd1nmQUyvlq3h9vfXIavtQOvLVnwSsHYs0BbeXA6yoIH/0Brz9HQryjwnC3wRVfAjLkm8+WOxaasx0gTw24z/Q648CWz73Q59R4Ph1zVfBvS8yGth99ar3QsvtJZAu/0we/b1DltEIRO4oAReIAJhTnMvfEYrp8+nNnLdnL2P79mQ2kroizs7JF2uGQ4PNbXQUcKvC3S486GEScHngt20bhTocrKC99zRGCq44kXmxTIELjQyVVfBnYE4eg1OnR5JFE0tfvhg9tDhzbu2wTrP2n5GU6q98IbV/qPyza37n5B6OYcUAIPMLhHOjccP5ypg/NYvLWMO95qxWIdhUVw5j/g1Puav8520RSMaXtDW4sdupmYApkFZj93sEkhMPzEwGsrdpht/nDIKoRER0dkd2JtpefI0OWRRNF8dh9883dY8krTcw9Phud/3Lq2vP4Lfy6e5GzTSVTvldTFwgHDASfwYNw1/7hkCqN6Z/Llut0cce8nfLo6zJJ3gTfC5EtaTkEw6WL4Q2noSJNYYYt6am5g1MyxtzaNshl5CmT2hZ++0zQVcrhEapHSc5TZ5g6Gq77yl0fioqmz3CmhFiK33V6RTpiqrzZRQjb9psDaOfCXUfDEcbD1+8ieIwjdmANS4AF6ZCTz8EWTyU51s62shsuf/p5vN0S4ElRLKBVoFXcEp9wP5z9v0hjbC5mEm8V52oPw25WQFSISpr3YAt9rDPR2LApu58lpDm2Jd3OLmdeFWSc2mC1f+613gFMfMPn57bJiEXgh/mmnuda9GVGQybzbpjNn5S6ufXkRFz7+LQVZyQzKT+eZK6aSmtSORbU7mh7D/BkyM6xoFh1m0ZPWJAC7YVnrUv7aAm9/NfxuNSx4Bj79k1kPNqMZF5DPau+6j2Dcuf7kbc76a/f7ZxQ3R8kqs/3J2yahWo9hJid++XZ49gz/ILMgxDExs+CVUk8ppUqUUstbvrrzSHG7OH1iX9699kgAdpXXMW/jXj5c2Y1jpjMK4OBfwsUhfNnhOP95uOK/TctzBph0xJGS3sPMB7DnBGT2NuGVANsXwaf3wiuXmOPaoEyftmtm1Tvwp36w8XNzXL3Xf03wPeHYv80MHg+eBgf/wpQlZ5oxgj4TYeeyyN9JELopsXTRPAOc3NJFXYVx/bL5+pbjGo9fmreFNxYWsylcTvmujFJw2gPQd3Lk94w5AwYeFp26r/oKjvytv6z3BJPi4NVLjCX/w7vGwr53gLHuwWSsXPZa4LNm32y2zoXCw602ZfPlQ2aQtrzYRA+F+lrpMdysXyuLkQhxTswEXmv9ObC3xQu7EH1zUpl74zHcfPJI5m3cy29fW8IxD3zKNS8tbH3M/IFMUlrgYG1KFvz8QzO4ayc2s/PefPmQ2b4YtL7t8JOgdBWUrg7M5f7OdSazZSi0ho/uhP9caSz47H6hr8sdZMI2ZWarEEzFLhOOGydpLQ7YQdZwDO6RzhWHD+ba44Zx00kjOXlsb95duoPjH/yMRVv2dXbzui8FY032ySstt8uHfzDbfRuhMkQE07G3mhm2s280/vIxZ1nXb4Jv/9HU+vbUBQp2+bamyxTa5A32P+tAQWv45h+m4xMC2b3W7wZ87kwTjmu7Ebs5nS7wSqkZSqn5Sqn5paWlnd0cAFKTXPzuxJFcfeww/nmpCafcsLuKC2Z+y/vLdrB6ZwW7KyXPeJuwk5U5sX3tTgrGw/Q7/ef6Tw087xTnjZ/D3b1g0fP+sspd4Sdd5VoCv3dDy+0t3x7ZdVp3bZfP/q3wwa3w2mWd3ZLQ+Lzmd91evA3wzvXGBRfp9X8vgpfON8f2mgdbvu3a/54R0ukCr7V+XGtdpLUu6tmznZNsYoBSiqevOJi7Th9DQXYyv3pxISc99DnT//JZ9FaKOpBwp8AFLwaWLZtltjMcvnZXosl5Y9PvoMB7npxu/gN++08zoQlg7v8LvOagy0O3IWegySv01d9g9k0w6+fh2/vgaDPJqjm0hrsL4L3fNX9dpNSWR3+BFDsnT1UzRlT5jshEbd1HsOGzlq9rDR//r/ldh/qaaw3bFppxnTd/Hdn12xeZbfH35t3rKsx4UUMV1HT/L/ZOF/juQJ/sVC4/YjBzfnM0d581jon9c9hf08C0++fy4Jw1lFbUUe/xieBHinNmrSsJ1rxvVsHqNQaO/j2MPdt//px/mTw7BWPN4t5grPvqPbDuY/jvLYE5b065H8adA7/+tmmWzMY6E02M/u7V8N3jsHxWyxZfc4JbucvE18//l7/M6zEdT/H85p8bjKcO7u0f/SUGa1oYDtu5HB4c5R/0bo4XzoHnzohKsxpZY0VwtVfgbcLliwpmszUZL72nuUf7zN8axN6Ft+5j+NvEmPr7Yxkm+TLwDTBSKVWslGrGTOoepLhdXHroQN66+gj+76xxDOuVwcMfr+WQez7i2Ac+Zdr9c3n2602d3cyuj3MSmO2ysVMmHHsbnPe0//z4c+HWrSbE8eyZcNsOOPH/zLmFz5qtM61x0RVw7lPh3TM2R1wfePzYkbBjSWCZc23b5jJR7nREAu+z8t3sWALL/g1v/qr5doDpPOyVr+x6Fr1gtus+MpZ1fXXkIaKhqG5hEp+d52fz181f52lDFtZISLAG5dubVtqeDW2vaNYS9u/bueawLfBPHBvbwdYPbjediHPZzCgTyyiai7TWfbTWbq11odb6Xy3f1X247NCBfPTbo7nv3An4NPi0ZkRBBne+vYJ3l26ntsHLGwuLqaiN8A/tQOP0h+HsJ/0Cnz80svuS0vxLBK5626RcOO4PcPNG+M3K8OvlBjPyFJOqAcwArnLBzKP96Y0B9joWJN/0RejnfPEgvPFL/7EtkFu/NVtnyujqvU3DPL0e+Meh8IKVZ8dOiOZONYPGL5wDfz8Y7ukDjxxkJoN990TggLLWsPr95r8ynHMJvB549nT44T1Huy1LNlSaCCd71jUt27vRP0ltxxJ45wb/cSgWPg/FQfmA7NnLtWVNr6/ZB3dl+zu9YAImwln3e5sZI1v3kRnk9/n8bpi6cr/YFzhmYEdjXCAc9kS+ytiNPYqLpp2cX9SfT288hs9vPpbZ1x3FuH5ZXPPSIib974f89rUl3PnWCraX1XTPePpYctBPYcJ5/lmpAw6N/N6svjQucDL2LBPrnpYXPiwyHIOnwU0bzCImV31u4uM/uN0vTqvf91/71tV+C99TD6veNQnUPv6jcX9kDzA5irZYAr/lG7OtdaRWuG8w/HkgLH7JX7bhU2PBbfwcdq30u4o8tf5BYzvlcVWpEeLZN8LDU/ypKNZ+CC9fCF8+GP5dbReNBnYuMfW9crH//LaFZuvs1EJR+kPgcfkOeHgSfGS5lF66EBY8bQZ1ndRXweKXjRi/fQ08eVzgeduCf+Ec2BPUBlt47ZBaJwuehT/m+Dsw2wr31IdPKvfx/8LXj8CSlwI7vl1W4sHcgf7JcS19+bQH24UY/LuKIiLwUWBQj3TcrgQSXQk8ctEUJhZmM310AaP7ZPHGom0cfu8nHPPAp1zx9He8+v0W9kgEjh87IVvhwZHfo5QJuSw8uOmiJK0lPd/fjiN/Y8IrnzrJCM3c/2dSL//sQ+Ojffe3RqBeu8xM2nr3N/7nTLwQBhwO6z81HcQWy4Iv22wGJJ1W5pu/8n/6lztcP/88DN6zJojV7INP7m6actr2VTdUwZw7/ddC6GgkG1vIGqpho+NrZPZN5mugyvJ971ppBr3DDbY6XVVej/+L4+tHzNZ2kdgZS/dtMoPYs34Gb14V+NXgJMExb+KbvweesxfYCbXQzvu/N1u7HbbAlxebpHKrQ8zOtpMFbv3O/O5sI6NkpdkmZ8PkS81+VQQ5lJzUlEUefeOyvu5iKPAHdC6aWDC4RzpvXWOm5nt9mn/P38ojn6xjT1Udc1eXMnd1KbCM4b0yeHnGoeyrqqdfbippSQfoP8Xh15pMj0OOad19Y84wP9FkyLFmW/ydv2zs2TDgEDj2dnj3Blj4HKz5wJxb8pIRg/OfgUHTYOkrZsB4+SxjbR99Cyx/3bhwfv5hYF3rPzEzbVuabHXxq4Fpku26+04x1j/4B5m3L4GqPabT8nnNF0ffyaZDtAW+Zm9gorXvHjdZNgEOu8aI6+s/N5EkI0NMRHe297XL/AvNgBFze8F5uyN46YJAq9/OEQSmTWl5xuW0dZ6/3F760cbuwIIF3uvxrzNQYf0Ogl08K98y4p8z0P8+9rXl282ze483naP9xZaS5V/lrLmoo2CK55voruP+x4zxhHMXVuwyk/HsTqlMBL5b4kpQXDh1ABdOHUBtg5ete6vZsLuKxz/fwILN+yi628zmzExOJDc9id+dOII6j48jh/Vgx/5aDhoYQVKt7o7L3XpxjxVZfcyyhu40+M8MUzZ4mtlOvhS+/KuZSQvQ/1DjZz/mFhhquRtGnmos0XduMPn5J11s7n/mVBMt4cR2jzgtV3c6/H4jPDEddi0zK2sNPQ4umWWiO1652CROy+htviw+vccMDu9ea+731JqvgwtfgkXPmS+MU+6DQ640XyZgnvPDe+b+niNNqOg+yy10xA1+6/nlC+CiV838g3mPwYQLzDiJbZkDrJ4d+E5PTPf7vm2Bdy6ZCIEDiv88Ak74o3E5OfnhPbOozrQbjW++UeCDnuVcAMZul23BDzrKuLOWzzIdMcBd1rlKq5OyBX7ESUbg7d9Rchak5pj9SLKg2nz3uNl+8n/m57L/+P82nMz/F2yY6z/eszbyOlqJCHwHkeJ2Mbwgk+EFmZw0tjdfrdvNF2t3s66kkjqPlzW7Krj+lcUB9/TOSuHyIwZxyOA8JvXP4d73f+C0CX2YUJjTKe9wQFB0hdn2HmfyydsplV1uE52z7iOTW2fI0ca6c+b8T8szXwHr5sAxtxpfbvBs2vxhgQOVPo8JEdU+4zpITIajbzbW8QArN9DwE8x26HHG3z78eBh4uCmzk6YluI0gzv1/ZsDWtrTfvxnWzzX5fIZOh/UfA9q0I3jSWUZPkz30k7th6aumDcplrOTy7XDm38N/cVzyemC6CVssg8MznZFKFdsDB6htyjabzmvfRuMXn2iFx2qvcX/Zaxisett8adRXGoFvqDFhnhm94fJ3jaX8lxH+52ptOkG7EyjbYsY30vID60/JMoPcSRnGX//FX+G2CNbzDf7dLH+9qcDPexw++3NgWckPge8VRUTgO4kjhvXgiGE9Go9X7Sjnkx9KSE5MoLSyju1ltazdVcG97wcOaj399SYW/OF4MpIT2VVeR+/slOBHC9GgYKw/XM6msMj82ISKsz/iOiMYdrI1Z06e6XcaP789mWbWFbDyTeh/CGT2MfeCcT3Z1qaTM/9hLPOpV5q6T/qTsf7mPwW+BhMuWrUbvptprj/kKmN9r7FcD0OONqK2Z61xD2WGWA8gZwD8eKax7j/+X1OWmucfU6jYYe5zWvJpPUync+oDxh2zc5nJCHrU74zP38nu1YCCi142A8Ng7gu24gGWvGy2Tr//6tmQ09+fEbTwYNi1HHYshcXWBDrb+rYXwbH5Y45/8NTZ0abm+a85+V7/v2t6D9N51FeY+Hw7DXco3rrGdKIjTjGd0v4tsDtExNGn9wQep/UwXwllm/wZWKOICHwXYXSfLEb3CVx5yeP1MfPzDdz/werGsnqPj/F3fciEwmyWFu/nD6eNpjA3jWG9MhjWy4Rd+Xya299cxvlF/Zk84ABw83QlBk/zu3Vszn7CRHQcZYm+nSt/9OlG4CtL4GchBgODyehpRNPmMGu2ZsE486WQkADH32WszyFHw+CjjZDbeX9yBsApf4YXzjZfAL0nwFmPGdG0rWQwPvvDrweUEdKSVfDh7WbWZ/l2M6u4YgdM+SlMneFfqGWqZY0XLzBRMp/ea45dyYFhi0f9ztSvXHDSPaHTVzhxfgW8auWIOewak6PosGvMGMSa92GbNanMXjTevAwmdMji+yfNdsgxfoFPz4fL3jShkvZym/bvy57stG2BSYDn85j5GuvnmrDNMx+Ff1/u70QzC+C8Z8yX1DePGpH/5u/m3/iCF4z7xzlDdvTpJupo18qYCLzSXSjfQlFRkZ4/v5Uz/w4AFm8t4/1lO3hz8TZ2lZv/KOlJLqrqA+OeTxnXm16ZyQwvyOQPby4nN83NojtODPVIoSugNXxwm+kQRp4Suzpq9sG8mebrwZ1iJgFFOl8ATOjmw5PMvisZfjHH5OcZcYp5XiieO8vvZ550KSx2xLDfsdf41m23ROlqeHSqmexWscPfYQST0dvvP7c57UET3vrs6eZ4zFkm7NXmq4dhzv8E3pPVDy543kTZgFlQvvf4pvXNvsnvV592s+lovn8S7iyDmdNg51Ljivv0T/57pt0Mx91u3DV/HWfGMOw5Bodf6482sts+/jwzc/nYP8DRN4V+7xZQSi3QWheFOicWfDdgUv8cJvXP4dZTR6O1prbBR2qSi9oGL/f9dzWzl+1gZ3kt7y8P/OPfV93As19v4uBBefTMTObT1SUsKS7jztPH8vGqEg4bmk92aiv+owvRRSk4+U8tX9feOtLyTHZOm9aIO5jsm30nGwt+yk+MVd9nYvP3HH6NX+AnXmAEPjXXuGPsSU22z9leYjJ/KFzymhmITe9lxiX6T4WHxhtr/5R74bWf+OtIyTYWd7rf1Um/KYHtOOI6I6x/zPGXjTgJejpmOucPD/0OzgXod62A1VaI5/3DrLkYBIo7mIl49jv1GuUXd/CLe/5w08HYs61zBvpDNKOMCHw3QynVuJRgitvFHaeP4Y7Tx/Dmom1U1DYwfXQBZz76FaUVdWQmJ3Ln2yuaPOOFb7cAMLZvFieN7c2ybfuZNrwHroQE5q4u4fCh+Vx26EASXTJNQrA4/3n4/H6/m6klhk7372f3hxvXGdeUc2avTUo2/OQt4zJKy2s69nH9UrN1p8B1i/zJ367+LlDcoem9YDq56xaZqJ2GahMBZQux/dxQODsLW9zB+Myrd5tOonRV4D3OUM5eY804QVq+WcrSFvtzngxMpVEwNjB8NIqIwMcJZ032R2t8e+t0Grw+khMTWLiljEc+WcvAvDQmFOZQWefh2a83sWF3FSu2l7Niu5lpOWelP2HXnJW7eOCD1bgSFGP6ZjG+XzZTBuQyvCCTPZV1jOuXTXqy/OkcUOT0hzMejvx6pUzqiBX/MZFGLa0D3FyorFOA84aYQeDs/n7L30mvEAJv33fIlSbU1R4oP+lPzbdr2PHwq2/gq4dMVFEwQ4/1C/xvVpqUw1McXxh2+o0+E82gOphkeH0nBT5n4BFWJJVu3XrJESA++AMQrTU+De8t28Gtry/l4MF5DO+VwRNfbOTSQwdQ2+BjfWkl9R5fYwfgJCfNzZXThrJg8z6OH92L0yf25Y/vrCA9OZETRhews7yW0X2yyElzs73sAInnFzoX219+Z1l4kfTUm1QMLSWiC2bbQpN4DKDPJJh0Cbx/k+kgMnoZf/vh1zS9r3QNfHQX/OhBWPqaGQu49A0YNr3pte2gOR+8CPwBjsfra3TF1NR7G90/YDqCbWU1fLamlPz0JEoq6uidlcLf565jabE/jC9BQagVDRMTFB6f5i/nTeSjVbuYt3Evlx46kN8cPxylFFprPD6NW1xBQjSIUSw5YDqHL/5ivgKSs+CbR2DiRaG/IsK1bfNXMPioqDdNBF6IKlprtuytpiArhf8u38nS4v1M7J9NaUUd8zbuZc7KXWQmJ3LOQYXMXraDkorA3Dtul+LYkb1YW1LJxt1VTBmQ0yjyT/y0iPSkRFwJigavr7G8qs5DSUUdg3uEyfEuCAcoIvBCh6K1RlmfyVv3VjNrQTHHjy5g6bYy1pdUUVXn4b1lO6isC52aNi3JxbBeGSwt3k9akouRvTNZtKUMV4LiztPHcNiQfN5YtI11JZVcduhANu+p4uJDBvLivM1MGZDLuH7ZAV8mghDPiMALXY6aei+vfL+FY0f2Ii8jicc+XU9eehIbd1fh9WnWllTSOzuFfVX1fL3epGztl5PKtrLQCzD0yU5hx36zWMTwXhmsK60kPz2ZAXmpnDq+Dyt3lFNV5yE71U2iK4Ee6UmcNbkfA/LSWtURaK2pafAeuMnhhC6HCLzQrfH5NIu27mNiYQ7/+HQ9+2saOHFMAUop3l26ndKKOnaW1zJ1UB6Lt5axtqSSk8YW8NnqUrZbom+PEygVmM01PcmFK0ExZWAufbJT8Po0kwfksnpnBVv2VnN+UX/G9Mnizx/8wPayGjxezbJt+xnbN4sZ04YwuX8u+2saGNgjjYykROo8PvZW15OZkkhtg5demSn4fJqEhNADf9X1HlLdrsYvHkFoLSLwwgFJdb0Hr09TUlFHYW4qeyrryUtP4vtNe8lOdfPGwm3Ue33Ue3zM27iHrXv9Xwcp7gSyU92NM4dtQs0gdpKcmECdx4crQZHmdjGidyZLi8uYOjiP3RX19M9LIzMlkexUN6t2lDNv416KBuZy9pRCUtwJNHh9LNu2n9y0JFLcLjxezcbdlVx66ECmDMhlbUklH/+wi9KKOn5/8ihS3K4mbajzeElMSGDL3mr65aSSlBj6C8XpShO6LyLwghAB+6rqSU1ysXJHOWOsvECzFhRT5/GRl+5GazhlXB8q6hpQKDburmJpcRmpSS7eWrydxVvLOGNiX7bsqWb1rgr21zSgFBw8KI+1uyrw+jTltf5xh8QExfGjC5i3cQ/7qv1LO9qdRDCDe6Sz0bEyWK/MZLJS3eyurGNQfjo9MpLJT0/i9YXFeLVGa8hOdZOVmkhNvY+DBuaQmJBAsjuBtCQXG0qrqG3w0jcnlcLcNLaX1aAUHDTQjGMoIDPFjdaaVTsrcCcohvXKYFCPdP49v5ipg3Mpq25gXUklJ4wpIC89qbHD2FVey8rt5Rw2NB+f1hTvq2HB5n1s3lPNr44eSnZa6Nm0DV4f5TUN5KQl4XJ89dR5vCS5EkJ2SPUeHwmKRleb1hqvT0fseuvuHZ0IvCB0AMEDuyXltdQ0eBmYbyJ/quo8/OvLjZw6vjfrS6s4fnQBrgSFz6fZuq+amgYv28tqOGJYD7SGz9aUMjA/jQSlmLNyF5/8UMLS4jLOmNiP0X0y+XDFLlwJitx0NxtKq1hXUokGzp1SSE6am56ZySzaWkZigqK63ts4mS0pMYH6EB1IVkoiCQmKMkdnE4wrQTGyIJOVOwLnR6Qluajz+HC7FFkpbqrqPFTVeynMTWVvVT3VQV89BVnJ9M5OBa05dlQv3lu6g017qmjwGj06fnQBfXNSWLSljH3V9ewqr6VPdirHjepFdb0Hj0+zcns54/tlM2/jXmobvIzrl832shq2l9VQ2+DjtAl96JWVzK79tVTVe8lNc9M3J5Xx/bJ5/tvNjCjI5PCh+dwzexVJiQmcM6WQcf2yqW3wUlnrYVy/bCrrPHy+ppTy2gamDMglOdHFoB5pvLV4O9+s38PvTx6FRrNtXw37qhvon5fKgLw0FIrsNDfvL9vBrvI6+mSnMLRXOqCY1D+HDaWVLC3ez7GjepGb5m5XByMCLwgHALUNXuq9PrJSQlvHFbUNjWGnT321kWE9M5g+uoAGr4+v1+9m6uB8tNZ8tqaUpcX7UUCy20VWSiKb9lQxMC+dNbsq+Grdbk4c25vKOg+j+2QxpEc6by/ZTl66sbr3VzfQ4PMxMC+d1xcWM7RnOj+a0Jfd1lKVr87fysiCTPZV11NR62mcTJeZksiPJ/dj1oJiquu9pCW56J+bxupdFUwdnMe6kkrKqusb8yelJSWyq7yWnLQkEhMUuyvrOGxoPsmJLpSCr9btprbBS7/cVGrqveyrbsBnfdl0NQqykpl32/FturfTBF4pdTLwN8AFPKm1vre560XgBeHAQmvNjv215GckNbpgfFYU1ZCe6biUYsPuKob2TMfjM64X57hDdb2H5EQXHp+P/TUN9Mr0pzWorPPQ4PGRm56Ez2cm1dV7fSwr3k9SYgJ9c1JYub2cMX2z0Nossbm+tJIUt4sGr49FW8pIdbs4ZXxvtu6tYW9VPcmJCazaUc6I3pnm2kQXPTKTGZiXRl56Ep/8UILblUCCgv01DQzqkU7xvhr6ZKeQ5Eqgss7Diu37SU50UTQolxXby6nz+EhxJ/DrY1pImxyGThF4pZQLWAOcABQD3wMXaa3Dpk0TgRcEQWgdzQl8LGeCTAXWaa03aK3rgVeAM1u4RxAEQYgSsRT4foBzufBiq0wQBEHoADp9LrdSaoZSar5San5paWlnN0cQBCFuiKXAbwP6O44LrbIAtNaPa62LtNZFPXv2DD4tCIIgtJFYCvz3wHCl1GClVBJwIfB2DOsTBEEQHMQsY5LW2qOUugb4ABMm+ZTWuun6cYIgCEJMiGlKPK31bGB2LOsQBEEQQtPpg6yCIAhCbOhSqQqUUqXA5jbe3gPYHcXmdAfknQ8M5J0PDNr6zgO11iEjVLqUwLcHpdT8cLO54hV55wMDeecDg1i8s7hoBEEQ4hQReEEQhDglngT+8c5uQCcg73xgIO98YBD1d44bH7wgCIIQSDxZ8IIgCIKDbi/wSqmTlVKrlVLrlFK3dHZ7ooVS6imlVIlSarmjLE8pNUcptdba5lrlSin1sPU7WKqUmtJ5LW87Sqn+Sqm5SqmVSqkVSqnrrfK4fW+lVIpS6jul1BLrnf9olQ9WSs2z3u1VK90HSqlk63iddX5Qp75AO1BKuZRSi5RS71rHcf3OSqlNSqllSqnFSqn5VllM/7a7tcBbi4o8CpwCjAEuUkqN6dxWRY1ngJODym4BPtZaDwc+to7BvP9w62cG8M8OamO08QC/01qPAQ4Frrb+PeP5veuA47TWE4FJwMlKqUOBPwN/1VoPA/YBP7eu/zmwzyr/q3Vdd+V6YJXj+EB452O11pMc4ZCx/dvWWnfbH+Aw4APH8a3ArZ3drii+3yBgueN4NdDH2u8DrLb2Z2JWy2pyXXf+Ad7CrAh2QLw3kAYsBA7BTHhJtMob/84xuZ0Os/YTretUZ7e9De9aaAnaccC7gDoA3nkT0COoLKZ/293agufAW1SkQGu9w9rfCRRY+3H3e7A+wycD84jz97ZcFYuBEmAOsB4o01p7rEuc79X4ztb5/UB+hzY4OjwE3Az4rON84v+dNfChUmqBUmqGVRbTv+2YJhsTYofWWiul4jIESimVAbwO3KC1LldKNZ6Lx/fWWnuBSUqpHOA/wKjObVFsUUr9CCjRWi9QSh3Tyc3pSI7UWm9TSvUC5iilfnCejMXfdne34CNaVCSO2KWU6gNgbUus8rj5PSil3Bhxf1Fr/YZVHPfvDaC1LgPmYtwTOUop2wBzvlfjO1vns4E9HdvSdnMEcIZSahNmrebjgL8R3++M1nqbtS3BdORTifHfdncX+ANtUZG3gZ9a+z/F+Kjt8p9YI++HAvsdn33dBmVM9X8Bq7TWDzpOxe17K6V6WpY7SqlUzJjDKozQn2tdFvzO9u/iXOATbTlpuwta61u11oVa60GY/7OfaK0vIY7fWSmVrpTKtPeBE4HlxPpvu7MHHqIwcHEqsAbjt7y9s9sTxfd6GdgBNGD8bz/H+B0/BtYCHwF51rUKE020HlgGFHV2+9v4zkdi/JRLgcXWz6nx/N7ABGCR9c7LgTus8iHAd8A64N9AslWeYh2vs84P6ex3aOf7HwO8G+/vbL3bEutnha1Vsf7blpmsgiAIcUp3d9EIgiAIYRCBFwRBiFNE4AVBEOIUEXhBEIQ4RQReEAQhThGBF7o0SimtlPqL4/hGpdRd7XjekVb2xh+snxmOcz2tbIWLlFJHBd33qTJZSxdbP7Pa2oYw7dqklOoRzWcKgqQqELo6dcDZSqk/aa3bsuJ8I0qp3sBLwFla64WWoH6glNqmtX4PmA4s01r/IswjLtFaz29PGwShIxELXujqeDBLmf0m+IRSapBS6hMrX/bHSqkBLTzrauAZrfVCAKvDuBm4RSk1CbgPONOy0FMjaZxS6hml1GNKqflKqTVWnhU7z/vTVv7vRUqpY61yl1LqAaXUcqvd1zoed61SaqF1zyjr+qMdXw2L7NmQghAJIvBCd+BR4BKlVHZQ+SPAs1rrCcCLwMMtPGcssCCobD4wVmu9GLgDeFWbfN01Ie5/0SG29zvKB2HyipwGPKaUSsF0JlprPR64CHjWKp9hXT/J0W6b3VrrKZjc3zdaZTcCV2utJwFHAaHaJQghEYEXujxa63LgOeC6oFOHYVwuAM9jUh3Ekkss8Z+ktb7JUf6a1tqntV4LbMBkgzwSeAFAa/0DsBkYARwPzNRWWlyt9V7Hc+zkagswnQDAV8CDSqnrgBztT6crCC0iAi90Fx7C5ONJb8czVgIHBZUdhMkN0h6C8320Nf9HnbX1Yo2Paa3vBX4BpAJf2a4bQYgEEXihW2BZuq/hX8YN4GtMNkKAS4AvWnjMo8Dllr8dpVQ+Zvm3+9rZvPOUUglKqaGYpFKrrbZcYtUzAhhglc8BrrTT4iql8pp7sFJqqNZ6mdb6z5jsqSLwQsSIwAvdib8AzlDCa4ErlFJLgcswa3yilLpKKXVV8M3apFu9FHjCWmzha+AprfU7Edbv9MF/5Cjfgsly+D5wlda6FvgHkKCUWga8Clyuta4DnrSuX6qUWgJc3EKdN9gDspjMou9H2FZBkGySgtAelFLPYNLdRjUuXhCigVjwgiAIcYpY8IIgCHGKWPCCIAhxigi8IAhCnCICLwiCEKeIwAuCIMQpIvCCIAhxigi8IAhCnPL/ASTpbccb8GDBAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "opt = optimizers.Adam(lr=0.0009)\n",
    "print('Train...')\n",
    "model.compile(optimizer = opt , loss=\"mse\")\n",
    "#model.compile(optimizer = \"adam\" , loss=\"mse\")\n",
    "history = model.fit([x_train,x_train], y_train, epochs = 500, batch_size=24, validation_split=0.1, shuffle=True)\n",
    "# history = model.fit(x_train, y_train, epochs = 500, batch_size=6, validation_split=0.1, shuffle=True)\n",
    "model.summary()\n",
    "#Save Model\n",
    "model.save('GRU_Single_Attention_model_neighbor.h5')  # creates a HDF5 file \n",
    "del model\n",
    "\n",
    "custom_ob = {'LayerNormalization': LayerNormalization , 'SeqSelfAttention':SeqSelfAttention}\n",
    "model = load_model('GRU_Single_Attention_model_neighbor.h5', custom_objects=custom_ob)\n",
    "t1 = time.time()\n",
    "y_pred2 = model.predict([x_test,x_test])\n",
    "#y_pred2 = model.predict(x_test)\n",
    "#y_pred = model.predict(x_train)\n",
    "t2 = time.time()\n",
    "print('Predict time: ',t2-t1)\n",
    "y_pred = scaler.inverse_transform(y_pred2)#Undo scaling\n",
    "rmse_lstm2 = np.sqrt(mean_squared_error(y_test, y_pred2))\n",
    "#rmse_lstm = np.sqrt(mean_squared_error(y_train, y_pred))\n",
    "#print('RMSE: ',rmse_lstm)\n",
    "print('RMSE2: ',rmse_lstm2)\n",
    "mae2 = mean_absolute_error(y_test, y_pred2)\n",
    "#mae = mean_absolute_error(y_train, y_pred)\n",
    "#print('MAE: ',mae)\n",
    "print('MAE2: ',mae2)\n",
    "r22 =  r2_score(y_test, y_pred2)\n",
    "# r2 =  r2_score(y_train, y_pred)\n",
    "# print('R-square: ',r2)\n",
    "print('R-square2: ',r22)\n",
    "\n",
    "# n = len(y_test)\n",
    "# p = 12\n",
    "# Adj_r2 = 1-(1-r2)*(n-1)/(n-p-1)\n",
    "# Adj_r22 = 1-(1-r22)*(n-1)/(n-p-1)\n",
    "# print('Adj R-square: ',Adj_r2)\n",
    "# print('Adj R-square2: ',Adj_r22)\n",
    "\n",
    "plt.plot(history.history[\"loss\"],label=\"loss\")\n",
    "plt.plot(history.history[\"val_loss\"],label=\"val_loss\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"No. Of Epochs\")\n",
    "plt.ylabel(\"mse score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d72d7b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa512b58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d116db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
